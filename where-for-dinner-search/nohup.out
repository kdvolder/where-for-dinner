Tilt started on http://localhost:51314/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-search/Tiltfile
Successfully loaded Tiltfile (839.156Âµs)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace default --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner-search:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 4.67s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 4.67s 
where-for-diâ€¦ â”‚ 
Tilt started on http://localhost:51315/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-search/Tiltfile
Successfully loaded Tiltfile (842.029Âµs)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner-search:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 4.96s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 4.96s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ [event: imagerepository workloads/where-for-dinner-search] Failed to update status: Operation cannot be fulfilled on imagerepositories.source.apps.tanzu.vmware.com "where-for-dinner-search": the object has been modified; please apply your changes to the latest version and try again
where-for-diâ€¦ â”‚ [event: imagerepository workloads/where-for-dinner-search-delivery] Failed to update status: Operation cannot be fulfilled on imagerepositories.source.apps.tanzu.vmware.com "where-for-dinner-search-delivery": the object has been modified; please apply your changes to the latest version and try again
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-dbnvg-test-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-dbnvg-test-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/03 06:17:17 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-dbnvg-test-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-dbnvg-test-pod] Container image "projects.registry.vmware.com/tanzu_adv_eng/gradle@sha256:fdb628563dde15aebb58ea90d179247e8c24fd25df5f267db33a7250fbe9c8b8" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/03 06:17:18 Decoded script /tekton/scripts/script-0-jzp54
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 4s
where-for-diâ€¦ â”‚      â”Š Completed       - 3s
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-build-1-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32menv:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_OCI_SOURCE[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_LIVE_RELOAD_ENABLED[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "true"[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_JVM_VERSION[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "17"[0m
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31msource: {}[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32msource:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  blob:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search/b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search/b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search/b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/ipillai/tkgi/buildservice@sha256:b69dec1f1d4f87a006d8861c931944b6c4adf1a3d887e3320c31fa9e55959dbc" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [analyze detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer started at 2024-01-03T06:18:55Z
where-for-diâ€¦ â”‚ [analyze] Restoring data for SBOM from previous image
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer ran for 6.690424231s and ended at 2024-01-03T06:19:02Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/ipillai/tkgi/buildservice@sha256:b69dec1f1d4f87a006d8861c931944b6c4adf1a3d887e3320c31fa9e55959dbc" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/dotnet-core-lite@2.9.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/go-lite@2.2.3' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/nodejs-lite@2.4.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] Timer: Detector started at 2024-01-03T06:19:03Z
where-for-diâ€¦ â”‚ [detect] 11 of 26 buildpacks participating
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/ca-certificates   3.6.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/bellsoft-liberica 9.13.0
where-for-diâ€¦ â”‚ [detect] paketo-buildpacks/syft             1.10.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/maven             6.15.12
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/watchexec         2.8.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/executable-jar    6.8.3
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/apache-tomcat     7.14.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/liberty           3.8.11
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/dist-zip          5.6.8
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/spring-boot       5.27.6
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/image-labels      4.5.6
where-for-diâ€¦ â”‚ [detect] Timer: Detector ran for 1.929661082s and ended at 2024-01-03T06:19:05Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/ipillai/tkgi/buildservice@sha256:b69dec1f1d4f87a006d8861c931944b6c4adf1a3d887e3320c31fa9e55959dbc" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/ipillai/tkgi/buildservice@sha256:b69dec1f1d4f87a006d8861c931944b6c4adf1a3d887e3320c31fa9e55959dbc" already present on machine
where-for-diâ€¦ â”‚ [restore] Timer: Restorer started at 2024-01-03T06:19:06Z
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/ca-certificates:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:java-security-properties" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:jre" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:spring-cloud-bindings" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:web-application-type" from app image
where-for-diâ€¦ â”‚ [restore] Timer: Restorer ran for 38.677247ms and ended at 2024-01-03T06:19:06Z
where-for-diâ€¦ â”‚ [build] Timer: Builder started at 2024-01-03T06:19:07Z
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for CA Certificates[0m[34m 3.6.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-ca-certificates[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for BellSoft Liberica[0m[34m 9.13.0[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-bellsoft-liberica[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ARGS           --no-man-pages --no-header-files --strip-debug --compress=1  configure custom link arguments (--output must be omitted)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ENABLED        false                                                        enables running jlink tool to generate custom JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_TYPE                 JRE                                                          the JVM type - JDK or JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_VERSION              17                                                           the Java version[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_ENABLED           false                                                        enables Java remote debugging support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_PORT              8000                                                         configure the remote debugging port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_SUSPEND           false                                                        configure whether to suspend execution until a debugger has attached[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_HEAP_DUMP_PATH                                                                       write heap dumps on error to this path[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_ENABLED        true                                                         enables Java Native Memory Tracking (NMT)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_LEVEL          summary                                                      configure level of NMT, summary or detail[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ARGS                                                                             configure custom Java Flight Recording (JFR) arguments[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ENABLED             false                                                        enables Java Flight Recording (JFR)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_ENABLED             false                                                        enables Java Management Extensions (JMX)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_PORT                5000                                                         configure the JMX port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_HEAD_ROOM           0                                                            the headroom in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_LOADED_CLASS_COUNT  35% of classes                                               the number of loaded classes in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_THREAD_COUNT        250                                                          the number of threads in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $JAVA_TOOL_OPTIONS                                                                        the JVM launch flags[0m
where-for-diâ€¦ â”‚ [build] [2m    [2mUsing Java version 17 from BP_JVM_VERSION[0m[2m[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JDK 17.0.9[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/bell-sw/Liberica/releases/download/17.0.9+11/bellsoft-jdk17.0.9+11-linux-amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Expanding to /layers/tanzu-buildpacks_bellsoft-liberica/jdk[0m
where-for-diâ€¦ â”‚ [build] [2m    Adding 137 container CA certificates to JVM truststore[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JAVA_HOME.override[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JDK_HOME.override[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JRE 17.0.9[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mJava Security Properties[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mPaketo Syft Buildpack[0m[34m 1.10.1[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/paketo-buildpacks/syft[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/anchore/syft/releases/download/v0.41.1/syft_0.41.1_linux_amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/SYFT_CHECK_FOR_APP_UPDATE.default[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Maven[0m[34m 6.15.12[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-maven[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_EXCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are removed[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_INCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are included[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JAVA_INSTALL_NODE                 false                                                  whether to install Yarn/Node binaries based on the presence of a package.json or yarn.lock file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ACTIVE_PROFILES                                                                    the active profiles (comma separated: such as: p1,!p2,?p3) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ADDITIONAL_BUILD_ARGUMENTS                                                         the additionnal arguments (appended to BP_MAVEN_BUILD_ARGUMENTS) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILD_ARGUMENTS             -Dmaven.test.skip=true --no-transfer-progress package  the arguments to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_ARTIFACT              target/*.[ejw]ar                                       the built application artifact explicitly.  Supersedes $BP_MAVEN_BUILT_MODULE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_MODULE                                                                       the module to find application artifact in[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_DAEMON_ENABLED              false                                                  use maven daemon[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_POM_FILE                    pom.xml                                                the location of the main pom.xml file, relative to the application root[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_SETTINGS_PATH                                                                      the path to a Maven settings file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_VERSION                     3                                                      the Maven version[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_NODE_PROJECT_PATH                                                                        configure a project subdirectory to look for `package.json` and `yarn.lock` files[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating cache directory /home/cnb/.m2[0m
where-for-diâ€¦ â”‚ [build]   [34mCompiled Application[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Executing mvnw --batch-mode -Dmaven.test.skip=true --no-transfer-progress package[0m
where-for-diâ€¦ â”‚ [build]         % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
where-for-diâ€¦ â”‚ [build]                                        Dload  Upload   Total   Spent    Left  Speed
where-for-diâ€¦ â”‚ [build]         0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 59925  100 59925    0     0   214k      0 --:--:-- --:--:-- --:--:--  214k
where-for-diâ€¦ â”‚ [build]       [0m[0m[INFO] Scanning for projects...
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] ----------------< com.example:where-for-dinner-search >-----------------
where-for-diâ€¦ â”‚ [build]       [INFO] Building where-for-dinner-search 0.0.1-SNAPSHOT
where-for-diâ€¦ â”‚ [build]       [INFO] --------------------------------[ jar ]---------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 1 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 4 resources from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Changes detected - recompiling the module! :dependency
where-for-diâ€¦ â”‚ [build]       [INFO] Compiling 7 source files with javac [debug release 17] to target/classes
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[15,17] httpBasic() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[16,13] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[17,13] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[25,16] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[28,12] and() in org.springframework.security.config.web.server.ServerHttpSecurity.AuthorizeExchangeSpec has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[29,12] oauth2ResourceServer() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[29,35] jwt() in org.springframework.security.config.web.server.ServerHttpSecurity.OAuth2ResourceServerSpec has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[30,12] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:testResources (default-testResources) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not copying test resources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:testCompile (default-testCompile) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not compiling test sources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-surefire-plugin:3.0.0:test (default-test) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Tests are skipped.
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-jar-plugin:3.3.0:jar (default-jar) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Building jar: /workspace/target/where-for-dinner-search-0.0.1-SNAPSHOT.jar
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- spring-boot-maven-plugin:3.1.3:repackage (repackage) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Replacing main artifact /workspace/target/where-for-dinner-search-0.0.1-SNAPSHOT.jar with repackaged archive, adding nested dependencies in BOOT-INF/.
where-for-diâ€¦ â”‚ [build]       [INFO] The original artifact has been renamed to /workspace/target/where-for-dinner-search-0.0.1-SNAPSHOT.jar.original
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] BUILD SUCCESS
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] Total time:  01:13 min
where-for-diâ€¦ â”‚ [build]       [INFO] Finished at: 2024-01-03T06:20:39Z
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [0m[0m
where-for-diâ€¦ â”‚ [build]   Removing source code
where-for-diâ€¦ â”‚ [build]   Restoring application artifact
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Watchexec[0m[34m 2.8.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-watchexec[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/watchexec/watchexec/releases/download/cli-v1.20.5/watchexec-1.20.5-x86_64-unknown-linux-musl.tar.xz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Executable JAR[0m[34m 6.8.3[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-executable-jar[0m
where-for-diâ€¦ â”‚ [build]   [34mClass Path[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.delim[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.prepend[0m
where-for-diâ€¦ â”‚ [build]   Process types:
where-for-diâ€¦ â”‚ [build]     [36mexecutable-jar[0m: java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mreload[0m:         watchexec -r --shell=none -- java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mtask[0m:           java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mweb[0m:            java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build] SKIPPED: `Main-Class` found in `META-INF/MANIFEST.MF`, skipping build
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Spring Boot[0m[34m 5.27.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-spring-boot[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_DISABLED   false  whether to contribute Spring Boot cloud bindings support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_VERSION    1      default version of Spring Cloud Bindings library to contribute[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_DISABLED  false  whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_ENABLED   true   Deprecated - whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build]   Creating slices from layers index
where-for-diâ€¦ â”‚ [build] [2m    dependencies (61.9 MB)[0m
where-for-diâ€¦ â”‚ [build] [2m    spring-boot-loader (269.4 KB)[0m
where-for-diâ€¦ â”‚ [build] [2m    snapshot-dependencies (0.0 B)[0m
where-for-diâ€¦ â”‚ [build] [2m    application (103.1 KB)[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mSpring Cloud Bindings 2.0.2[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-bindings/2.0.2/spring-cloud-bindings-2.0.2.jar[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Copying to /layers/tanzu-buildpacks_spring-boot/spring-cloud-bindings[0m
where-for-diâ€¦ â”‚ [build]   [34mWeb Application Type[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Reactive web application detected[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/BPL_JVM_THREAD_COUNT.default[0m
where-for-diâ€¦ â”‚ [build]   4 application slices
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.title
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.version
where-for-diâ€¦ â”‚ [build]     org.springframework.boot.version
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Image Labels[0m[34m 4.5.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-image-labels[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_IMAGE_LABELS                                                                                                                                                           arbitrary image labels[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_AUTHORS                                                                                                                                                            the org.opencontainers.image.authors image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_CREATED                                                                                                                                                            the org.opencontainers.image.created image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DESCRIPTION                                                                                                                                                        the org.opencontainers.image.description image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DOCUMENTATION                                                                                                                                                      the org.opencontainers.image.documentation image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_LICENSES                                                                                                                                                           the org.opencontainers.image.licenses image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REF_NAME                                                                                                                                                           the org.opencontainers.image.ref.name image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REVISION                                                                                                                                                           the org.opencontainers.image.revision image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_SOURCE         tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634  the org.opencontainers.image.source image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_TITLE                                                                                                                                                              the org.opencontainers.image.title image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_URL                                                                                                                                                                the org.opencontainers.image.url image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VENDOR                                                                                                                                                             the org.opencontainers.image.vendor image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VERSION                                                                                                                                                            the org.opencontainers.image.version image label[0m
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.source
where-for-diâ€¦ â”‚ [build] Timer: Builder ran for 1m42.476014285s and ended at 2024-01-03T06:20:49Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/ipillai/tkgi/buildservice@sha256:b69dec1f1d4f87a006d8861c931944b6c4adf1a3d887e3320c31fa9e55959dbc" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [export] Reusing layers from image 'tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-workloads@sha256:088a49bffdd67800ed7b9c9aca9ed9f08fe821837d939515aef50da60ebd3d2b'
where-for-diâ€¦ â”‚ [export] Timer: Exporter started at 2024-01-03T06:22:26Z
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/ca-certificates:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:java-security-properties'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:jre'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/watchexec:watchexec'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/executable-jar:classpath'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:spring-cloud-bindings'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:web-application-type'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:launch.sbom'
where-for-diâ€¦ â”‚ [export] Reusing 2/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Adding 3/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:launcher'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:config'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:process-types'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.lifecycle.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.build.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.project.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.title'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.springframework.boot.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.source'
where-for-diâ€¦ â”‚ [export] Setting default process type 'reload'
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-workloads... started at 2024-01-03T06:22:30Z
where-for-diâ€¦ â”‚ [export] *** Images (sha256:0a73dabd0d894738662db811bd5553f234d051f0d70615806a22cf2e25d6884f):
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-workloads
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-workloads:b1.20240103.061739
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-workloads... ran for 4.086008191s and ended at 2024-01-03T06:22:34Z
where-for-diâ€¦ â”‚ [export] Timer: Exporter ran for 7.708518094s and ended at 2024-01-03T06:22:34Z
where-for-diâ€¦ â”‚ [export] Timer: Cache started at 2024-01-03T06:22:34Z
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/bellsoft-liberica:jdk'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'paketo-buildpacks/syft:syft'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:application'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:cache'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'buildpacksio/lifecycle:cache.sbom'
where-for-diâ€¦ â”‚ [export] Timer: Cache ran for 18.595938816s and ended at 2024-01-03T06:22:53Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:0b5c1bfe87a700cf473725e7cd133f4c7da48ec85baccf2d0c5d6127beb675e7" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 5m12s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Build successful
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): error upgrading connection: unable to upgrade connection: pod not found ("where-for-dinner-search-build-1-build-pod_workloads")
where-for-diâ€¦ â”‚ [event: taskrun workloads/where-for-dinner-search-config-writer-kvl7z] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-config-writer-kvl7z-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-config-writer-kvl7z-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/03 06:23:17 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-config-writer-kvl7z-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-config-writer-kvl7z-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/03 06:23:18 Decoded script /tekton/scripts/script-0-nhqrz
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.IoG5pgecIZ
where-for-diâ€¦ â”‚ + base64 --decode
where-for-diâ€¦ â”‚ + echo -e {"apiDescriptor.yml":"apiVersion: apis.apps.tanzu.vmware.com/v1alpha1\nkind: APIDescriptor\nmetadata:\n  name: where-for-dinner-search\n  annotations:\n    local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634\n    autoscaling.knative.dev/minScale: \"1\"\n  labels:\n    apis.apps.tanzu.vmware.com/register-api: \"true\"\n    app.kubernetes.io/part-of: where-for-dinner-search\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search\nspec:\n  type: openapi\n  description: Where For Dinner Search API\n  system: where-for-dinner\n  owner: where-for-dinner-team\n  location:\n    apiSpec:\n      path: /v3/api-docs\n    server:\n      ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: where-for-dinner-search\n","delivery.yml":"apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: where-for-dinner-search\n  annotations:\n    ootb.apps.tanzu.vmware.com/servicebinding-workload: \"true\"\n    ootb.apps.tanzu.vmware.com/apidescriptor-ref: \"true\"\n    kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings\n  labels:\n    apis.apps.tanzu.vmware.com/register-api: \"true\"\n    app.kubernetes.io/part-of: where-for-dinner-search\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search\nspec:\n  template:\n    metadata:\n      annotations:\n        apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search?sub_path=/\n        apps.tanzu.vmware.com/debug: \"true\"\n        apps.tanzu.vmware.com/live-update: \"true\"\n        autoscaling.knative.dev/maxScale: \"1\"\n        autoscaling.knative.dev/minScale: \"1\"\n        boot.spring.io/actuator: http://:8081/actuator\n        boot.spring.io/version: 3.1.3\n        conventions.carto.run/applied-conventions: |-\n          appliveview-sample/app-live-view-appflavour-check\n          developer-conventions/debug-convention\n          developer-conventions/live-update-convention\n          developer-conventions/add-source-image-label\n          spring-boot-convention/auto-configure-actuators-check\n          spring-boot-convention/is-native-app-check\n          spring-boot-convention/spring-boot\n          spring-boot-convention/spring-boot-web\n          spring-boot-convention/spring-boot-actuator\n          spring-boot-convention/spring-boot-actuator-probes\n          spring-boot-convention/app-live-view-appflavour-check\n          spring-boot-convention/app-live-view-connector-boot\n          spring-boot-convention/app-live-view-appflavours-boot\n          spring-boot-convention/service-intent-postgres\n          spring-boot-convention/service-intent-rabbitmq\n          spring-boot-convention/service-intent-kafka\n        developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634\n        developer.conventions/target-containers: workload\n        local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634\n        services.conventions.carto.run/kafka: kafka-clients/3.4.1\n        services.conventions.carto.run/postgres: r2dbc-postgresql/1.0.2.RELEASE\n        services.conventions.carto.run/rabbitmq: amqp-client/5.17.1\n      labels:\n        apis.apps.tanzu.vmware.com/register-api: \"true\"\n        app.kubernetes.io/component: run\n        app.kubernetes.io/part-of: where-for-dinner-search\n        apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n        apps.tanzu.vmware.com/has-tests: \"true\"\n        apps.tanzu.vmware.com/workload-type: web\n        carto.run/workload-name: where-for-dinner-search\n        conventions.carto.run/framework: spring-boot\n        networking.knative.dev/visibility: cluster-local\n        services.conventions.carto.run/kafka: workload\n        services.conventions.carto.run/postgres: workload\n        services.conventions.carto.run/rabbitmq: workload\n        tanzu.app.live.view: \"true\"\n        tanzu.app.live.view.application.actuator.path: actuator\n        tanzu.app.live.view.application.actuator.port: \"8081\"\n        tanzu.app.live.view.application.flavours: spring-boot\n        tanzu.app.live.view.application.name: where-for-dinner-search\n    spec:\n      containers:\n      - env:\n        - name: BPL_DEBUG_ENABLED\n          value: \"true\"\n        - name: BPL_DEBUG_PORT\n          value: \"9005\"\n        - name: JAVA_TOOL_OPTIONS\n          value: -Dmanagement.endpoint.health.probes.add-additional-paths=\"true\" -Dmanagement.endpoint.health.show-details=\"always\" -Dmanagement.endpoints.web.base-path=\"/actuator\" -Dmanagement.endpoints.web.exposure.include=\"*\" -Dmanagement.health.probes.enabled=\"true\" -Dmanagement.server.port=\"8081\" -Dserver.port=\"8080\"\n        image: tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-workloads@sha256:0a73dabd0d894738662db811bd5553f234d051f0d70615806a22cf2e25d6884f\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: 8080\n            scheme: HTTP\n        name: workload\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 1500m\n            memory: 750M\n          requests:\n            cpu: 100m\n            memory: 500M\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        startupProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n            scheme: HTTP\n      serviceAccountName: default\n","kapp-config.yml":"apiVersion: kapp.k14s.io/v1alpha1\nkind: Config\nrebaseRules:\n- path:\n  - metadata\n  - annotations\n  - serving.knative.dev/creator\n  type: copy\n  sources:\n  - new\n  - existing\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n- path:\n  - metadata\n  - annotations\n  - serving.knative.dev/lastModifier\n  type: copy\n  sources:\n  - new\n  - existing\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\nwaitRules:\n- resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n  conditionMatchers:\n  - type: Ready\n    status: \"True\"\n    success: true\n  - type: Ready\n    status: \"False\"\n    failure: true\nownershipLabelRules:\n- path:\n  - spec\n  - template\n  - metadata\n  - labels\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n","serviceclaims.yml":"apiVersion: kapp.k14s.io/v1alpha1\nkind: Config\nwaitRules:\n- conditionMatchers:\n  - type: ServiceAvailable\n    status: \"False\"\n    failure: true\n  - type: ServiceAvailable\n    status: \"True\"\n    success: true\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: servicebinding.io/v1alpha3\n      kind: ServiceBinding\n---\napiVersion: servicebinding.io/v1alpha3\nkind: ServiceBinding\nmetadata:\n  name: where-for-dinner-search-rmq\n  annotations:\n    local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634\n    autoscaling.knative.dev/minScale: \"1\"\n    kapp.k14s.io/change-group: servicebinding.io/ServiceBindings\n  labels:\n    apis.apps.tanzu.vmware.com/register-api: \"true\"\n    app.kubernetes.io/part-of: where-for-dinner-search\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search\nspec:\n  name: rmq\n  service:\n    apiVersion: services.apps.tanzu.vmware.com/v1alpha1\n    kind: ClassClaim\n    name: msgbroker-where-for-dinner\n  workload:\n    apiVersion: serving.knative.dev/v1\n    kind: Service\n    name: where-for-dinner-search\n"}
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''apiDescriptor.yml'\'') && echo '\''apiVersion: apis.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚ kind: APIDescriptor
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   type: openapi
where-for-diâ€¦ â”‚   description: Where For Dinner Search API
where-for-diâ€¦ â”‚   system: where-for-dinner
where-for-diâ€¦ â”‚   owner: where-for-dinner-team
where-for-diâ€¦ â”‚   location:
where-for-diâ€¦ â”‚     apiSpec:
where-for-diâ€¦ â”‚       path: /v3/api-docs
where-for-diâ€¦ â”‚     server:
where-for-diâ€¦ â”‚       ref:
where-for-diâ€¦ â”‚         apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚         kind: Service
where-for-diâ€¦ â”‚         name: where-for-dinner-search
where-for-diâ€¦ â”‚ '\'' > '\''apiDescriptor.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-postgres
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-rabbitmq
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-kafka
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: kafka-clients/3.4.1
where-for-diâ€¦ â”‚         services.conventions.carto.run/postgres: r2dbc-postgresql/1.0.2.RELEASE
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: amqp-client/5.17.1
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/postgres: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: workload
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner-search
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-workloads@sha256:0a73dabd0d894738662db811bd5553f234d051f0d70615806a22cf2e25d6884f
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''serviceclaims.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - conditionMatchers:
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚       kind: ServiceBinding
where-for-diâ€¦ â”‚ ---
where-for-diâ€¦ â”‚ apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚ kind: ServiceBinding
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-rmq
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-group: servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   name: rmq
where-for-diâ€¦ â”‚   service:
where-for-diâ€¦ â”‚     apiVersion: services.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚     kind: ClassClaim
where-for-diâ€¦ â”‚     name: msgbroker-where-for-dinner
where-for-diâ€¦ â”‚   workload:
where-for-diâ€¦ â”‚     apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚     kind: Service
where-for-diâ€¦ â”‚     name: where-for-dinner-search
where-for-diâ€¦ â”‚ '\'' > '\''serviceclaims.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname apiDescriptor.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: apis.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚ kind: APIDescriptor
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   type: openapi
where-for-diâ€¦ â”‚   description: Where For Dinner Search API
where-for-diâ€¦ â”‚   system: where-for-dinner
where-for-diâ€¦ â”‚   owner: where-for-dinner-team
where-for-diâ€¦ â”‚   location:
where-for-diâ€¦ â”‚     apiSpec:
where-for-diâ€¦ â”‚       path: /v3/api-docs
where-for-diâ€¦ â”‚     server:
where-for-diâ€¦ â”‚       ref:
where-for-diâ€¦ â”‚         apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚         kind: Service
where-for-diâ€¦ â”‚         name: where-for-dinner-search
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-postgres
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-rabbitmq
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-kafka
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: kafka-clients/3.4.1
where-for-diâ€¦ â”‚         services.conventions.carto.run/postgres: r2dbc-postgresql/1.0.2.RELEASE
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: amqp-client/5.17.1
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/postgres: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: workload
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner-search
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-workloads@sha256:0a73dabd0d894738662db811bd5553f234d051f0d70615806a22cf2e25d6884f
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname serviceclaims.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - conditionMatchers:
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚       kind: ServiceBinding
where-for-diâ€¦ â”‚ ---
where-for-diâ€¦ â”‚ apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚ kind: ServiceBinding
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-rmq
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:b21014032bf0a016f8fc0978e67a00888c9b49fa4aa3a4bb99228a4cecef5634
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-group: servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   name: rmq
where-for-diâ€¦ â”‚   service:
where-for-diâ€¦ â”‚     apiVersion: services.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚     kind: ClassClaim
where-for-diâ€¦ â”‚     name: msgbroker-where-for-dinner
where-for-diâ€¦ â”‚   workload:
where-for-diâ€¦ â”‚     apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚     kind: Service
where-for-diâ€¦ â”‚     name: where-for-dinner-search
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-workloads-bundle:934a3a44-5bf5-4dd3-af23-78c15615ee59 -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 3s
where-for-diâ€¦ â”‚      â”Š Completed       - 5s
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): error upgrading connection: unable to upgrade connection: pod not found ("where-for-dinner-search-config-writer-kvl7z-pod_workloads")
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-00001-deployment-586bd764c8-p7m6g):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:39:46.438686913Z","logger":"queueproxy","caller":"sharedmain/main.go:260","message":"Starting queue-proxy","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-586bd764c8-p7m6g"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:39:46.439795447Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server metrics:9090","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-586bd764c8-p7m6g"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:39:46.447909184Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server main:8012","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-586bd764c8-p7m6g"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:39:46.448047366Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server admin:8022","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-586bd764c8-p7m6g"}
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:40:15.618146521Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.2.55:8022 map[] map[] <nil> map[] 10.200.2.1:46124 /wait-for-drain <nil> <nil> <nil> 0xc000362960}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-586bd764c8-p7m6g"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-586bd764c8-p7m6g. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 193 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:40:45.623260464Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.2.55:8022 map[] map[] <nil> map[] 10.200.2.1:46124 /wait-for-drain <nil> <nil> <nil> 0xc0006d3ae0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-586bd764c8-p7m6g"}
where-for-diâ€¦ â”‚ [event: imagerepository workloads/where-for-dinner-search-delivery] Failed to update status: Operation cannot be fulfilled on imagerepositories.source.apps.tanzu.vmware.com "where-for-dinner-search-delivery": the object has been modified; please apply your changes to the latest version and try again
where-for-diâ€¦ â”‚ [event: imagerepository workloads/where-for-dinner-search-delivery] Failed to patch finalizer "source.apps.tanzu.vmware.com/finalizer": imagerepositories.source.apps.tanzu.vmware.com "where-for-dinner-search-delivery" not found
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:40:47.912473561Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.2.55:8022 map[] map[] <nil> map[] 10.200.2.1:46124 /wait-for-drain <nil> <nil> <nil> 0xc000752af0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-586bd764c8-p7m6g"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-586bd764c8-p7m6g. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:40:47.970171691Z","logger":"queueproxy","caller":"sharedmain/main.go:290","message":"Received TERM signal, attempting to gracefully shutdown servers.","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-586bd764c8-p7m6g"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:40:47.970386153Z","logger":"queueproxy","caller":"sharedmain/main.go:291","message":"Sleeping 30s to allow K8s propagation of non-ready state","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-586bd764c8-p7m6g"}
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:41:17.913716156Z","logger":"queueproxy","caller":"sharedmain/main.go:295","message":"Shutting down server: metrics","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-586bd764c8-p7m6g"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:41:17.914312905Z","logger":"queueproxy","caller":"sharedmain/main.go:295","message":"Shutting down server: main","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-586bd764c8-p7m6g"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:41:17.914657313Z","logger":"queueproxy","caller":"sharedmain/main.go:295","message":"Shutting down server: admin","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-586bd764c8-p7m6g"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:41:17.916165943Z","logger":"queueproxy","caller":"sharedmain/main.go:306","message":"Shutdown complete, exiting...","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-586bd764c8-p7m6g"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): error upgrading connection: unable to upgrade connection: pod not found ("where-for-dinner-search-00001-deployment-586bd764c8-p7m6g_workloads")
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [workload queue-proxy]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): error upgrading connection: pods "where-for-dinner-search-00001-deployment-586bd764c8-p7m6g" not found
Tilt started on http://localhost:51314/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-search/Tiltfile
Successfully loaded Tiltfile (633.977Âµs)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner-search:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 5.15s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 5.15s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-build-1-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32menv:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_OCI_SOURCE[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_LIVE_RELOAD_ENABLED[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "true"[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_JVM_VERSION[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "17"[0m
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31msource: {}[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32msource:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  blob:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search/5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search/5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search/5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [analyze detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer started at 2024-01-03T09:33:51Z
where-for-diâ€¦ â”‚ [analyze] Restoring data for SBOM from previous image
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer ran for 7.093901426s and ended at 2024-01-03T09:33:58Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/dotnet-core-lite@2.9.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/go-lite@2.2.3' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/nodejs-lite@2.4.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] Timer: Detector started at 2024-01-03T09:34:00Z
where-for-diâ€¦ â”‚ [detect] 11 of 26 buildpacks participating
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/ca-certificates   3.6.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/bellsoft-liberica 9.13.0
where-for-diâ€¦ â”‚ [detect] paketo-buildpacks/syft             1.10.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/maven             6.15.12
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/watchexec         2.8.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/executable-jar    6.8.3
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/apache-tomcat     7.14.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/liberty           3.8.11
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/dist-zip          5.6.8
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/spring-boot       5.27.6
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/image-labels      4.5.6
where-for-diâ€¦ â”‚ [detect] Timer: Detector ran for 1.225754671s and ended at 2024-01-03T09:34:02Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [restore] Timer: Restorer started at 2024-01-03T09:34:02Z
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/ca-certificates:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:java-security-properties" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:jre" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:spring-cloud-bindings" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:web-application-type" from app image
where-for-diâ€¦ â”‚ [restore] Timer: Restorer ran for 5.092357ms and ended at 2024-01-03T09:34:02Z
where-for-diâ€¦ â”‚ [build] Timer: Builder started at 2024-01-03T09:34:03Z
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for CA Certificates[0m[34m 3.6.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-ca-certificates[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for BellSoft Liberica[0m[34m 9.13.0[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-bellsoft-liberica[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ARGS           --no-man-pages --no-header-files --strip-debug --compress=1  configure custom link arguments (--output must be omitted)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ENABLED        false                                                        enables running jlink tool to generate custom JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_TYPE                 JRE                                                          the JVM type - JDK or JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_VERSION              17                                                           the Java version[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_ENABLED           false                                                        enables Java remote debugging support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_PORT              8000                                                         configure the remote debugging port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_SUSPEND           false                                                        configure whether to suspend execution until a debugger has attached[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_HEAP_DUMP_PATH                                                                       write heap dumps on error to this path[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_ENABLED        true                                                         enables Java Native Memory Tracking (NMT)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_LEVEL          summary                                                      configure level of NMT, summary or detail[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ARGS                                                                             configure custom Java Flight Recording (JFR) arguments[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ENABLED             false                                                        enables Java Flight Recording (JFR)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_ENABLED             false                                                        enables Java Management Extensions (JMX)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_PORT                5000                                                         configure the JMX port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_HEAD_ROOM           0                                                            the headroom in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_LOADED_CLASS_COUNT  35% of classes                                               the number of loaded classes in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_THREAD_COUNT        250                                                          the number of threads in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $JAVA_TOOL_OPTIONS                                                                        the JVM launch flags[0m
where-for-diâ€¦ â”‚ [build] [2m    [2mUsing Java version 17 from BP_JVM_VERSION[0m[2m[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JDK 17.0.9[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/bell-sw/Liberica/releases/download/17.0.9+11/bellsoft-jdk17.0.9+11-linux-amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Expanding to /layers/tanzu-buildpacks_bellsoft-liberica/jdk[0m
where-for-diâ€¦ â”‚ [build] [2m    Adding 137 container CA certificates to JVM truststore[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JAVA_HOME.override[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JDK_HOME.override[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JRE 17.0.9[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mJava Security Properties[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mPaketo Syft Buildpack[0m[34m 1.10.1[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/paketo-buildpacks/syft[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/anchore/syft/releases/download/v0.41.1/syft_0.41.1_linux_amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/SYFT_CHECK_FOR_APP_UPDATE.default[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Maven[0m[34m 6.15.12[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-maven[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_EXCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are removed[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_INCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are included[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JAVA_INSTALL_NODE                 false                                                  whether to install Yarn/Node binaries based on the presence of a package.json or yarn.lock file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ACTIVE_PROFILES                                                                    the active profiles (comma separated: such as: p1,!p2,?p3) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ADDITIONAL_BUILD_ARGUMENTS                                                         the additionnal arguments (appended to BP_MAVEN_BUILD_ARGUMENTS) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILD_ARGUMENTS             -Dmaven.test.skip=true --no-transfer-progress package  the arguments to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_ARTIFACT              target/*.[ejw]ar                                       the built application artifact explicitly.  Supersedes $BP_MAVEN_BUILT_MODULE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_MODULE                                                                       the module to find application artifact in[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_DAEMON_ENABLED              false                                                  use maven daemon[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_POM_FILE                    pom.xml                                                the location of the main pom.xml file, relative to the application root[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_SETTINGS_PATH                                                                      the path to a Maven settings file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_VERSION                     3                                                      the Maven version[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_NODE_PROJECT_PATH                                                                        configure a project subdirectory to look for `package.json` and `yarn.lock` files[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating cache directory /home/cnb/.m2[0m
where-for-diâ€¦ â”‚ [build]   [34mCompiled Application[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Executing mvnw --batch-mode -Dmaven.test.skip=true --no-transfer-progress package[0m
where-for-diâ€¦ â”‚ [build]         % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
where-for-diâ€¦ â”‚ [build]                                        Dload  Upload   Total   Spent    Left  Speed
where-for-diâ€¦ â”‚ [build]         0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 59925  100 59925    0     0   236k      0 --:--:-- --:--:-- --:--:--  236k
where-for-diâ€¦ â”‚ [build]       [0m[0m[INFO] Scanning for projects...
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] ----------------< com.example:where-for-dinner-search >-----------------
where-for-diâ€¦ â”‚ [build]       [INFO] Building where-for-dinner-search 0.0.1-SNAPSHOT
where-for-diâ€¦ â”‚ [build]       [INFO] --------------------------------[ jar ]---------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 1 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 4 resources from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Changes detected - recompiling the module! :dependency
where-for-diâ€¦ â”‚ [build]       [INFO] Compiling 7 source files with javac [debug release 17] to target/classes
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[15,17] httpBasic() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[16,13] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[17,13] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[25,16] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[28,12] and() in org.springframework.security.config.web.server.ServerHttpSecurity.AuthorizeExchangeSpec has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[29,12] oauth2ResourceServer() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[29,35] jwt() in org.springframework.security.config.web.server.ServerHttpSecurity.OAuth2ResourceServerSpec has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[30,12] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:testResources (default-testResources) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not copying test resources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:testCompile (default-testCompile) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not compiling test sources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-surefire-plugin:3.0.0:test (default-test) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Tests are skipped.
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-jar-plugin:3.3.0:jar (default-jar) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Building jar: /workspace/target/where-for-dinner-search-0.0.1-SNAPSHOT.jar
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- spring-boot-maven-plugin:3.1.3:repackage (repackage) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Replacing main artifact /workspace/target/where-for-dinner-search-0.0.1-SNAPSHOT.jar with repackaged archive, adding nested dependencies in BOOT-INF/.
where-for-diâ€¦ â”‚ [build]       [INFO] The original artifact has been renamed to /workspace/target/where-for-dinner-search-0.0.1-SNAPSHOT.jar.original
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] BUILD SUCCESS
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] Total time:  01:06 min
where-for-diâ€¦ â”‚ [build]       [INFO] Finished at: 2024-01-03T09:35:32Z
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [0m[0m
where-for-diâ€¦ â”‚ [build]   Removing source code
where-for-diâ€¦ â”‚ [build]   Restoring application artifact
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Watchexec[0m[34m 2.8.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-watchexec[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/watchexec/watchexec/releases/download/cli-v1.20.5/watchexec-1.20.5-x86_64-unknown-linux-musl.tar.xz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Executable JAR[0m[34m 6.8.3[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-executable-jar[0m
where-for-diâ€¦ â”‚ [build]   [34mClass Path[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.delim[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.prepend[0m
where-for-diâ€¦ â”‚ [build]   Process types:
where-for-diâ€¦ â”‚ [build]     [36mexecutable-jar[0m: java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mreload[0m:         watchexec -r --shell=none -- java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mtask[0m:           java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mweb[0m:            java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build] SKIPPED: `Main-Class` found in `META-INF/MANIFEST.MF`, skipping build
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Spring Boot[0m[34m 5.27.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-spring-boot[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_DISABLED   false  whether to contribute Spring Boot cloud bindings support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_VERSION    1      default version of Spring Cloud Bindings library to contribute[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_DISABLED  false  whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_ENABLED   true   Deprecated - whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build]   Creating slices from layers index
where-for-diâ€¦ â”‚ [build] [2m    dependencies (61.9 MB)[0m
where-for-diâ€¦ â”‚ [build] [2m    spring-boot-loader (269.4 KB)[0m
where-for-diâ€¦ â”‚ [build] [2m    snapshot-dependencies (0.0 B)[0m
where-for-diâ€¦ â”‚ [build] [2m    application (103.1 KB)[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mSpring Cloud Bindings 2.0.2[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-bindings/2.0.2/spring-cloud-bindings-2.0.2.jar[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Copying to /layers/tanzu-buildpacks_spring-boot/spring-cloud-bindings[0m
where-for-diâ€¦ â”‚ [build]   [34mWeb Application Type[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Reactive web application detected[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/BPL_JVM_THREAD_COUNT.default[0m
where-for-diâ€¦ â”‚ [build]   4 application slices
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.title
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.version
where-for-diâ€¦ â”‚ [build]     org.springframework.boot.version
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Image Labels[0m[34m 4.5.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-image-labels[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_IMAGE_LABELS                                                                                                                                                           arbitrary image labels[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_AUTHORS                                                                                                                                                            the org.opencontainers.image.authors image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_CREATED                                                                                                                                                            the org.opencontainers.image.created image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DESCRIPTION                                                                                                                                                        the org.opencontainers.image.description image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DOCUMENTATION                                                                                                                                                      the org.opencontainers.image.documentation image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_LICENSES                                                                                                                                                           the org.opencontainers.image.licenses image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REF_NAME                                                                                                                                                           the org.opencontainers.image.ref.name image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REVISION                                                                                                                                                           the org.opencontainers.image.revision image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_SOURCE         tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa  the org.opencontainers.image.source image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_TITLE                                                                                                                                                              the org.opencontainers.image.title image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_URL                                                                                                                                                                the org.opencontainers.image.url image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VENDOR                                                                                                                                                             the org.opencontainers.image.vendor image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VERSION                                                                                                                                                            the org.opencontainers.image.version image label[0m
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.source
where-for-diâ€¦ â”‚ [build] Timer: Builder ran for 1m33.275586476s and ended at 2024-01-03T09:35:37Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [export] Reusing layers from image 'tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads@sha256:61f0d6f933395be608b55c66ecab88dea7ecf50148cdd68c77c689eebffe7ef3'
where-for-diâ€¦ â”‚ [export] Timer: Exporter started at 2024-01-03T09:37:13Z
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/ca-certificates:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:java-security-properties'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:jre'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/watchexec:watchexec'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/executable-jar:classpath'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:spring-cloud-bindings'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:web-application-type'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:launch.sbom'
where-for-diâ€¦ â”‚ [export] Reusing 2/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Adding 3/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:launcher'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:config'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:process-types'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.lifecycle.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.build.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.project.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.title'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.springframework.boot.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.source'
where-for-diâ€¦ â”‚ [export] Setting default process type 'reload'
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads... started at 2024-01-03T09:37:18Z
where-for-diâ€¦ â”‚ [export] *** Images (sha256:74ac428eac7f18e19e6904d7db53aa92fd8e58cbd7e81f02e2671fd70521245e):
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads:b1.20240103.093235
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads... ran for 9.957294208s and ended at 2024-01-03T09:37:28Z
where-for-diâ€¦ â”‚ [export] Timer: Exporter ran for 14.446679027s and ended at 2024-01-03T09:37:28Z
where-for-diâ€¦ â”‚ [export] Timer: Cache started at 2024-01-03T09:37:28Z
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/bellsoft-liberica:jdk'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'paketo-buildpacks/syft:syft'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:application'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:cache'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'buildpacksio/lifecycle:cache.sbom'
where-for-diâ€¦ â”‚ [export] Timer: Cache ran for 19.735162716s and ended at 2024-01-03T09:37:47Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:0b5c1bfe87a700cf473725e7cd133f4c7da48ec85baccf2d0c5d6127beb675e7" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 5m12s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ Build successful
where-for-diâ€¦ â”‚ [event: taskrun workloads/where-for-dinner-search-config-writer-lkvmg] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-config-writer-lkvmg-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-config-writer-lkvmg-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-config-writer-lkvmg-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/03 09:38:33 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-config-writer-lkvmg-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/03 09:38:34 Decoded script /tekton/scripts/script-0-k9s65
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.Gn7LDuRupU
where-for-diâ€¦ â”‚ + base64 --decode
where-for-diâ€¦ â”‚ + echo -e {"apiDescriptor.yml":"apiVersion: apis.apps.tanzu.vmware.com/v1alpha1\nkind: APIDescriptor\nmetadata:\n  name: where-for-dinner-search\n  annotations:\n    local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa\n    autoscaling.knative.dev/minScale: \"1\"\n  labels:\n    apis.apps.tanzu.vmware.com/register-api: \"true\"\n    app.kubernetes.io/part-of: where-for-dinner-search\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search\nspec:\n  type: openapi\n  description: Where For Dinner Search API\n  system: where-for-dinner\n  owner: where-for-dinner-team\n  location:\n    apiSpec:\n      path: /v3/api-docs\n    server:\n      ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: where-for-dinner-search\n","delivery.yml":"apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: where-for-dinner-search\n  annotations:\n    ootb.apps.tanzu.vmware.com/servicebinding-workload: \"true\"\n    ootb.apps.tanzu.vmware.com/apidescriptor-ref: \"true\"\n    kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings\n  labels:\n    apis.apps.tanzu.vmware.com/register-api: \"true\"\n    app.kubernetes.io/part-of: where-for-dinner-search\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search\nspec:\n  template:\n    metadata:\n      annotations:\n        apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search?sub_path=/\n        apps.tanzu.vmware.com/debug: \"true\"\n        apps.tanzu.vmware.com/live-update: \"true\"\n        autoscaling.knative.dev/maxScale: \"1\"\n        autoscaling.knative.dev/minScale: \"1\"\n        boot.spring.io/actuator: http://:8081/actuator\n        boot.spring.io/version: 3.1.3\n        conventions.carto.run/applied-conventions: |-\n          appliveview-sample/app-live-view-appflavour-check\n          developer-conventions/debug-convention\n          developer-conventions/live-update-convention\n          developer-conventions/add-source-image-label\n          spring-boot-convention/auto-configure-actuators-check\n          spring-boot-convention/is-native-app-check\n          spring-boot-convention/spring-boot\n          spring-boot-convention/spring-boot-web\n          spring-boot-convention/spring-boot-actuator\n          spring-boot-convention/spring-boot-actuator-probes\n          spring-boot-convention/app-live-view-appflavour-check\n          spring-boot-convention/app-live-view-connector-boot\n          spring-boot-convention/app-live-view-appflavours-boot\n          spring-boot-convention/service-intent-postgres\n          spring-boot-convention/service-intent-rabbitmq\n          spring-boot-convention/service-intent-kafka\n        developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa\n        developer.conventions/target-containers: workload\n        local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa\n        services.conventions.carto.run/kafka: kafka-clients/3.4.1\n        services.conventions.carto.run/postgres: r2dbc-postgresql/1.0.2.RELEASE\n        services.conventions.carto.run/rabbitmq: amqp-client/5.17.1\n      labels:\n        apis.apps.tanzu.vmware.com/register-api: \"true\"\n        app.kubernetes.io/component: run\n        app.kubernetes.io/part-of: where-for-dinner-search\n        apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n        apps.tanzu.vmware.com/has-tests: \"true\"\n        apps.tanzu.vmware.com/workload-type: web\n        carto.run/workload-name: where-for-dinner-search\n        conventions.carto.run/framework: spring-boot\n        networking.knative.dev/visibility: cluster-local\n        services.conventions.carto.run/kafka: workload\n        services.conventions.carto.run/postgres: workload\n        services.conventions.carto.run/rabbitmq: workload\n        tanzu.app.live.view: \"true\"\n        tanzu.app.live.view.application.actuator.path: actuator\n        tanzu.app.live.view.application.actuator.port: \"8081\"\n        tanzu.app.live.view.application.flavours: spring-boot\n        tanzu.app.live.view.application.name: where-for-dinner-search\n    spec:\n      containers:\n      - env:\n        - name: BPL_DEBUG_ENABLED\n          value: \"true\"\n        - name: BPL_DEBUG_PORT\n          value: \"9005\"\n        - name: JAVA_TOOL_OPTIONS\n          value: -Dmanagement.endpoint.health.probes.add-additional-paths=\"true\" -Dmanagement.endpoint.health.show-details=\"always\" -Dmanagement.endpoints.web.base-path=\"/actuator\" -Dmanagement.endpoints.web.exposure.include=\"*\" -Dmanagement.health.probes.enabled=\"true\" -Dmanagement.server.port=\"8081\" -Dserver.port=\"8080\"\n        image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads@sha256:74ac428eac7f18e19e6904d7db53aa92fd8e58cbd7e81f02e2671fd70521245e\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: 8080\n            scheme: HTTP\n        name: workload\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 1500m\n            memory: 750M\n          requests:\n            cpu: 100m\n            memory: 500M\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        startupProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n            scheme: HTTP\n      serviceAccountName: default\n","kapp-config.yml":"apiVersion: kapp.k14s.io/v1alpha1\nkind: Config\nrebaseRules:\n- path:\n  - metadata\n  - annotations\n  - serving.knative.dev/creator\n  type: copy\n  sources:\n  - new\n  - existing\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n- path:\n  - metadata\n  - annotations\n  - serving.knative.dev/lastModifier\n  type: copy\n  sources:\n  - new\n  - existing\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\nwaitRules:\n- resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n  conditionMatchers:\n  - type: Ready\n    status: \"True\"\n    success: true\n  - type: Ready\n    status: \"False\"\n    failure: true\nownershipLabelRules:\n- path:\n  - spec\n  - template\n  - metadata\n  - labels\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n","serviceclaims.yml":"apiVersion: kapp.k14s.io/v1alpha1\nkind: Config\nwaitRules:\n- conditionMatchers:\n  - type: ServiceAvailable\n    status: \"False\"\n    failure: true\n  - type: ServiceAvailable\n    status: \"True\"\n    success: true\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: servicebinding.io/v1alpha3\n      kind: ServiceBinding\n---\napiVersion: servicebinding.io/v1alpha3\nkind: ServiceBinding\nmetadata:\n  name: where-for-dinner-search-rmq\n  annotations:\n    local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa\n    autoscaling.knative.dev/minScale: \"1\"\n    kapp.k14s.io/change-group: servicebinding.io/ServiceBindings\n  labels:\n    apis.apps.tanzu.vmware.com/register-api: \"true\"\n    app.kubernetes.io/part-of: where-for-dinner-search\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search\nspec:\n  name: rmq\n  service:\n    apiVersion: services.apps.tanzu.vmware.com/v1alpha1\n    kind: ClassClaim\n    name: msgbroker-where-for-dinner\n  workload:\n    apiVersion: serving.knative.dev/v1\n    kind: Service\n    name: where-for-dinner-search\n"}
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''apiDescriptor.yml'\'') && echo '\''apiVersion: apis.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚ kind: APIDescriptor
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   type: openapi
where-for-diâ€¦ â”‚   description: Where For Dinner Search API
where-for-diâ€¦ â”‚   system: where-for-dinner
where-for-diâ€¦ â”‚   owner: where-for-dinner-team
where-for-diâ€¦ â”‚   location:
where-for-diâ€¦ â”‚     apiSpec:
where-for-diâ€¦ â”‚       path: /v3/api-docs
where-for-diâ€¦ â”‚     server:
where-for-diâ€¦ â”‚       ref:
where-for-diâ€¦ â”‚         apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚         kind: Service
where-for-diâ€¦ â”‚         name: where-for-dinner-search
where-for-diâ€¦ â”‚ '\'' > '\''apiDescriptor.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-postgres
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-rabbitmq
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-kafka
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: kafka-clients/3.4.1
where-for-diâ€¦ â”‚         services.conventions.carto.run/postgres: r2dbc-postgresql/1.0.2.RELEASE
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: amqp-client/5.17.1
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/postgres: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: workload
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner-search
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads@sha256:74ac428eac7f18e19e6904d7db53aa92fd8e58cbd7e81f02e2671fd70521245e
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''serviceclaims.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - conditionMatchers:
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚       kind: ServiceBinding
where-for-diâ€¦ â”‚ ---
where-for-diâ€¦ â”‚ apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚ kind: ServiceBinding
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-rmq
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-group: servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   name: rmq
where-for-diâ€¦ â”‚   service:
where-for-diâ€¦ â”‚     apiVersion: services.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚     kind: ClassClaim
where-for-diâ€¦ â”‚     name: msgbroker-where-for-dinner
where-for-diâ€¦ â”‚   workload:
where-for-diâ€¦ â”‚     apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚     kind: Service
where-for-diâ€¦ â”‚     name: where-for-dinner-search
where-for-diâ€¦ â”‚ '\'' > '\''serviceclaims.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname apiDescriptor.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: apis.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚ kind: APIDescriptor
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   type: openapi
where-for-diâ€¦ â”‚   description: Where For Dinner Search API
where-for-diâ€¦ â”‚   system: where-for-dinner
where-for-diâ€¦ â”‚   owner: where-for-dinner-team
where-for-diâ€¦ â”‚   location:
where-for-diâ€¦ â”‚     apiSpec:
where-for-diâ€¦ â”‚       path: /v3/api-docs
where-for-diâ€¦ â”‚     server:
where-for-diâ€¦ â”‚       ref:
where-for-diâ€¦ â”‚         apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚         kind: Service
where-for-diâ€¦ â”‚         name: where-for-dinner-search
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-postgres
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-rabbitmq
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-kafka
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: kafka-clients/3.4.1
where-for-diâ€¦ â”‚         services.conventions.carto.run/postgres: r2dbc-postgresql/1.0.2.RELEASE
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: amqp-client/5.17.1
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/postgres: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: workload
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner-search
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads@sha256:74ac428eac7f18e19e6904d7db53aa92fd8e58cbd7e81f02e2671fd70521245e
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname serviceclaims.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - conditionMatchers:
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚       kind: ServiceBinding
where-for-diâ€¦ â”‚ ---
where-for-diâ€¦ â”‚ apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚ kind: ServiceBinding
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-rmq
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:5ddeef0357693740d4176e529308fb72950fa5d17ee6999b025b549af79fa6fa
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-group: servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   name: rmq
where-for-diâ€¦ â”‚   service:
where-for-diâ€¦ â”‚     apiVersion: services.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚     kind: ClassClaim
where-for-diâ€¦ â”‚     name: msgbroker-where-for-dinner
where-for-diâ€¦ â”‚   workload:
where-for-diâ€¦ â”‚     apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚     kind: Service
where-for-diâ€¦ â”‚     name: where-for-dinner-search
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads-bundle:7086d72b-e9bd-4ec8-8954-141959240d79 -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 3s
where-for-diâ€¦ â”‚      â”Š Completed       - 5s
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): error upgrading connection: unable to upgrade connection: pod not found ("where-for-dinner-search-config-writer-lkvmg-pod_workloads")
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-00001-deployment-779bc5485c-rrdkh):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:38:59.488869485Z","logger":"queueproxy","caller":"sharedmain/main.go:260","message":"Starting queue-proxy","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:38:59.489081453Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server metrics:9090","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:38:59.4943195Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server main:8012","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:38:59.49444453Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server admin:8022","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 193 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 193 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:39:27.652292459Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc00001b770}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:39:57.652208072Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc0006a4230}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:40:27.655853204Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc000867c70}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:40:57.660932454Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc00050c0a0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 193 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:41:27.652961475Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc000679540}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [workload queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 194 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:42:37.660936586Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc0006d97c0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 190 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:44:27.653233Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc000637310}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:47:47.65232066Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc00071da90}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:53:27.652940105Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc0006cc780}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:53:57.652272473Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc000662c30}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 187 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:59:37.652524626Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc000707b80}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:00:07.652458707Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc0006bcff0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:05:47.652796263Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc00061d720}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:06:17.652148519Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc000674a00}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:11:57.654546878Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc0007098b0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:12:27.654525366Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc000803e00}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 139 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 163 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:17:57.651861611Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc0007a9720}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:18:27.653505192Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc0006aa730}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:24:07.654242078Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc0006d1f90}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:24:37.654757164Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc0006bc190}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 194 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:30:07.656227627Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc0007cb450}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 183 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:30:37.651382165Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc00078c280}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 180 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 149 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 168 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 183 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 146 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 184 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 186 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 183 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:36:17.661838423Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc00065da40}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:36:47.669180149Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc00077d1d0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 191 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 188 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:42:17.653027208Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc00066da40}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:42:47.652695145Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc00077c5f0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:48:17.656880803Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc000695220}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-00001-deployment-779bc5485c-rrdkh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:48:47.654168589Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.45:8022 map[] map[] <nil> map[] 10.200.1.1:48018 /wait-for-drain <nil> <nil> <nil> 0xc00077c870}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-779bc5485c-rrdkh"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
Tilt started on http://localhost:53305/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner/where-for-dinner-search/Tiltfile
Successfully loaded Tiltfile (717.845Âµs)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner-search:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 4.94s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 4.94s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ [event: imagerepository workloads/where-for-dinner-search] Failed to update status: Operation cannot be fulfilled on imagerepositories.source.apps.tanzu.vmware.com "where-for-dinner-search": the object has been modified; please apply your changes to the latest version and try again
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-build-1-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32menv:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_OCI_SOURCE[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:d1391ed1a575a7649f4651848009d8cee7d423ca9cd17c3c636a5b8cdc5eecff[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_LIVE_RELOAD_ENABLED[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "true"[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_JVM_VERSION[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "17"[0m
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31msource: {}[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32msource:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  blob:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search/d1391ed1a575a7649f4651848009d8cee7d423ca9cd17c3c636a5b8cdc5eecff.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search/d1391ed1a575a7649f4651848009d8cee7d423ca9cd17c3c636a5b8cdc5eecff.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search/d1391ed1a575a7649f4651848009d8cee7d423ca9cd17c3c636a5b8cdc5eecff.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [analyze detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer started at 2024-01-04T10:10:33Z
where-for-diâ€¦ â”‚ [analyze] Restoring data for SBOM from previous image
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer ran for 6.543484296s and ended at 2024-01-04T10:10:39Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/dotnet-core-lite@2.9.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/go-lite@2.2.3' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/nodejs-lite@2.4.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] Timer: Detector started at 2024-01-04T10:10:40Z
where-for-diâ€¦ â”‚ [detect] 11 of 26 buildpacks participating
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/ca-certificates   3.6.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/bellsoft-liberica 9.13.0
where-for-diâ€¦ â”‚ [detect] paketo-buildpacks/syft             1.10.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/maven             6.15.12
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/watchexec         2.8.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/executable-jar    6.8.3
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/apache-tomcat     7.14.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/liberty           3.8.11
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/dist-zip          5.6.8
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/spring-boot       5.27.6
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/image-labels      4.5.6
where-for-diâ€¦ â”‚ [detect] Timer: Detector ran for 612.441413ms and ended at 2024-01-04T10:10:41Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [restore] Timer: Restorer started at 2024-01-04T10:10:41Z
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/ca-certificates:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:jre" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:java-security-properties" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:web-application-type" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:spring-cloud-bindings" from app image
where-for-diâ€¦ â”‚ [restore] Timer: Restorer ran for 3.165999ms and ended at 2024-01-04T10:10:41Z
where-for-diâ€¦ â”‚ [build] Timer: Builder started at 2024-01-04T10:10:42Z
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for CA Certificates[0m[34m 3.6.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-ca-certificates[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for BellSoft Liberica[0m[34m 9.13.0[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-bellsoft-liberica[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ARGS           --no-man-pages --no-header-files --strip-debug --compress=1  configure custom link arguments (--output must be omitted)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ENABLED        false                                                        enables running jlink tool to generate custom JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_TYPE                 JRE                                                          the JVM type - JDK or JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_VERSION              17                                                           the Java version[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_ENABLED           false                                                        enables Java remote debugging support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_PORT              8000                                                         configure the remote debugging port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_SUSPEND           false                                                        configure whether to suspend execution until a debugger has attached[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_HEAP_DUMP_PATH                                                                       write heap dumps on error to this path[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_ENABLED        true                                                         enables Java Native Memory Tracking (NMT)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_LEVEL          summary                                                      configure level of NMT, summary or detail[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ARGS                                                                             configure custom Java Flight Recording (JFR) arguments[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ENABLED             false                                                        enables Java Flight Recording (JFR)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_ENABLED             false                                                        enables Java Management Extensions (JMX)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_PORT                5000                                                         configure the JMX port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_HEAD_ROOM           0                                                            the headroom in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_LOADED_CLASS_COUNT  35% of classes                                               the number of loaded classes in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_THREAD_COUNT        250                                                          the number of threads in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $JAVA_TOOL_OPTIONS                                                                        the JVM launch flags[0m
where-for-diâ€¦ â”‚ [build] [2m    [2mUsing Java version 17 from BP_JVM_VERSION[0m[2m[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JDK 17.0.9[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/bell-sw/Liberica/releases/download/17.0.9+11/bellsoft-jdk17.0.9+11-linux-amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Expanding to /layers/tanzu-buildpacks_bellsoft-liberica/jdk[0m
where-for-diâ€¦ â”‚ [build] [2m    Adding 137 container CA certificates to JVM truststore[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JAVA_HOME.override[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JDK_HOME.override[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JRE 17.0.9[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mJava Security Properties[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mPaketo Syft Buildpack[0m[34m 1.10.1[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/paketo-buildpacks/syft[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/anchore/syft/releases/download/v0.41.1/syft_0.41.1_linux_amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/SYFT_CHECK_FOR_APP_UPDATE.default[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Maven[0m[34m 6.15.12[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-maven[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_EXCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are removed[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_INCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are included[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JAVA_INSTALL_NODE                 false                                                  whether to install Yarn/Node binaries based on the presence of a package.json or yarn.lock file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ACTIVE_PROFILES                                                                    the active profiles (comma separated: such as: p1,!p2,?p3) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ADDITIONAL_BUILD_ARGUMENTS                                                         the additionnal arguments (appended to BP_MAVEN_BUILD_ARGUMENTS) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILD_ARGUMENTS             -Dmaven.test.skip=true --no-transfer-progress package  the arguments to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_ARTIFACT              target/*.[ejw]ar                                       the built application artifact explicitly.  Supersedes $BP_MAVEN_BUILT_MODULE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_MODULE                                                                       the module to find application artifact in[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_DAEMON_ENABLED              false                                                  use maven daemon[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_POM_FILE                    pom.xml                                                the location of the main pom.xml file, relative to the application root[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_SETTINGS_PATH                                                                      the path to a Maven settings file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_VERSION                     3                                                      the Maven version[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_NODE_PROJECT_PATH                                                                        configure a project subdirectory to look for `package.json` and `yarn.lock` files[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating cache directory /home/cnb/.m2[0m
where-for-diâ€¦ â”‚ [build]   [34mCompiled Application[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Executing mvnw --batch-mode -Dmaven.test.skip=true --no-transfer-progress package[0m
where-for-diâ€¦ â”‚ [build]         % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
where-for-diâ€¦ â”‚ [build]                                        Dload  Upload   Total   Spent    Left  Speed
where-for-diâ€¦ â”‚ [build]         0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 59925  100 59925    0     0   181k      0 --:--:-- --:--:-- --:--:--  181k
where-for-diâ€¦ â”‚ [build]       [0m[0m[INFO] Scanning for projects...
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] ----------------< com.example:where-for-dinner-search >-----------------
where-for-diâ€¦ â”‚ [build]       [INFO] Building where-for-dinner-search 0.0.1-SNAPSHOT
where-for-diâ€¦ â”‚ [build]       [INFO] --------------------------------[ jar ]---------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 1 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 4 resources from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [build export]
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
Tilt started on http://localhost:53305/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner/where-for-dinner-search/Tiltfile
Successfully loaded Tiltfile (687.488Âµs)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner-search:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 5.36s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 5.36s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-build-1-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32menv:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_OCI_SOURCE[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_LIVE_RELOAD_ENABLED[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "true"[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_JVM_VERSION[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "17"[0m
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31msource: {}[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32msource:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  blob:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search/77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search/77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search/77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [analyze detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer started at 2024-01-04T10:16:43Z
where-for-diâ€¦ â”‚ [analyze] Restoring data for SBOM from previous image
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer ran for 6.426628582s and ended at 2024-01-04T10:16:50Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/dotnet-core-lite@2.9.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/go-lite@2.2.3' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/nodejs-lite@2.4.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] Timer: Detector started at 2024-01-04T10:16:51Z
where-for-diâ€¦ â”‚ [detect] 11 of 26 buildpacks participating
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/ca-certificates   3.6.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/bellsoft-liberica 9.13.0
where-for-diâ€¦ â”‚ [detect] paketo-buildpacks/syft             1.10.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/maven             6.15.12
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/watchexec         2.8.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/executable-jar    6.8.3
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/apache-tomcat     7.14.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/liberty           3.8.11
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/dist-zip          5.6.8
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/spring-boot       5.27.6
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/image-labels      4.5.6
where-for-diâ€¦ â”‚ [detect] Timer: Detector ran for 1.125599155s and ended at 2024-01-04T10:16:52Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [restore] Timer: Restorer started at 2024-01-04T10:16:53Z
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/ca-certificates:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:jre" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:java-security-properties" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:spring-cloud-bindings" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:web-application-type" from app image
where-for-diâ€¦ â”‚ [restore] Timer: Restorer ran for 5.403305ms and ended at 2024-01-04T10:16:53Z
where-for-diâ€¦ â”‚ [build] Timer: Builder started at 2024-01-04T10:16:54Z
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for CA Certificates[0m[34m 3.6.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-ca-certificates[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for BellSoft Liberica[0m[34m 9.13.0[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-bellsoft-liberica[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ARGS           --no-man-pages --no-header-files --strip-debug --compress=1  configure custom link arguments (--output must be omitted)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ENABLED        false                                                        enables running jlink tool to generate custom JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_TYPE                 JRE                                                          the JVM type - JDK or JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_VERSION              17                                                           the Java version[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_ENABLED           false                                                        enables Java remote debugging support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_PORT              8000                                                         configure the remote debugging port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_SUSPEND           false                                                        configure whether to suspend execution until a debugger has attached[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_HEAP_DUMP_PATH                                                                       write heap dumps on error to this path[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_ENABLED        true                                                         enables Java Native Memory Tracking (NMT)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_LEVEL          summary                                                      configure level of NMT, summary or detail[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ARGS                                                                             configure custom Java Flight Recording (JFR) arguments[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ENABLED             false                                                        enables Java Flight Recording (JFR)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_ENABLED             false                                                        enables Java Management Extensions (JMX)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_PORT                5000                                                         configure the JMX port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_HEAD_ROOM           0                                                            the headroom in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_LOADED_CLASS_COUNT  35% of classes                                               the number of loaded classes in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_THREAD_COUNT        250                                                          the number of threads in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $JAVA_TOOL_OPTIONS                                                                        the JVM launch flags[0m
where-for-diâ€¦ â”‚ [build] [2m    [2mUsing Java version 17 from BP_JVM_VERSION[0m[2m[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JDK 17.0.9[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/bell-sw/Liberica/releases/download/17.0.9+11/bellsoft-jdk17.0.9+11-linux-amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Expanding to /layers/tanzu-buildpacks_bellsoft-liberica/jdk[0m
where-for-diâ€¦ â”‚ [build] [2m    Adding 137 container CA certificates to JVM truststore[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JAVA_HOME.override[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JDK_HOME.override[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JRE 17.0.9[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mJava Security Properties[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mPaketo Syft Buildpack[0m[34m 1.10.1[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/paketo-buildpacks/syft[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/anchore/syft/releases/download/v0.41.1/syft_0.41.1_linux_amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/SYFT_CHECK_FOR_APP_UPDATE.default[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Maven[0m[34m 6.15.12[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-maven[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_EXCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are removed[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_INCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are included[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JAVA_INSTALL_NODE                 false                                                  whether to install Yarn/Node binaries based on the presence of a package.json or yarn.lock file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ACTIVE_PROFILES                                                                    the active profiles (comma separated: such as: p1,!p2,?p3) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ADDITIONAL_BUILD_ARGUMENTS                                                         the additionnal arguments (appended to BP_MAVEN_BUILD_ARGUMENTS) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILD_ARGUMENTS             -Dmaven.test.skip=true --no-transfer-progress package  the arguments to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_ARTIFACT              target/*.[ejw]ar                                       the built application artifact explicitly.  Supersedes $BP_MAVEN_BUILT_MODULE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_MODULE                                                                       the module to find application artifact in[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_DAEMON_ENABLED              false                                                  use maven daemon[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_POM_FILE                    pom.xml                                                the location of the main pom.xml file, relative to the application root[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_SETTINGS_PATH                                                                      the path to a Maven settings file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_VERSION                     3                                                      the Maven version[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_NODE_PROJECT_PATH                                                                        configure a project subdirectory to look for `package.json` and `yarn.lock` files[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating cache directory /home/cnb/.m2[0m
where-for-diâ€¦ â”‚ [build]   [34mCompiled Application[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Executing mvnw --batch-mode -Dmaven.test.skip=true --no-transfer-progress package[0m
where-for-diâ€¦ â”‚ [build]         % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
where-for-diâ€¦ â”‚ [build]                                        Dload  Upload   Total   Spent    Left  Speed
where-for-diâ€¦ â”‚ [build]         0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 59925  100 59925    0     0   236k      0 --:--:-- --:--:-- --:--:--  236k
where-for-diâ€¦ â”‚ [build]       [0m[0m[INFO] Scanning for projects...
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] ----------------< com.example:where-for-dinner-search >-----------------
where-for-diâ€¦ â”‚ [build]       [INFO] Building where-for-dinner-search 0.0.1-SNAPSHOT
where-for-diâ€¦ â”‚ [build]       [INFO] --------------------------------[ jar ]---------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 1 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 4 resources from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Changes detected - recompiling the module! :dependency
where-for-diâ€¦ â”‚ [build]       [INFO] Compiling 7 source files with javac [debug release 17] to target/classes
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[15,17] httpBasic() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[16,13] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[17,13] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[25,16] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[28,12] and() in org.springframework.security.config.web.server.ServerHttpSecurity.AuthorizeExchangeSpec has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[29,12] oauth2ResourceServer() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[29,35] jwt() in org.springframework.security.config.web.server.ServerHttpSecurity.OAuth2ResourceServerSpec has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/config/WebSecurityConfig.java:[30,12] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:testResources (default-testResources) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not copying test resources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:testCompile (default-testCompile) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not compiling test sources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-surefire-plugin:3.0.0:test (default-test) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Tests are skipped.
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-jar-plugin:3.3.0:jar (default-jar) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Building jar: /workspace/target/where-for-dinner-search-0.0.1-SNAPSHOT.jar
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- spring-boot-maven-plugin:3.1.3:repackage (repackage) @ where-for-dinner-search ---
where-for-diâ€¦ â”‚ [build]       [INFO] Replacing main artifact /workspace/target/where-for-dinner-search-0.0.1-SNAPSHOT.jar with repackaged archive, adding nested dependencies in BOOT-INF/.
where-for-diâ€¦ â”‚ [build]       [INFO] The original artifact has been renamed to /workspace/target/where-for-dinner-search-0.0.1-SNAPSHOT.jar.original
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] BUILD SUCCESS
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] Total time:  01:06 min
where-for-diâ€¦ â”‚ [build]       [INFO] Finished at: 2024-01-04T10:18:16Z
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [0m[0m
where-for-diâ€¦ â”‚ [build]   Removing source code
where-for-diâ€¦ â”‚ [build]   Restoring application artifact
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Watchexec[0m[34m 2.8.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-watchexec[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/watchexec/watchexec/releases/download/cli-v1.20.5/watchexec-1.20.5-x86_64-unknown-linux-musl.tar.xz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Executable JAR[0m[34m 6.8.3[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-executable-jar[0m
where-for-diâ€¦ â”‚ [build]   [34mClass Path[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.delim[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.prepend[0m
where-for-diâ€¦ â”‚ [build]   Process types:
where-for-diâ€¦ â”‚ [build]     [36mexecutable-jar[0m: java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mreload[0m:         watchexec -r --shell=none -- java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mtask[0m:           java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mweb[0m:            java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build] SKIPPED: `Main-Class` found in `META-INF/MANIFEST.MF`, skipping build
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Spring Boot[0m[34m 5.27.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-spring-boot[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_DISABLED   false  whether to contribute Spring Boot cloud bindings support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_VERSION    1      default version of Spring Cloud Bindings library to contribute[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_DISABLED  false  whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_ENABLED   true   Deprecated - whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build]   Creating slices from layers index
where-for-diâ€¦ â”‚ [build] [2m    dependencies (61.9 MB)[0m
where-for-diâ€¦ â”‚ [build] [2m    spring-boot-loader (269.4 KB)[0m
where-for-diâ€¦ â”‚ [build] [2m    snapshot-dependencies (0.0 B)[0m
where-for-diâ€¦ â”‚ [build] [2m    application (103.1 KB)[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mSpring Cloud Bindings 2.0.2[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-bindings/2.0.2/spring-cloud-bindings-2.0.2.jar[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Copying to /layers/tanzu-buildpacks_spring-boot/spring-cloud-bindings[0m
where-for-diâ€¦ â”‚ [build]   [34mWeb Application Type[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Reactive web application detected[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/BPL_JVM_THREAD_COUNT.default[0m
where-for-diâ€¦ â”‚ [build]   4 application slices
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.title
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.version
where-for-diâ€¦ â”‚ [build]     org.springframework.boot.version
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Image Labels[0m[34m 4.5.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-image-labels[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_IMAGE_LABELS                                                                                                                                                           arbitrary image labels[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_AUTHORS                                                                                                                                                            the org.opencontainers.image.authors image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_CREATED                                                                                                                                                            the org.opencontainers.image.created image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DESCRIPTION                                                                                                                                                        the org.opencontainers.image.description image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DOCUMENTATION                                                                                                                                                      the org.opencontainers.image.documentation image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_LICENSES                                                                                                                                                           the org.opencontainers.image.licenses image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REF_NAME                                                                                                                                                           the org.opencontainers.image.ref.name image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REVISION                                                                                                                                                           the org.opencontainers.image.revision image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_SOURCE         tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8  the org.opencontainers.image.source image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_TITLE                                                                                                                                                              the org.opencontainers.image.title image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_URL                                                                                                                                                                the org.opencontainers.image.url image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VENDOR                                                                                                                                                             the org.opencontainers.image.vendor image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VERSION                                                                                                                                                            the org.opencontainers.image.version image label[0m
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.source
where-for-diâ€¦ â”‚ [build] Timer: Builder ran for 1m29.303065596s and ended at 2024-01-04T10:18:23Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [export] Reusing layers from image 'tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads@sha256:088a49bffdd67800ed7b9c9aca9ed9f08fe821837d939515aef50da60ebd3d2b'
where-for-diâ€¦ â”‚ [export] Timer: Exporter started at 2024-01-04T10:20:00Z
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/ca-certificates:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:java-security-properties'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:jre'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/watchexec:watchexec'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/executable-jar:classpath'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:spring-cloud-bindings'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:web-application-type'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:launch.sbom'
where-for-diâ€¦ â”‚ [export] Reusing 2/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Adding 3/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:launcher'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:config'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:process-types'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.lifecycle.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.build.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.project.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.title'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.springframework.boot.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.source'
where-for-diâ€¦ â”‚ [export] Setting default process type 'reload'
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads... started at 2024-01-04T10:20:01Z
where-for-diâ€¦ â”‚ [export] *** Images (sha256:de69ce23e028e8a5384a23aac74195d24d3f45fb6f9b1d39e75d39260e3d2f49):
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads:b1.20240104.101522
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads... ran for 4.055212802s and ended at 2024-01-04T10:20:05Z
where-for-diâ€¦ â”‚ [export] Timer: Exporter ran for 5.398898808s and ended at 2024-01-04T10:20:05Z
where-for-diâ€¦ â”‚ [export] Timer: Cache started at 2024-01-04T10:20:05Z
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/bellsoft-liberica:jdk'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'paketo-buildpacks/syft:syft'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:application'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:cache'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'buildpacksio/lifecycle:cache.sbom'
where-for-diâ€¦ â”‚ [export] Timer: Cache ran for 10.219935178s and ended at 2024-01-04T10:20:15Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:0b5c1bfe87a700cf473725e7cd133f4c7da48ec85baccf2d0c5d6127beb675e7" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 4m52s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ Build successful
where-for-diâ€¦ â”‚ [event: taskrun workloads/where-for-dinner-search-config-writer-6tjng] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-config-writer-6tjng-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-config-writer-6tjng-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-config-writer-6tjng-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/04 10:20:42 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-config-writer-6tjng-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/04 10:20:42 Decoded script /tekton/scripts/script-0-8bgvs
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.CF8xPInMKS
where-for-diâ€¦ â”‚ + base64 --decode
where-for-diâ€¦ â”‚ + echo -e {"apiDescriptor.yml":"apiVersion: apis.apps.tanzu.vmware.com/v1alpha1\nkind: APIDescriptor\nmetadata:\n  name: where-for-dinner-search\n  annotations:\n    local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8\n    autoscaling.knative.dev/minScale: \"1\"\n  labels:\n    apis.apps.tanzu.vmware.com/register-api: \"true\"\n    app.kubernetes.io/part-of: where-for-dinner-search\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search\nspec:\n  type: openapi\n  description: Where For Dinner Search API\n  system: where-for-dinner\n  owner: where-for-dinner-team\n  location:\n    apiSpec:\n      path: /v3/api-docs\n    server:\n      ref:\n        apiVersion: serving.knative.dev/v1\n        kind: Service\n        name: where-for-dinner-search\n","delivery.yml":"apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: where-for-dinner-search\n  annotations:\n    ootb.apps.tanzu.vmware.com/servicebinding-workload: \"true\"\n    ootb.apps.tanzu.vmware.com/apidescriptor-ref: \"true\"\n    kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings\n  labels:\n    apis.apps.tanzu.vmware.com/register-api: \"true\"\n    app.kubernetes.io/part-of: where-for-dinner-search\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search\nspec:\n  template:\n    metadata:\n      annotations:\n        apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search?sub_path=/\n        apps.tanzu.vmware.com/debug: \"true\"\n        apps.tanzu.vmware.com/live-update: \"true\"\n        autoscaling.knative.dev/maxScale: \"1\"\n        autoscaling.knative.dev/minScale: \"1\"\n        boot.spring.io/actuator: http://:8081/actuator\n        boot.spring.io/version: 3.1.3\n        conventions.carto.run/applied-conventions: |-\n          appliveview-sample/app-live-view-appflavour-check\n          developer-conventions/debug-convention\n          developer-conventions/live-update-convention\n          developer-conventions/add-source-image-label\n          spring-boot-convention/auto-configure-actuators-check\n          spring-boot-convention/is-native-app-check\n          spring-boot-convention/spring-boot\n          spring-boot-convention/spring-boot-web\n          spring-boot-convention/spring-boot-actuator\n          spring-boot-convention/spring-boot-actuator-probes\n          spring-boot-convention/app-live-view-appflavour-check\n          spring-boot-convention/app-live-view-connector-boot\n          spring-boot-convention/app-live-view-appflavours-boot\n          spring-boot-convention/service-intent-postgres\n          spring-boot-convention/service-intent-rabbitmq\n          spring-boot-convention/service-intent-kafka\n        developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8\n        developer.conventions/target-containers: workload\n        local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8\n        services.conventions.carto.run/kafka: kafka-clients/3.4.1\n        services.conventions.carto.run/postgres: r2dbc-postgresql/1.0.2.RELEASE\n        services.conventions.carto.run/rabbitmq: amqp-client/5.17.1\n      labels:\n        apis.apps.tanzu.vmware.com/register-api: \"true\"\n        app.kubernetes.io/component: run\n        app.kubernetes.io/part-of: where-for-dinner-search\n        apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n        apps.tanzu.vmware.com/has-tests: \"true\"\n        apps.tanzu.vmware.com/workload-type: web\n        carto.run/workload-name: where-for-dinner-search\n        conventions.carto.run/framework: spring-boot\n        networking.knative.dev/visibility: cluster-local\n        services.conventions.carto.run/kafka: workload\n        services.conventions.carto.run/postgres: workload\n        services.conventions.carto.run/rabbitmq: workload\n        tanzu.app.live.view: \"true\"\n        tanzu.app.live.view.application.actuator.path: actuator\n        tanzu.app.live.view.application.actuator.port: \"8081\"\n        tanzu.app.live.view.application.flavours: spring-boot\n        tanzu.app.live.view.application.name: where-for-dinner-search\n    spec:\n      containers:\n      - env:\n        - name: JAVA_TOOL_OPTIONS\n          value: -Djava.net.preferIPv4Stack=\"true\" -Dmanagement.endpoint.health.probes.add-additional-paths=\"true\" -Dmanagement.endpoint.health.show-details=\"always\" -Dmanagement.endpoints.web.base-path=\"/actuator\" -Dmanagement.endpoints.web.exposure.include=\"*\" -Dmanagement.health.probes.enabled=\"true\" -Dmanagement.server.port=\"8081\" -Dserver.port=\"8080\"\n        - name: BPL_DEBUG_ENABLED\n          value: \"true\"\n        - name: BPL_DEBUG_PORT\n          value: \"9005\"\n        image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads@sha256:de69ce23e028e8a5384a23aac74195d24d3f45fb6f9b1d39e75d39260e3d2f49\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: 8080\n            scheme: HTTP\n        name: workload\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 1500m\n            memory: 750M\n          requests:\n            cpu: 100m\n            memory: 500M\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        startupProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n            scheme: HTTP\n      serviceAccountName: default\n","kapp-config.yml":"apiVersion: kapp.k14s.io/v1alpha1\nkind: Config\nrebaseRules:\n- path:\n  - metadata\n  - annotations\n  - serving.knative.dev/creator\n  type: copy\n  sources:\n  - new\n  - existing\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n- path:\n  - metadata\n  - annotations\n  - serving.knative.dev/lastModifier\n  type: copy\n  sources:\n  - new\n  - existing\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\nwaitRules:\n- resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n  conditionMatchers:\n  - type: Ready\n    status: \"True\"\n    success: true\n  - type: Ready\n    status: \"False\"\n    failure: true\nownershipLabelRules:\n- path:\n  - spec\n  - template\n  - metadata\n  - labels\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n","serviceclaims.yml":"apiVersion: kapp.k14s.io/v1alpha1\nkind: Config\nwaitRules:\n- conditionMatchers:\n  - type: ServiceAvailable\n    status: \"False\"\n    failure: true\n  - type: ServiceAvailable\n    status: \"True\"\n    success: true\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: servicebinding.io/v1alpha3\n      kind: ServiceBinding\n---\napiVersion: servicebinding.io/v1alpha3\nkind: ServiceBinding\nmetadata:\n  name: where-for-dinner-search-rmq\n  annotations:\n    local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8\n    autoscaling.knative.dev/minScale: \"1\"\n    kapp.k14s.io/change-group: servicebinding.io/ServiceBindings\n  labels:\n    apis.apps.tanzu.vmware.com/register-api: \"true\"\n    app.kubernetes.io/part-of: where-for-dinner-search\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search\nspec:\n  name: rmq\n  service:\n    apiVersion: services.apps.tanzu.vmware.com/v1alpha1\n    kind: ClassClaim\n    name: msgbroker-where-for-dinner\n  workload:\n    apiVersion: serving.knative.dev/v1\n    kind: Service\n    name: where-for-dinner-search\n"}
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''apiDescriptor.yml'\'') && echo '\''apiVersion: apis.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚ kind: APIDescriptor
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   type: openapi
where-for-diâ€¦ â”‚   description: Where For Dinner Search API
where-for-diâ€¦ â”‚   system: where-for-dinner
where-for-diâ€¦ â”‚   owner: where-for-dinner-team
where-for-diâ€¦ â”‚   location:
where-for-diâ€¦ â”‚     apiSpec:
where-for-diâ€¦ â”‚       path: /v3/api-docs
where-for-diâ€¦ â”‚     server:
where-for-diâ€¦ â”‚       ref:
where-for-diâ€¦ â”‚         apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚         kind: Service
where-for-diâ€¦ â”‚         name: where-for-dinner-search
where-for-diâ€¦ â”‚ '\'' > '\''apiDescriptor.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-postgres
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-rabbitmq
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-kafka
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: kafka-clients/3.4.1
where-for-diâ€¦ â”‚         services.conventions.carto.run/postgres: r2dbc-postgresql/1.0.2.RELEASE
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: amqp-client/5.17.1
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/postgres: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: workload
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner-search
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads@sha256:de69ce23e028e8a5384a23aac74195d24d3f45fb6f9b1d39e75d39260e3d2f49
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''serviceclaims.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - conditionMatchers:
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚       kind: ServiceBinding
where-for-diâ€¦ â”‚ ---
where-for-diâ€¦ â”‚ apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚ kind: ServiceBinding
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-rmq
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-group: servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   name: rmq
where-for-diâ€¦ â”‚   service:
where-for-diâ€¦ â”‚     apiVersion: services.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚     kind: ClassClaim
where-for-diâ€¦ â”‚     name: msgbroker-where-for-dinner
where-for-diâ€¦ â”‚   workload:
where-for-diâ€¦ â”‚     apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚     kind: Service
where-for-diâ€¦ â”‚     name: where-for-dinner-search
where-for-diâ€¦ â”‚ '\'' > '\''serviceclaims.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname apiDescriptor.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: apis.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚ kind: APIDescriptor
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   type: openapi
where-for-diâ€¦ â”‚   description: Where For Dinner Search API
where-for-diâ€¦ â”‚   system: where-for-dinner
where-for-diâ€¦ â”‚   owner: where-for-dinner-team
where-for-diâ€¦ â”‚   location:
where-for-diâ€¦ â”‚     apiSpec:
where-for-diâ€¦ â”‚       path: /v3/api-docs
where-for-diâ€¦ â”‚     server:
where-for-diâ€¦ â”‚       ref:
where-for-diâ€¦ â”‚         apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚         kind: Service
where-for-diâ€¦ â”‚         name: where-for-dinner-search
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-postgres
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-rabbitmq
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-kafka
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: kafka-clients/3.4.1
where-for-diâ€¦ â”‚         services.conventions.carto.run/postgres: r2dbc-postgresql/1.0.2.RELEASE
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: amqp-client/5.17.1
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/postgres: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: workload
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner-search
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads@sha256:de69ce23e028e8a5384a23aac74195d24d3f45fb6f9b1d39e75d39260e3d2f49
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname serviceclaims.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - conditionMatchers:
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚       kind: ServiceBinding
where-for-diâ€¦ â”‚ ---
where-for-diâ€¦ â”‚ apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚ kind: ServiceBinding
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-rmq
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search@sha256:77eeb7d0bb61a918947c59b49a75fb5d9127d8673e08e4d3a9f1eb2c998fb2a8
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-group: servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     apis.apps.tanzu.vmware.com/register-api: "true"
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   name: rmq
where-for-diâ€¦ â”‚   service:
where-for-diâ€¦ â”‚     apiVersion: services.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚     kind: ClassClaim
where-for-diâ€¦ â”‚     name: msgbroker-where-for-dinner
where-for-diâ€¦ â”‚   workload:
where-for-diâ€¦ â”‚     apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚     kind: Service
where-for-diâ€¦ â”‚     name: where-for-dinner-search
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-workloads-bundle:b1c84ad8-b12b-4f7d-a98d-52317e00d594 -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 3s
where-for-diâ€¦ â”‚      â”Š Completed       - 5s
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-00001-deployment-55578df97c-vsqhw):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 17963, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx309804K -XX:MaxMetaspaceSize=115415K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T10:22:10.448875928Z","logger":"queueproxy","caller":"sharedmain/main.go:260","message":"Starting queue-proxy","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-55578df97c-vsqhw"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T10:22:10.449240768Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server metrics:9090","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-55578df97c-vsqhw"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T10:22:10.457164707Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server main:8012","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-55578df97c-vsqhw"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T10:22:10.457362125Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server admin:8022","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-00001","knative.dev/pod":"where-for-dinner-search-00001-deployment-55578df97c-vsqhw"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload]   .   ____          _            __ _ _
where-for-diâ€¦ â”‚ [workload]  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
where-for-diâ€¦ â”‚ [workload] ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
where-for-diâ€¦ â”‚ [workload]  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
where-for-diâ€¦ â”‚ [workload]   '  |____| .__|_| |_|_| |_\__, | / / / /
where-for-diâ€¦ â”‚ [workload]  =========|_|==============|___/=/_/_/_/
where-for-diâ€¦ â”‚ [workload]  :: Spring Boot ::                (v3.1.3)
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:12.754Z  INFO 115 --- [           main] c.j.e.t.w.WhereForDinnerResApplication   : Starting WhereForDinnerResApplication v0.0.1-SNAPSHOT using Java 17.0.9 with PID 115 (/workspace/BOOT-INF/classes started by cnb in /workspace)
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:12.815Z  INFO 115 --- [           main] c.j.e.t.w.WhereForDinnerResApplication   : No active profile set, falling back to 1 default profile: "default"
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:13.038Z  INFO 115 --- [           main] .BindingSpecificEnvironmentPostProcessor : Creating binding-specific PropertySource from Kubernetes Service Bindings
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:17.654Z  INFO 115 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data R2DBC repositories in DEFAULT mode.
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:18.073Z  INFO 115 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 360 ms. Found 1 R2DBC repository interfaces.
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:18.572Z  INFO 115 --- [           main] faultConfiguringBeanFactoryPostProcessor : No bean named 'errorChannel' has been explicitly defined. Therefore, a default PublishSubscribeChannel will be created.
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:18.628Z  INFO 115 --- [           main] faultConfiguringBeanFactoryPostProcessor : No bean named 'integrationHeaderChannelRegistry' has been explicitly defined. Therefore, a default DefaultHeaderChannelRegistry will be created.
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:23.152Z  INFO 115 --- [           main] ctiveUserDetailsServiceAutoConfiguration : 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] Using generated security password: 74069194-a0ea-45e9-a604-29bb678a6f11
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:26.038Z  INFO 115 --- [           main] o.s.c.f.web.flux.FunctionHandlerMapping  : FunctionCatalog: org.springframework.cloud.function.context.catalog.BeanFactoryAwareFunctionRegistry@2ac9e53c
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.213Z  INFO 115 --- [           main] o.s.i.endpoint.EventDrivenConsumer       : Adding {logging-channel-adapter:_org.springframework.integration.errorLogger} as a subscriber to the 'errorChannel' channel
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.213Z  INFO 115 --- [           main] o.s.i.channel.PublishSubscribeChannel    : Channel 'where-for-dinner-search.errorChannel' has 1 subscriber(s).
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.216Z  INFO 115 --- [           main] o.s.i.endpoint.EventDrivenConsumer       : started bean '_org.springframework.integration.errorLogger'
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.216Z  INFO 115 --- [           main] o.s.i.endpoint.EventDrivenConsumer       : Adding {splitter} as a subscriber to the 'submittedSearches_integrationflow.channel#0' channel
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.217Z  INFO 115 --- [           main] o.s.integration.channel.DirectChannel    : Channel 'where-for-dinner-search.submittedSearches_integrationflow.channel#0' has 1 subscriber(s).
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.217Z  INFO 115 --- [           main] o.s.i.endpoint.EventDrivenConsumer       : started bean 'submittedSearches_integrationflow.org.springframework.integration.config.ConsumerEndpointFactoryBean#0'
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.217Z  INFO 115 --- [           main] o.s.i.endpoint.EventDrivenConsumer       : Adding {router} as a subscriber to the 'submittedSearches_integrationflow.channel#1' channel
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.217Z  INFO 115 --- [           main] o.s.integration.channel.DirectChannel    : Channel 'where-for-dinner-search.submittedSearches_integrationflow.channel#1' has 1 subscriber(s).
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.217Z  INFO 115 --- [           main] o.s.i.endpoint.EventDrivenConsumer       : started bean 'submittedSearches_integrationflow.org.springframework.integration.config.ConsumerEndpointFactoryBean#1'
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.219Z  INFO 115 --- [           main] o.s.c.s.binder.DefaultBinderFactory      : Creating binder: rabbit
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.221Z  INFO 115 --- [           main] o.s.c.s.binder.DefaultBinderFactory      : Constructing binder child context for rabbit
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.414Z  INFO 115 --- [           main] o.s.c.s.binder.DefaultBinderFactory      : Caching the binder: rabbit
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.434Z  INFO 115 --- [           main] o.s.a.r.c.CachingConnectionFactory       : Attempting to connect to: [msgbroker-where-for-dinner.service-instances.svc:5672]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.638Z  INFO 115 --- [           main] o.s.a.r.c.CachingConnectionFactory       : Created new connection: rabbitConnectionFactory#3df04fa1:0/SimpleConnection@570b85dd [delegate=amqp://default_user_zB7KCw1r2aJ8u7icyMW@10.100.200.203:5672/, localPort=40406]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.734Z  INFO 115 --- [           main] o.s.c.s.m.DirectWithAttributesChannel    : Channel 'where-for-dinner-search.submittedSearches-out-0' has 1 subscriber(s).
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.742Z  INFO 115 --- [           main] o.s.i.e.SourcePollingChannelAdapter      : started bean 'submittedSearches-out-0_spca'
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:30.745Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:31.430Z  INFO 115 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8080
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:31.640Z  INFO 115 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 17 endpoint(s) beneath base path '/actuator'
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:31.823Z  INFO 115 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8081
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:31.956Z  INFO 115 --- [           main] c.j.e.t.w.WhereForDinnerResApplication   : Started WhereForDinnerResApplication in 20.44 seconds (process running for 21.397)
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”Š Ready           - 1m1s
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:44.439Z  INFO 115 --- [     parallel-1] o.springdoc.api.AbstractOpenApiResource  : Init duration for springdoc-openapi is: 1323 ms
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:24:31.222Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:26:31.229Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:28:31.258Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:30:31.273Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:32:31.311Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:34:31.329Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:36:31.342Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:38:31.354Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:40:31.365Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:42:31.377Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:44:31.385Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:46:31.397Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:48:31.408Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:50:31.479Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:52:31.482Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:54:31.530Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:56:31.534Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:58:31.540Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:00:31.552Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:02:31.559Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:04:31.571Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:06:31.575Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:08:31.628Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:10:31.663Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:12:31.706Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:14:31.751Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:16:31.798Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:18:31.837Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:20:31.879Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:22:31.917Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:24:31.957Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:26:32.012Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:28:32.056Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:30:32.096Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:32:32.137Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:34:32.177Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:36:32.185Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:38:32.232Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:40:32.279Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:42:32.321Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:44:32.366Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:46:32.410Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:48:32.483Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:50:32.528Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:52:32.575Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:54:32.616Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:56:32.664Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T11:58:32.715Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:00:32.781Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:02:32.829Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:04:32.877Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:06:32.927Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:08:32.971Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:10:33.042Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:12:33.087Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:14:33.136Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:16:33.181Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:18:33.226Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:20:33.270Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:22:33.285Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:24:33.364Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:26:33.409Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:28:33.422Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:30:33.490Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:32:33.535Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:34:33.580Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:36:33.622Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:38:33.628Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:40:33.645Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:42:33.664Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:44:33.711Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:46:33.726Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:48:33.742Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:50:33.759Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:52:33.801Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:54:33.809Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:56:33.852Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T12:58:33.856Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T13:00:33.863Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T13:02:33.879Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T13:04:33.893Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T13:06:33.941Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T13:08:33.986Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T13:10:34.000Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T13:12:34.006Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T13:14:34.027Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T13:16:34.043Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T13:18:34.054Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
where-for-diâ€¦ â”‚ [workload] 2024-01-04T13:20:34.073Z  INFO 115 --- [   scheduling-1] c.j.e.t.w.functions.SearchSupplier       : Gathering and sending all active searches to downstream processing
