Tilt started on http://localhost:51311/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-api-gateway/Tiltfile
Successfully loaded Tiltfile (740.323Âµs)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Error: Writing 'local-source-proxy.tap-local-source-system.svc.cluster.local/source:workloads-where-for-dinner': Writing image: HEAD https://tap-full.eng.vmware.com:8443/api/v1/namespaces/tap-local-source-system/services/http:local-source-proxy:5001/proxy/v2/source/blobs/sha256:5a94465ed7ea35303703a92e0e26363b2d110121815b13adf6df5cf6d7791ddd: unexpected status code 401 Unauthorized (HEAD responses have no body, use GET for details)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ ERROR: Build Failed: apply command exited with status 1Tilt started on http://localhost:51311/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-api-gateway/Tiltfile
Successfully loaded Tiltfile (727.356Âµs)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace default --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 5.08s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 5.08s 
where-for-diâ€¦ â”‚ 
Tilt started on http://localhost:51312/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-api-gateway/Tiltfile
Successfully loaded Tiltfile (841.209Âµs)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 4.90s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 4.90s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-build-1-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32menv:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_OCI_SOURCE[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_LIVE_RELOAD_ENABLED[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "true"[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_JVM_VERSION[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "17"[0m
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31msource: {}[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32msource:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  blob:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Pulling image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Successfully pulled image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" in 595.96679ms (596.030629ms including waiting)
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer started at 2024-01-03T09:33:04Z
where-for-diâ€¦ â”‚ [analyze] Restoring data for SBOM from previous image
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer ran for 7.769086837s and ended at 2024-01-03T09:33:12Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/dotnet-core-lite@2.9.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/go-lite@2.2.3' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/nodejs-lite@2.4.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] Timer: Detector started at 2024-01-03T09:33:13Z
where-for-diâ€¦ â”‚ [detect] 11 of 26 buildpacks participating
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/ca-certificates   3.6.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/bellsoft-liberica 9.13.0
where-for-diâ€¦ â”‚ [detect] paketo-buildpacks/syft             1.10.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/maven             6.15.12
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/watchexec         2.8.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/executable-jar    6.8.3
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/apache-tomcat     7.14.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/liberty           3.8.11
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/dist-zip          5.6.8
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/spring-boot       5.27.6
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/image-labels      4.5.6
where-for-diâ€¦ â”‚ [detect] Timer: Detector ran for 2.509358522s and ended at 2024-01-03T09:33:15Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [restore] [33;1mWarning: [0mNo cached data will be used, no cache specified.
where-for-diâ€¦ â”‚ [restore] Timer: Restorer started at 2024-01-03T09:33:16Z
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/ca-certificates:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:jre" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:java-security-properties" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:spring-cloud-bindings" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:web-application-type" from app image
where-for-diâ€¦ â”‚ [restore] Timer: Restorer ran for 6.489546ms and ended at 2024-01-03T09:33:16Z
where-for-diâ€¦ â”‚ [build] Timer: Builder started at 2024-01-03T09:33:17Z
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for CA Certificates[0m[34m 3.6.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-ca-certificates[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for BellSoft Liberica[0m[34m 9.13.0[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-bellsoft-liberica[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ARGS           --no-man-pages --no-header-files --strip-debug --compress=1  configure custom link arguments (--output must be omitted)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ENABLED        false                                                        enables running jlink tool to generate custom JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_TYPE                 JRE                                                          the JVM type - JDK or JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_VERSION              17                                                           the Java version[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_ENABLED           false                                                        enables Java remote debugging support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_PORT              8000                                                         configure the remote debugging port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_SUSPEND           false                                                        configure whether to suspend execution until a debugger has attached[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_HEAP_DUMP_PATH                                                                       write heap dumps on error to this path[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_ENABLED        true                                                         enables Java Native Memory Tracking (NMT)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_LEVEL          summary                                                      configure level of NMT, summary or detail[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ARGS                                                                             configure custom Java Flight Recording (JFR) arguments[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ENABLED             false                                                        enables Java Flight Recording (JFR)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_ENABLED             false                                                        enables Java Management Extensions (JMX)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_PORT                5000                                                         configure the JMX port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_HEAD_ROOM           0                                                            the headroom in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_LOADED_CLASS_COUNT  35% of classes                                               the number of loaded classes in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_THREAD_COUNT        250                                                          the number of threads in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $JAVA_TOOL_OPTIONS                                                                        the JVM launch flags[0m
where-for-diâ€¦ â”‚ [build] [2m    [2mUsing Java version 17 from BP_JVM_VERSION[0m[2m[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JDK 17.0.9[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/bell-sw/Liberica/releases/download/17.0.9+11/bellsoft-jdk17.0.9+11-linux-amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Expanding to /layers/tanzu-buildpacks_bellsoft-liberica/jdk[0m
where-for-diâ€¦ â”‚ [build] [2m    Adding 137 container CA certificates to JVM truststore[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JAVA_HOME.override[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JDK_HOME.override[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JRE 17.0.9[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mJava Security Properties[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mPaketo Syft Buildpack[0m[34m 1.10.1[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/paketo-buildpacks/syft[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/anchore/syft/releases/download/v0.41.1/syft_0.41.1_linux_amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/SYFT_CHECK_FOR_APP_UPDATE.default[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Maven[0m[34m 6.15.12[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-maven[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_EXCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are removed[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_INCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are included[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JAVA_INSTALL_NODE                 false                                                  whether to install Yarn/Node binaries based on the presence of a package.json or yarn.lock file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ACTIVE_PROFILES                                                                    the active profiles (comma separated: such as: p1,!p2,?p3) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ADDITIONAL_BUILD_ARGUMENTS                                                         the additionnal arguments (appended to BP_MAVEN_BUILD_ARGUMENTS) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILD_ARGUMENTS             -Dmaven.test.skip=true --no-transfer-progress package  the arguments to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_ARTIFACT              target/*.[ejw]ar                                       the built application artifact explicitly.  Supersedes $BP_MAVEN_BUILT_MODULE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_MODULE                                                                       the module to find application artifact in[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_DAEMON_ENABLED              false                                                  use maven daemon[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_POM_FILE                    pom.xml                                                the location of the main pom.xml file, relative to the application root[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_SETTINGS_PATH                                                                      the path to a Maven settings file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_VERSION                     3                                                      the Maven version[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_NODE_PROJECT_PATH                                                                        configure a project subdirectory to look for `package.json` and `yarn.lock` files[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating cache directory /home/cnb/.m2[0m
where-for-diâ€¦ â”‚ [build]   [34mCompiled Application[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Executing mvnw --batch-mode -Dmaven.test.skip=true --no-transfer-progress package[0m
where-for-diâ€¦ â”‚ [build]         % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
where-for-diâ€¦ â”‚ [build]                                        Dload  Upload   Total   Spent    Left  Speed
where-for-diâ€¦ â”‚ [build]         0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 59925  100 59925    0     0   173k      0 --:--:-- --:--:-- --:--:--  174k
where-for-diâ€¦ â”‚ [build]       [0m[0m[INFO] Scanning for projects...
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --------------< com.example:where-for-dinner-api-gateway >--------------
where-for-diâ€¦ â”‚ [build]       [INFO] Building where-for-dinner-api-gateway 0.0.1-SNAPSHOT
where-for-diâ€¦ â”‚ [build]       [INFO] --------------------------------[ jar ]---------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 1 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 0 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Changes detected - recompiling the module! :dependency
where-for-diâ€¦ â”‚ [build]       [INFO] Compiling 1 source file with javac [debug release 17] to target/classes
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[40,17] httpBasic() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[41,13] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[42,13] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[60,12] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[63,12] and() in org.springframework.security.config.web.server.ServerHttpSecurity.AuthorizeExchangeSpec has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[65,8] logout() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[70,12] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:testResources (default-testResources) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not copying test resources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:testCompile (default-testCompile) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not compiling test sources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-surefire-plugin:3.0.0:test (default-test) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Tests are skipped.
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-jar-plugin:3.3.0:jar (default-jar) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Building jar: /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- spring-boot-maven-plugin:3.1.3:repackage (repackage) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Replacing main artifact /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar with repackaged archive, adding nested dependencies in BOOT-INF/.
where-for-diâ€¦ â”‚ [build]       [INFO] The original artifact has been renamed to /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar.original
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] BUILD SUCCESS
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] Total time:  53.078 s
where-for-diâ€¦ â”‚ [build]       [INFO] Finished at: 2024-01-03T09:34:28Z
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [0m[0m
where-for-diâ€¦ â”‚ [build]   Removing source code
where-for-diâ€¦ â”‚ [build]   Restoring application artifact
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Watchexec[0m[34m 2.8.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-watchexec[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/watchexec/watchexec/releases/download/cli-v1.20.5/watchexec-1.20.5-x86_64-unknown-linux-musl.tar.xz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Executable JAR[0m[34m 6.8.3[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-executable-jar[0m
where-for-diâ€¦ â”‚ [build]   [34mClass Path[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.delim[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.prepend[0m
where-for-diâ€¦ â”‚ [build]   Process types:
where-for-diâ€¦ â”‚ [build]     [36mexecutable-jar[0m: java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mreload[0m:         watchexec -r --shell=none -- java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mtask[0m:           java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mweb[0m:            java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build] SKIPPED: `Main-Class` found in `META-INF/MANIFEST.MF`, skipping build
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Spring Boot[0m[34m 5.27.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-spring-boot[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_DISABLED   false  whether to contribute Spring Boot cloud bindings support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_VERSION    1      default version of Spring Cloud Bindings library to contribute[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_DISABLED  false  whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_ENABLED   true   Deprecated - whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build]   Creating slices from layers index
where-for-diâ€¦ â”‚ [build] [2m    dependencies (74.3 MB)[0m
where-for-diâ€¦ â”‚ [build] [2m    spring-boot-loader (269.4 KB)[0m
where-for-diâ€¦ â”‚ [build] [2m    snapshot-dependencies (0.0 B)[0m
where-for-diâ€¦ â”‚ [build] [2m    application (59.1 KB)[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mSpring Cloud Bindings 2.0.2[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-bindings/2.0.2/spring-cloud-bindings-2.0.2.jar[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Copying to /layers/tanzu-buildpacks_spring-boot/spring-cloud-bindings[0m
where-for-diâ€¦ â”‚ [build]   [34mWeb Application Type[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Reactive web application detected[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/BPL_JVM_THREAD_COUNT.default[0m
where-for-diâ€¦ â”‚ [build]   4 application slices
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.title
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.version
where-for-diâ€¦ â”‚ [build]     org.springframework.boot.version
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Image Labels[0m[34m 4.5.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-image-labels[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_IMAGE_LABELS                                                                                                                                                    arbitrary image labels[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_AUTHORS                                                                                                                                                     the org.opencontainers.image.authors image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_CREATED                                                                                                                                                     the org.opencontainers.image.created image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DESCRIPTION                                                                                                                                                 the org.opencontainers.image.description image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DOCUMENTATION                                                                                                                                               the org.opencontainers.image.documentation image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_LICENSES                                                                                                                                                    the org.opencontainers.image.licenses image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REF_NAME                                                                                                                                                    the org.opencontainers.image.ref.name image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REVISION                                                                                                                                                    the org.opencontainers.image.revision image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_SOURCE         tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb  the org.opencontainers.image.source image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_TITLE                                                                                                                                                       the org.opencontainers.image.title image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_URL                                                                                                                                                         the org.opencontainers.image.url image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VENDOR                                                                                                                                                      the org.opencontainers.image.vendor image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VERSION                                                                                                                                                     the org.opencontainers.image.version image label[0m
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.source
where-for-diâ€¦ â”‚ [build] Timer: Builder ran for 1m25.478732379s and ended at 2024-01-03T09:34:42Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [export] [33;1mWarning: [0mNo cached data will be used, no cache specified.
where-for-diâ€¦ â”‚ [export] Reusing layers from image 'tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:06c1d79581aee906e389b3b5a3d26ef96e1fef7788ec9d9e61282961b5d45dbc'
where-for-diâ€¦ â”‚ [export] Timer: Exporter started at 2024-01-03T09:36:20Z
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/ca-certificates:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:java-security-properties'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:jre'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/watchexec:watchexec'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/executable-jar:classpath'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:spring-cloud-bindings'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:web-application-type'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:launch.sbom'
where-for-diâ€¦ â”‚ [export] Reusing 2/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Adding 3/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:launcher'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:config'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:process-types'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.lifecycle.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.build.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.project.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.title'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.springframework.boot.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.source'
where-for-diâ€¦ â”‚ [export] Setting default process type 'reload'
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads... started at 2024-01-03T09:36:21Z
where-for-diâ€¦ â”‚ [export] *** Images (sha256:d2c271771f5f9e249ff716a7d4f58154b86f252b509614a9e2b920d30075e7af):
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads:b1.20240103.093150
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads... ran for 7.202599376s and ended at 2024-01-03T09:36:29Z
where-for-diâ€¦ â”‚ [export] Timer: Exporter ran for 8.948813216s and ended at 2024-01-03T09:36:29Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:0b5c1bfe87a700cf473725e7cd133f4c7da48ec85baccf2d0c5d6127beb675e7" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 4m37s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ Build successful
where-for-diâ€¦ â”‚ [event: taskrun workloads/where-for-dinner-config-writer-px86j] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-config-writer-px86j-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-px86j-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/03 09:36:57 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-px86j-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-px86j-pod] Pulling image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad"
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/03 09:36:58 Decoded script /tekton/scripts/script-0-zm4rz
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-px86j-pod] Successfully pulled image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" in 30.958901671s (30.959044937s including waiting)
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 3s
where-for-diâ€¦ â”‚      â”Š Ready           - 32s
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.RUAWA9jDqG
where-for-diâ€¦ â”‚ + base64 --decode
where-for-diâ€¦ â”‚ + echo -e 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
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:d2c271771f5f9e249ff716a7d4f58154b86f252b509614a9e2b920d30075e7af
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:d2c271771f5f9e249ff716a7d4f58154b86f252b509614a9e2b920d30075e7af
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads-bundle:c9744df2-0aef-4de3-92d7-a5828cc1bc18 -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-00001-deployment-5996fd764b-dfcxc):
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:38:09.320787652Z","logger":"queueproxy","caller":"sharedmain/main.go:260","message":"Starting queue-proxy","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:38:09.321781235Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server metrics:9090","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:38:09.353113101Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server main:8012","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:38:09.353174461Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server admin:8022","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:38:32.160969592Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc00074ebe0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:39:02.159599327Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc0007620a0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 193 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 192 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:39:32.159256122Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc0006ff450}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:40:02.160075297Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc000883c20}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:40:32.1605947Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc0006f7590}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [workload queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:41:42.157533049Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc00078fe00}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 192 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:43:32.158856692Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc0006f76d0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:46:52.16649748Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc0006914f0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:52:32.159436947Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc000697310}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:53:02.157051531Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc000462460}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:58:32.158322029Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc00071ecd0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 193 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:59:02.16049037Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc00086b040}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:04:32.162487392Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc0006585f0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:05:02.161162696Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc000412b40}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:10:42.170148574Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc000869680}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:11:12.157401514Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc00060ecd0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 137 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 164 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:16:52.158669616Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc00086bd60}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:17:22.157011989Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc0006b0b40}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:22:52.161430605Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc00086bbd0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:23:22.158761188Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc000719220}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:28:52.158590749Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc00079d540}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:29:22.159422211Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc0007ccd20}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 182 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 192 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 190 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 141 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 171 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 169 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 156 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 190 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 193 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 184 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:34:52.158297747Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc0005a0870}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:35:22.161452063Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc00076ec80}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 181 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 191 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:40:52.161302505Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc0005e62d0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:41:22.157826482Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc00074fdb0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:46:52.158281701Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc0008678b0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-5996fd764b-dfcxc. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:47:22.166598258Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.41:8022 map[] map[] <nil> map[] 10.200.1.1:51416 /wait-for-drain <nil> <nil> <nil> 0xc00070d180}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-5996fd764b-dfcxc"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition

1 File Changed: [Tiltfile]
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-api-gateway/Tiltfile
Successfully loaded Tiltfile (731.167Âµs)

1 File Changed: [Tiltfile]
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-api-gateway/Tiltfile
Successfully loaded Tiltfile (557.705Âµs)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ 1 File Changed: [Tiltfile]
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace default --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ 1 File Changed: [Tiltfile]
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 4.79s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 4.79s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 5.14s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 5.14s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [event: taskrun workloads/where-for-dinner-config-writer-4sz7s] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-config-writer-4sz7s-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-4sz7s-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/03 10:51:45 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-4sz7s-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-4sz7s-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/03 10:51:46 Decoded script /tekton/scripts/script-0-rcjpd
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-build-2-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.2Dz4Yxsa5K
where-for-diâ€¦ â”‚ + echo -e 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 base64 --decode
where-for-diâ€¦ â”‚ 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
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/carvel-package-workflow: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:d2c271771f5f9e249ff716a7d4f58154b86f252b509614a9e2b920d30075e7af
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/carvel-package-workflow: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:d2c271771f5f9e249ff716a7d4f58154b86f252b509614a9e2b920d30075e7af
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads-bundle:c9744df2-0aef-4de3-92d7-a5828cc1bc18 -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [event: taskrun workloads/where-for-dinner-config-writer-phkz4] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-config-writer-phkz4-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-2-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-phkz4-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	env:
where-for-diâ€¦ â”‚ [prepare] 	- name: BP_OCI_SOURCE
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:d6a128e4fb73b1f117b5024ef6c15f389fc980108013793bd13cfdc4d95be8a3[0m
where-for-diâ€¦ â”‚ [prepare] 	- name: BP_LIVE_RELOAD_ENABLED
where-for-diâ€¦ â”‚ [prepare] 	  value: "true"
where-for-diâ€¦ â”‚ [prepare] 	- name: BP_JVM_VERSION
where-for-diâ€¦ â”‚ [prepare] 	  value: "17"
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	source:
where-for-diâ€¦ â”‚ [prepare] 	  blob:
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/d6a128e4fb73b1f117b5024ef6c15f389fc980108013793bd13cfdc4d95be8a3.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] 2024/01/03 10:52:00 Entrypoint initialization
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-phkz4-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/d6a128e4fb73b1f117b5024ef6c15f389fc980108013793bd13cfdc4d95be8a3.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/d6a128e4fb73b1f117b5024ef6c15f389fc980108013793bd13cfdc4d95be8a3.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-2-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-phkz4-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/03 10:52:01 Decoded script /tekton/scripts/script-0-lpp9g
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 4s
where-for-diâ€¦ â”‚      â”Š Ready           - 1s
Tilt started on http://localhost:51311/
v0.33.10, built 2023-12-15
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.vyPzH7vrXH
where-for-diâ€¦ â”‚ + base64 --decode
where-for-diâ€¦ â”‚ + echo -e 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
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/carvel-package-workflow: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/target: "200"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:d6a128e4fb73b1f117b5024ef6c15f389fc980108013793bd13cfdc4d95be8a3
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/carvel-package-workflow: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:d2c271771f5f9e249ff716a7d4f58154b86f252b509614a9e2b920d30075e7af
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/carvel-package-workflow: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/target: "200"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:082adf4b5da1d8d574d4fcc93e46bd457fc7e366f1a1a247fe1f72ff49cb16eb
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:d6a128e4fb73b1f117b5024ef6c15f389fc980108013793bd13cfdc4d95be8a3
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/carvel-package-workflow: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:d2c271771f5f9e249ff716a7d4f58154b86f252b509614a9e2b920d30075e7af
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads-bundle:c9744df2-0aef-4de3-92d7-a5828cc1bc18 -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
Error: Tilt cannot start because you already have another process on port 51311
If you want to run multiple Tilt instances simultaneously,
use the --port flag or TILT_PORT env variable to set a custom port
Original error: listen tcp 127.0.0.1:51311: bind: address already in use
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
Tilt started on http://localhost:51311/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-api-gateway/Tiltfile
Successfully loaded Tiltfile (1.089129ms)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads3 --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 5.38s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 5.38s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-build-1-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32menv:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_OCI_SOURCE[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner@sha256:05a0fc5cb0d74853a265fe33687c0e0760c1e36edcecc993a00bd68657f14e9c[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_LIVE_RELOAD_ENABLED[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "true"[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_JVM_VERSION[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "17"[0m
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31msource: {}[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32msource:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  blob:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads3/where-for-dinner/05a0fc5cb0d74853a265fe33687c0e0760c1e36edcecc993a00bd68657f14e9c.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads3/where-for-dinner/05a0fc5cb0d74853a265fe33687c0e0760c1e36edcecc993a00bd68657f14e9c.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads3/where-for-dinner/05a0fc5cb0d74853a265fe33687c0e0760c1e36edcecc993a00bd68657f14e9c.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [analyze detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer started at 2024-01-03T10:54:09Z
where-for-diâ€¦ â”‚ [analyze] Image with name "tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3" not found
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer ran for 2.147457131s and ended at 2024-01-03T10:54:11Z
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/dotnet-core-lite@2.9.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/go-lite@2.2.3' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/nodejs-lite@2.4.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] Timer: Detector started at 2024-01-03T10:54:13Z
where-for-diâ€¦ â”‚ [detect] 11 of 26 buildpacks participating
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/ca-certificates   3.6.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/bellsoft-liberica 9.13.0
where-for-diâ€¦ â”‚ [detect] paketo-buildpacks/syft             1.10.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/maven             6.15.12
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/watchexec         2.8.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/executable-jar    6.8.3
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/apache-tomcat     7.14.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/liberty           3.8.11
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/dist-zip          5.6.8
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/spring-boot       5.27.6
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/image-labels      4.5.6
where-for-diâ€¦ â”‚ [detect] Timer: Detector ran for 2.737932791s and ended at 2024-01-03T10:54:15Z
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [restore] Timer: Restorer started at 2024-01-03T10:54:16Z
where-for-diâ€¦ â”‚ [restore] Timer: Restorer ran for 925.995Âµs and ended at 2024-01-03T10:54:16Z
where-for-diâ€¦ â”‚ [build] Timer: Builder started at 2024-01-03T10:54:17Z
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for CA Certificates[0m[34m 3.6.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-ca-certificates[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_ca-certificates/helper/exec.d/ca-certificates-helper[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for BellSoft Liberica[0m[34m 9.13.0[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-bellsoft-liberica[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ARGS           --no-man-pages --no-header-files --strip-debug --compress=1  configure custom link arguments (--output must be omitted)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ENABLED        false                                                        enables running jlink tool to generate custom JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_TYPE                 JRE                                                          the JVM type - JDK or JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_VERSION              17                                                           the Java version[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_ENABLED           false                                                        enables Java remote debugging support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_PORT              8000                                                         configure the remote debugging port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_SUSPEND           false                                                        configure whether to suspend execution until a debugger has attached[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_HEAP_DUMP_PATH                                                                       write heap dumps on error to this path[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_ENABLED        true                                                         enables Java Native Memory Tracking (NMT)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_LEVEL          summary                                                      configure level of NMT, summary or detail[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ARGS                                                                             configure custom Java Flight Recording (JFR) arguments[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ENABLED             false                                                        enables Java Flight Recording (JFR)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_ENABLED             false                                                        enables Java Management Extensions (JMX)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_PORT                5000                                                         configure the JMX port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_HEAD_ROOM           0                                                            the headroom in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_LOADED_CLASS_COUNT  35% of classes                                               the number of loaded classes in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_THREAD_COUNT        250                                                          the number of threads in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $JAVA_TOOL_OPTIONS                                                                        the JVM launch flags[0m
where-for-diâ€¦ â”‚ [build] [2m    [2mUsing Java version 17 from BP_JVM_VERSION[0m[2m[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JDK 17.0.9[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/bell-sw/Liberica/releases/download/17.0.9+11/bellsoft-jdk17.0.9+11-linux-amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Expanding to /layers/tanzu-buildpacks_bellsoft-liberica/jdk[0m
where-for-diâ€¦ â”‚ [build] [2m    Adding 137 container CA certificates to JVM truststore[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JAVA_HOME.override[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JDK_HOME.override[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JRE 17.0.9[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/bell-sw/Liberica/releases/download/17.0.9+11/bellsoft-jre17.0.9+11-linux-amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Expanding to /layers/tanzu-buildpacks_bellsoft-liberica/jre[0m
where-for-diâ€¦ â”‚ [build] [2m    Adding 137 container CA certificates to JVM truststore[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/BPI_APPLICATION_PATH.default[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/BPI_JVM_CACERTS.default[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/BPI_JVM_CLASS_COUNT.default[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/BPI_JVM_SECURITY_PROVIDERS.default[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/JAVA_HOME.default[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/JAVA_TOOL_OPTIONS.append[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/JAVA_TOOL_OPTIONS.delim[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/MALLOC_ARENA_MAX.default[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_bellsoft-liberica/helper/exec.d/active-processor-count[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_bellsoft-liberica/helper/exec.d/java-opts[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_bellsoft-liberica/helper/exec.d/jvm-heap[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_bellsoft-liberica/helper/exec.d/link-local-dns[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_bellsoft-liberica/helper/exec.d/memory-calculator[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_bellsoft-liberica/helper/exec.d/security-providers-configurer[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_bellsoft-liberica/helper/exec.d/jmx[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_bellsoft-liberica/helper/exec.d/jfr[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_bellsoft-liberica/helper/exec.d/openssl-certificate-loader[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_bellsoft-liberica/helper/exec.d/security-providers-classpath-9[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_bellsoft-liberica/helper/exec.d/debug-9[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_bellsoft-liberica/helper/exec.d/nmt[0m
where-for-diâ€¦ â”‚ [build]   [34mJava Security Properties[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/JAVA_SECURITY_PROPERTIES.default[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/JAVA_TOOL_OPTIONS.append[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/JAVA_TOOL_OPTIONS.delim[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mPaketo Syft Buildpack[0m[34m 1.10.1[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/paketo-buildpacks/syft[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/anchore/syft/releases/download/v0.41.1/syft_0.41.1_linux_amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/SYFT_CHECK_FOR_APP_UPDATE.default[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Maven[0m[34m 6.15.12[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-maven[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_EXCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are removed[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_INCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are included[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JAVA_INSTALL_NODE                 false                                                  whether to install Yarn/Node binaries based on the presence of a package.json or yarn.lock file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ACTIVE_PROFILES                                                                    the active profiles (comma separated: such as: p1,!p2,?p3) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ADDITIONAL_BUILD_ARGUMENTS                                                         the additionnal arguments (appended to BP_MAVEN_BUILD_ARGUMENTS) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILD_ARGUMENTS             -Dmaven.test.skip=true --no-transfer-progress package  the arguments to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_ARTIFACT              target/*.[ejw]ar                                       the built application artifact explicitly.  Supersedes $BP_MAVEN_BUILT_MODULE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_MODULE                                                                       the module to find application artifact in[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_DAEMON_ENABLED              false                                                  use maven daemon[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_POM_FILE                    pom.xml                                                the location of the main pom.xml file, relative to the application root[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_SETTINGS_PATH                                                                      the path to a Maven settings file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_VERSION                     3                                                      the Maven version[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_NODE_PROJECT_PATH                                                                        configure a project subdirectory to look for `package.json` and `yarn.lock` files[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating cache directory /home/cnb/.m2[0m
where-for-diâ€¦ â”‚ [build]   [34mCompiled Application[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Executing mvnw --batch-mode -Dmaven.test.skip=true --no-transfer-progress package[0m
where-for-diâ€¦ â”‚ [build]         % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
where-for-diâ€¦ â”‚ [build]                                        Dload  Upload   Total   Spent    Left  Speed
where-for-diâ€¦ â”‚ [build]         0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 59925  100 59925    0     0   240k      0 --:--:-- --:--:-- --:--:--  240k
where-for-diâ€¦ â”‚ [build]       [0m[0m[INFO] Scanning for projects...
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --------------< com.example:where-for-dinner-api-gateway >--------------
where-for-diâ€¦ â”‚ [build]       [INFO] Building where-for-dinner-api-gateway 0.0.1-SNAPSHOT
where-for-diâ€¦ â”‚ [build]       [INFO] --------------------------------[ jar ]---------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 1 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 0 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Changes detected - recompiling the module! :dependency
where-for-diâ€¦ â”‚ [build]       [INFO] Compiling 1 source file with javac [debug release 17] to target/classes
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[40,17] httpBasic() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[41,13] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[42,13] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[60,12] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[63,12] and() in org.springframework.security.config.web.server.ServerHttpSecurity.AuthorizeExchangeSpec has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[65,8] logout() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[70,12] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:testResources (default-testResources) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not copying test resources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:testCompile (default-testCompile) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not compiling test sources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-surefire-plugin:3.0.0:test (default-test) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Tests are skipped.
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-jar-plugin:3.3.0:jar (default-jar) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Building jar: /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- spring-boot-maven-plugin:3.1.3:repackage (repackage) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Replacing main artifact /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar with repackaged archive, adding nested dependencies in BOOT-INF/.
where-for-diâ€¦ â”‚ [build]       [INFO] The original artifact has been renamed to /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar.original
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] BUILD SUCCESS
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] Total time:  46.619 s
where-for-diâ€¦ â”‚ [build]       [INFO] Finished at: 2024-01-03T10:55:24Z
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [0m[0m
where-for-diâ€¦ â”‚ [build]   Removing source code
where-for-diâ€¦ â”‚ [build]   Restoring application artifact
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Watchexec[0m[34m 2.8.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-watchexec[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/watchexec/watchexec/releases/download/cli-v1.20.5/watchexec-1.20.5-x86_64-unknown-linux-musl.tar.xz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Executable JAR[0m[34m 6.8.3[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-executable-jar[0m
where-for-diâ€¦ â”‚ [build]   [34mClass Path[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.delim[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.prepend[0m
where-for-diâ€¦ â”‚ [build]   Process types:
where-for-diâ€¦ â”‚ [build]     [36mexecutable-jar[0m: java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mreload[0m:         watchexec -r --shell=none -- java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mtask[0m:           java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mweb[0m:            java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build] SKIPPED: `Main-Class` found in `META-INF/MANIFEST.MF`, skipping build
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Spring Boot[0m[34m 5.27.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-spring-boot[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_DISABLED   false  whether to contribute Spring Boot cloud bindings support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_VERSION    1      default version of Spring Cloud Bindings library to contribute[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_DISABLED  false  whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_ENABLED   true   Deprecated - whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build]   Creating slices from layers index
where-for-diâ€¦ â”‚ [build] [2m    dependencies (74.3 MB)[0m
where-for-diâ€¦ â”‚ [build] [2m    spring-boot-loader (269.4 KB)[0m
where-for-diâ€¦ â”‚ [build] [2m    snapshot-dependencies (0.0 B)[0m
where-for-diâ€¦ â”‚ [build] [2m    application (59.1 KB)[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Creating /layers/tanzu-buildpacks_spring-boot/helper/exec.d/spring-cloud-bindings[0m
where-for-diâ€¦ â”‚ [build]   [34mSpring Cloud Bindings 2.0.2[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-bindings/2.0.2/spring-cloud-bindings-2.0.2.jar[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Copying to /layers/tanzu-buildpacks_spring-boot/spring-cloud-bindings[0m
where-for-diâ€¦ â”‚ [build]   [34mWeb Application Type[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Reactive web application detected[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/BPL_JVM_THREAD_COUNT.default[0m
where-for-diâ€¦ â”‚ [build]   4 application slices
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.title
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.version
where-for-diâ€¦ â”‚ [build]     org.springframework.boot.version
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Image Labels[0m[34m 4.5.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-image-labels[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_IMAGE_LABELS                                                                                                                                                     arbitrary image labels[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_AUTHORS                                                                                                                                                      the org.opencontainers.image.authors image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_CREATED                                                                                                                                                      the org.opencontainers.image.created image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DESCRIPTION                                                                                                                                                  the org.opencontainers.image.description image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DOCUMENTATION                                                                                                                                                the org.opencontainers.image.documentation image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_LICENSES                                                                                                                                                     the org.opencontainers.image.licenses image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REF_NAME                                                                                                                                                     the org.opencontainers.image.ref.name image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REVISION                                                                                                                                                     the org.opencontainers.image.revision image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_SOURCE         tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner@sha256:05a0fc5cb0d74853a265fe33687c0e0760c1e36edcecc993a00bd68657f14e9c  the org.opencontainers.image.source image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_TITLE                                                                                                                                                        the org.opencontainers.image.title image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_URL                                                                                                                                                          the org.opencontainers.image.url image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VENDOR                                                                                                                                                       the org.opencontainers.image.vendor image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VERSION                                                                                                                                                      the org.opencontainers.image.version image label[0m
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.source
where-for-diâ€¦ â”‚ [build] Timer: Builder ran for 1m12.587380676s and ended at 2024-01-03T10:55:30Z
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [export] Timer: Exporter started at 2024-01-03T10:56:35Z
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/ca-certificates:helper'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/bellsoft-liberica:helper'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/bellsoft-liberica:java-security-properties'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/bellsoft-liberica:jre'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/watchexec:watchexec'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/executable-jar:classpath'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/spring-boot:helper'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/spring-boot:spring-cloud-bindings'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/spring-boot:web-application-type'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:launch.sbom'
where-for-diâ€¦ â”‚ [export] Adding 5/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:launcher'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:config'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:process-types'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.lifecycle.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.build.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.project.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.title'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.springframework.boot.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.source'
where-for-diâ€¦ â”‚ [export] Setting default process type 'reload'
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3... started at 2024-01-03T10:56:41Z
where-for-diâ€¦ â”‚ [export] *** Images (sha256:26e9dc33923dcfb3e2621fa8a022dec5ded853b4d5f7ede0cac4b899acfd8147):
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3:b1.20240103.105252
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3... ran for 18.044011875s and ended at 2024-01-03T10:56:59Z
where-for-diâ€¦ â”‚ [export] Timer: Exporter ran for 24.707369838s and ended at 2024-01-03T10:56:59Z
where-for-diâ€¦ â”‚ [export] Timer: Cache started at 2024-01-03T10:56:59Z
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/bellsoft-liberica:jdk'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'paketo-buildpacks/syft:syft'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:application'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:cache'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'buildpacksio/lifecycle:cache.sbom'
where-for-diâ€¦ â”‚ [export] Timer: Cache ran for 14.734528687s and ended at 2024-01-03T10:57:14Z
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:0b5c1bfe87a700cf473725e7cd133f4c7da48ec85baccf2d0c5d6127beb675e7" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 4m22s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ Build successful
where-for-diâ€¦ â”‚ [event: taskrun workloads3/where-for-dinner-config-writer-gr78v] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ [event: taskrun workloads3/where-for-dinner-config-writer-gr78v] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ [event: taskrun workloads3/where-for-dinner-config-writer-gr78v] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ [event: taskrun workloads3/where-for-dinner-config-writer-gr78v] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ [event: taskrun workloads3/where-for-dinner-config-writer-gr78v] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-config-writer-gr78v-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-config-writer-gr78v-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/03 10:57:51 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-config-writer-gr78v-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-config-writer-gr78v-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/03 10:57:52 Decoded script /tekton/scripts/script-0-czkxh
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.R8P57hR12N
where-for-diâ€¦ â”‚ + echo -e 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
where-for-diâ€¦ â”‚ + base64 --decode
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/carvel-package-workflow: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/target: "200"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner@sha256:05a0fc5cb0d74853a265fe33687c0e0760c1e36edcecc993a00bd68657f14e9c
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner@sha256:05a0fc5cb0d74853a265fe33687c0e0760c1e36edcecc993a00bd68657f14e9c
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/carvel-package-workflow: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3@sha256:26e9dc33923dcfb3e2621fa8a022dec5ded853b4d5f7ede0cac4b899acfd8147
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/carvel-package-workflow: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/target: "200"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner@sha256:05a0fc5cb0d74853a265fe33687c0e0760c1e36edcecc993a00bd68657f14e9c
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner@sha256:05a0fc5cb0d74853a265fe33687c0e0760c1e36edcecc993a00bd68657f14e9c
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/carvel-package-workflow: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3@sha256:26e9dc33923dcfb3e2621fa8a022dec5ded853b4d5f7ede0cac4b899acfd8147
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3-bundle:2741e190-a330-4c75-8062-6c4c168c8ae4 -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 3s
where-for-diâ€¦ â”‚      â”Š Completed       - 4s
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-00001-deployment-697f566574-2hcpb):
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [workload queue-proxy]
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:59:12.416405372Z","logger":"queueproxy","caller":"sharedmain/main.go:260","message":"Starting queue-proxy","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:59:12.417213513Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server main:8012","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:59:12.432260788Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server admin:8022","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:59:12.432298753Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server metrics:9090","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:59:41.575798734Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00040df90}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:00:11.571466848Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000706b90}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:00:41.572372035Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00040c870}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:01:11.574734093Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0007455e0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:01:41.574221362Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0006ea730}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [workload queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:03:01.573084433Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0008d44b0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 186 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:03:31.579253015Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000655180}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [workload queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:06:41.572293347Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00040cd20}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 185 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:12:11.574383776Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00066bb30}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:12:41.573832321Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00040db80}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:18:11.572889029Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0007a8730}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:18:41.574345913Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00040c5a0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:24:21.582578946Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00062e730}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 193 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:24:51.571715786Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00072a910}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:30:21.574829005Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0004dfbd0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:30:51.572080314Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0006ddef0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:36:21.57330344Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00068bf40}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:36:51.572470951Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000886320}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:42:21.573608552Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00067b590}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:42:51.573815516Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000886410}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:48:31.573254908Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00068ba90}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:49:01.572065675Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000846960}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:54:31.57407149Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00074f8b0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T11:55:01.578592689Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000725400}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:00:41.57280021Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0007d2550}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 178 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:01:11.571782754Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0007c6d20}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 194 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:06:41.581475215Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000790870}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:07:11.572299807Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000596e10}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:12:41.573630768Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0006858b0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:13:11.575436132Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0002fb130}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:18:41.577237161Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00009aff0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:19:11.571461876Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0007f2050}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:24:51.573427331Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000817e00}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:25:21.572385501Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00084bb30}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:31:01.575012322Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000887590}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:31:31.571632895Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00062fef0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:37:01.572749558Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000751360}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:37:31.57399048Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00067a500}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [workload queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:43:01.572373546Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00040d950}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:43:31.572231902Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00009ad70}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 194 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:49:01.573231068Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000751c70}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:49:31.591305581Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000834b90}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 177 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 194 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:55:01.592238988Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000817400}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 181 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 175 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 189 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T12:55:31.572691975Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0005cce10}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 184 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 194 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:01:01.574577511Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00077ce60}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:01:31.572248973Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00062ef50}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:07:01.572547577Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0006c08c0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:07:31.571184978Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000886af0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 184 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 175 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:13:01.575478107Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00069c320}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:13:31.57434023Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0008357c0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition

1 File Changed: [Tiltfile]
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-api-gateway/Tiltfile
Successfully loaded Tiltfile (620.543Âµs)
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:19:01.578456071Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0007f2910}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:19:31.573183579Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000729f40}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:25:11.574580682Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00076ac80}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:25:41.575040618Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00067b310}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:31:11.57333042Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00040d770}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:31:41.572244136Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00069ce10}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:37:21.573724756Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc00076f8b0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:37:51.572778476Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000886e10}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:43:21.573592826Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc000666320}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-697f566574-2hcpb. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T13:43:51.573721241Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.39:8022 map[] map[] <nil> map[] 10.200.0.1:59936 /wait-for-drain <nil> <nil> <nil> 0xc0008618b0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-697f566574-2hcpb"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
Tilt started on http://localhost:10350/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-api-gateway/Tiltfile
Successfully loaded Tiltfile (1.202525ms)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads3 --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 5.08s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 5.08s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ [event: imagerepository workloads3/where-for-dinner] Failed to update status: Operation cannot be fulfilled on imagerepositories.source.apps.tanzu.vmware.com "where-for-dinner": the object has been modified; please apply your changes to the latest version and try again
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-build-1-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32menv:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_OCI_SOURCE[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner@sha256:16ccf9128033f4a1f673d8b73e983d793d89f1509533a80c053eea926923f9cc[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_LIVE_RELOAD_ENABLED[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "true"[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_JVM_VERSION[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "17"[0m
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31msource: {}[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32msource:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  blob:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads3/where-for-dinner/16ccf9128033f4a1f673d8b73e983d793d89f1509533a80c053eea926923f9cc.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads3/where-for-dinner/16ccf9128033f4a1f673d8b73e983d793d89f1509533a80c053eea926923f9cc.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads3/where-for-dinner/16ccf9128033f4a1f673d8b73e983d793d89f1509533a80c053eea926923f9cc.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Pulling image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:d03ae9a647d116070e78c1785d7150549976b088dc0d3a130db22191c2d258bf"
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [analyze detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Successfully pulled image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:d03ae9a647d116070e78c1785d7150549976b088dc0d3a130db22191c2d258bf" in 30.341792877s (30.341957075s including waiting)
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer started at 2024-01-03T17:24:04Z
where-for-diâ€¦ â”‚ [analyze] Restoring data for SBOM from previous image
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer ran for 7.656453534s and ended at 2024-01-03T17:24:12Z
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:d03ae9a647d116070e78c1785d7150549976b088dc0d3a130db22191c2d258bf" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/dotnet-core-lite@2.9.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/go-lite@2.2.3' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/nodejs-lite@2.4.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] Timer: Detector started at 2024-01-03T17:24:12Z
where-for-diâ€¦ â”‚ [detect] 11 of 26 buildpacks participating
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/ca-certificates   3.6.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/bellsoft-liberica 9.13.0
where-for-diâ€¦ â”‚ [detect] paketo-buildpacks/syft             1.10.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/maven             6.15.12
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/watchexec         2.8.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/executable-jar    6.8.3
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/apache-tomcat     7.14.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/liberty           3.8.11
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/dist-zip          5.6.8
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/spring-boot       5.27.6
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/image-labels      4.5.6
where-for-diâ€¦ â”‚ [detect] Timer: Detector ran for 1.15209525s and ended at 2024-01-03T17:24:14Z
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:d03ae9a647d116070e78c1785d7150549976b088dc0d3a130db22191c2d258bf" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:d03ae9a647d116070e78c1785d7150549976b088dc0d3a130db22191c2d258bf" already present on machine
where-for-diâ€¦ â”‚ [restore] Timer: Restorer started at 2024-01-03T17:24:14Z
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/ca-certificates:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:jre" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:java-security-properties" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/watchexec:watchexec" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:spring-cloud-bindings" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:web-application-type" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:helper" from app image
where-for-diâ€¦ â”‚ [restore] Timer: Restorer ran for 4.205532ms and ended at 2024-01-03T17:24:14Z
where-for-diâ€¦ â”‚ [build] Timer: Builder started at 2024-01-03T17:24:15Z
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for CA Certificates[0m[34m 3.6.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-ca-certificates[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for BellSoft Liberica[0m[34m 9.13.0[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-bellsoft-liberica[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ARGS           --no-man-pages --no-header-files --strip-debug --compress=1  configure custom link arguments (--output must be omitted)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ENABLED        false                                                        enables running jlink tool to generate custom JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_TYPE                 JRE                                                          the JVM type - JDK or JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_VERSION              17                                                           the Java version[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_ENABLED           false                                                        enables Java remote debugging support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_PORT              8000                                                         configure the remote debugging port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_SUSPEND           false                                                        configure whether to suspend execution until a debugger has attached[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_HEAP_DUMP_PATH                                                                       write heap dumps on error to this path[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_ENABLED        true                                                         enables Java Native Memory Tracking (NMT)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_LEVEL          summary                                                      configure level of NMT, summary or detail[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ARGS                                                                             configure custom Java Flight Recording (JFR) arguments[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ENABLED             false                                                        enables Java Flight Recording (JFR)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_ENABLED             false                                                        enables Java Management Extensions (JMX)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_PORT                5000                                                         configure the JMX port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_HEAD_ROOM           0                                                            the headroom in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_LOADED_CLASS_COUNT  35% of classes                                               the number of loaded classes in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_THREAD_COUNT        250                                                          the number of threads in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $JAVA_TOOL_OPTIONS                                                                        the JVM launch flags[0m
where-for-diâ€¦ â”‚ [build] [2m    [2mUsing Java version 17 from BP_JVM_VERSION[0m[2m[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JDK 17.0.9[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/bell-sw/Liberica/releases/download/17.0.9+11/bellsoft-jdk17.0.9+11-linux-amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Expanding to /layers/tanzu-buildpacks_bellsoft-liberica/jdk[0m
where-for-diâ€¦ â”‚ [build] [2m    Adding 137 container CA certificates to JVM truststore[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JAVA_HOME.override[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JDK_HOME.override[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JRE 17.0.9[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mJava Security Properties[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mPaketo Syft Buildpack[0m[34m 1.10.1[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/paketo-buildpacks/syft[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/anchore/syft/releases/download/v0.41.1/syft_0.41.1_linux_amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/SYFT_CHECK_FOR_APP_UPDATE.default[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Maven[0m[34m 6.15.12[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-maven[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_EXCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are removed[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_INCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are included[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JAVA_INSTALL_NODE                 false                                                  whether to install Yarn/Node binaries based on the presence of a package.json or yarn.lock file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ACTIVE_PROFILES                                                                    the active profiles (comma separated: such as: p1,!p2,?p3) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ADDITIONAL_BUILD_ARGUMENTS                                                         the additionnal arguments (appended to BP_MAVEN_BUILD_ARGUMENTS) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILD_ARGUMENTS             -Dmaven.test.skip=true --no-transfer-progress package  the arguments to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_ARTIFACT              target/*.[ejw]ar                                       the built application artifact explicitly.  Supersedes $BP_MAVEN_BUILT_MODULE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_MODULE                                                                       the module to find application artifact in[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_DAEMON_ENABLED              false                                                  use maven daemon[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_POM_FILE                    pom.xml                                                the location of the main pom.xml file, relative to the application root[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_SETTINGS_PATH                                                                      the path to a Maven settings file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_VERSION                     3                                                      the Maven version[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_NODE_PROJECT_PATH                                                                        configure a project subdirectory to look for `package.json` and `yarn.lock` files[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating cache directory /home/cnb/.m2[0m
where-for-diâ€¦ â”‚ [build]   [34mCompiled Application[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Executing mvnw --batch-mode -Dmaven.test.skip=true --no-transfer-progress package[0m
where-for-diâ€¦ â”‚ [build]         % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
where-for-diâ€¦ â”‚ [build]                                        Dload  Upload   Total   Spent    Left  Speed
where-for-diâ€¦ â”‚ [build]         0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 59925  100 59925    0     0   244k      0 --:--:-- --:--:-- --:--:--  245k
where-for-diâ€¦ â”‚ [build]       [0m[0m[INFO] Scanning for projects...
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --------------< com.example:where-for-dinner-api-gateway >--------------
where-for-diâ€¦ â”‚ [build]       [INFO] Building where-for-dinner-api-gateway 0.0.1-SNAPSHOT
where-for-diâ€¦ â”‚ [build]       [INFO] --------------------------------[ jar ]---------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 1 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 0 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Changes detected - recompiling the module! :dependency
where-for-diâ€¦ â”‚ [build]       [INFO] Compiling 1 source file with javac [debug release 17] to target/classes
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[40,17] httpBasic() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[41,13] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[42,13] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[60,12] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[63,12] and() in org.springframework.security.config.web.server.ServerHttpSecurity.AuthorizeExchangeSpec has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[65,8] logout() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[70,12] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:testResources (default-testResources) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not copying test resources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:testCompile (default-testCompile) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not compiling test sources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-surefire-plugin:3.0.0:test (default-test) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Tests are skipped.
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-jar-plugin:3.3.0:jar (default-jar) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Building jar: /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- spring-boot-maven-plugin:3.1.3:repackage (repackage) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Replacing main artifact /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar with repackaged archive, adding nested dependencies in BOOT-INF/.
where-for-diâ€¦ â”‚ [build]       [INFO] The original artifact has been renamed to /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar.original
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] BUILD SUCCESS
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] Total time:  43.908 s
where-for-diâ€¦ â”‚ [build]       [INFO] Finished at: 2024-01-03T17:25:19Z
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [0m[0m
where-for-diâ€¦ â”‚ [build]   Removing source code
where-for-diâ€¦ â”‚ [build]   Restoring application artifact
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Watchexec[0m[34m 2.8.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-watchexec[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/watchexec/watchexec/releases/download/cli-v1.20.5/watchexec-1.20.5-x86_64-unknown-linux-musl.tar.xz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Executable JAR[0m[34m 6.8.3[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-executable-jar[0m
where-for-diâ€¦ â”‚ [build]   [34mClass Path[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.delim[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.prepend[0m
where-for-diâ€¦ â”‚ [build]   Process types:
where-for-diâ€¦ â”‚ [build]     [36mexecutable-jar[0m: java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mreload[0m:         watchexec -r --shell=none -- java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mtask[0m:           java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mweb[0m:            java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build] SKIPPED: `Main-Class` found in `META-INF/MANIFEST.MF`, skipping build
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Spring Boot[0m[34m 5.27.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-spring-boot[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_DISABLED   false  whether to contribute Spring Boot cloud bindings support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_VERSION    1      default version of Spring Cloud Bindings library to contribute[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_DISABLED  false  whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_ENABLED   true   Deprecated - whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build]   Creating slices from layers index
where-for-diâ€¦ â”‚ [build] [2m    dependencies (74.3 MB)[0m
where-for-diâ€¦ â”‚ [build] [2m    spring-boot-loader (269.4 KB)[0m
where-for-diâ€¦ â”‚ [build] [2m    snapshot-dependencies (0.0 B)[0m
where-for-diâ€¦ â”‚ [build] [2m    application (59.1 KB)[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mSpring Cloud Bindings 2.0.2[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-bindings/2.0.2/spring-cloud-bindings-2.0.2.jar[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Copying to /layers/tanzu-buildpacks_spring-boot/spring-cloud-bindings[0m
where-for-diâ€¦ â”‚ [build]   [34mWeb Application Type[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   4 application slices
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.title
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.version
where-for-diâ€¦ â”‚ [build]     org.springframework.boot.version
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Image Labels[0m[34m 4.5.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-image-labels[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_IMAGE_LABELS                                                                                                                                                     arbitrary image labels[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_AUTHORS                                                                                                                                                      the org.opencontainers.image.authors image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_CREATED                                                                                                                                                      the org.opencontainers.image.created image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DESCRIPTION                                                                                                                                                  the org.opencontainers.image.description image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DOCUMENTATION                                                                                                                                                the org.opencontainers.image.documentation image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_LICENSES                                                                                                                                                     the org.opencontainers.image.licenses image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REF_NAME                                                                                                                                                     the org.opencontainers.image.ref.name image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REVISION                                                                                                                                                     the org.opencontainers.image.revision image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_SOURCE         tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner@sha256:16ccf9128033f4a1f673d8b73e983d793d89f1509533a80c053eea926923f9cc  the org.opencontainers.image.source image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_TITLE                                                                                                                                                        the org.opencontainers.image.title image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_URL                                                                                                                                                          the org.opencontainers.image.url image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VENDOR                                                                                                                                                       the org.opencontainers.image.vendor image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VERSION                                                                                                                                                      the org.opencontainers.image.version image label[0m
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.source
where-for-diâ€¦ â”‚ [build] Timer: Builder ran for 1m10.02756183s and ended at 2024-01-03T17:25:25Z
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:d03ae9a647d116070e78c1785d7150549976b088dc0d3a130db22191c2d258bf" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [export] Reusing layers from image 'tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3@sha256:26e9dc33923dcfb3e2621fa8a022dec5ded853b4d5f7ede0cac4b899acfd8147'
where-for-diâ€¦ â”‚ [export] Timer: Exporter started at 2024-01-03T17:27:03Z
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/ca-certificates:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:java-security-properties'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:jre'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/watchexec:watchexec'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/executable-jar:classpath'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:spring-cloud-bindings'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:web-application-type'
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:launch.sbom'
where-for-diâ€¦ â”‚ [export] Reusing 5/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:launcher'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:config'
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:process-types'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.lifecycle.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.build.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.project.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.title'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.springframework.boot.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.source'
where-for-diâ€¦ â”‚ [export] Setting default process type 'reload'
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3... started at 2024-01-03T17:27:04Z
where-for-diâ€¦ â”‚ [export] *** Images (sha256:25cb5ece453b5834fa7dfacdf1af481cfd09b0d157c8c9671f5a4167bb67de47):
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3:b1.20240103.172208
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3... ran for 4.255021689s and ended at 2024-01-03T17:27:08Z
where-for-diâ€¦ â”‚ [export] Timer: Exporter ran for 5.263962933s and ended at 2024-01-03T17:27:08Z
where-for-diâ€¦ â”‚ [export] Timer: Cache started at 2024-01-03T17:27:08Z
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/bellsoft-liberica:jdk'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'paketo-buildpacks/syft:syft'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:application'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:cache'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'buildpacksio/lifecycle:cache.sbom'
where-for-diâ€¦ â”‚ [export] Timer: Cache ran for 22.243150319s and ended at 2024-01-03T17:27:30Z
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:0b5c1bfe87a700cf473725e7cd133f4c7da48ec85baccf2d0c5d6127beb675e7" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 5m22s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ Build successful
where-for-diâ€¦ â”‚ [event: taskrun workloads3/where-for-dinner-config-writer-pdwv4] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-config-writer-pdwv4-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-config-writer-pdwv4-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/03 17:28:05 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-config-writer-pdwv4-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads3/where-for-dinner-config-writer-pdwv4-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/03 17:28:06 Decoded script /tekton/scripts/script-0-l7bxc
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.rRglgRiCRP
where-for-diâ€¦ â”‚ + echo -e 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
where-for-diâ€¦ â”‚ + base64 --decode
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner@sha256:16ccf9128033f4a1f673d8b73e983d793d89f1509533a80c053eea926923f9cc
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner@sha256:16ccf9128033f4a1f673d8b73e983d793d89f1509533a80c053eea926923f9cc
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3@sha256:25cb5ece453b5834fa7dfacdf1af481cfd09b0d157c8c9671f5a4167bb67de47
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner@sha256:16ccf9128033f4a1f673d8b73e983d793d89f1509533a80c053eea926923f9cc
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads3-where-for-dinner@sha256:16ccf9128033f4a1f673d8b73e983d793d89f1509533a80c053eea926923f9cc
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3@sha256:25cb5ece453b5834fa7dfacdf1af481cfd09b0d157c8c9671f5a4167bb67de47
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads3-bundle:08fea19b-9911-4016-8dc7-78a875943b7e -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 4s
where-for-diâ€¦ â”‚      â”Š Completed       - 5s
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-00001-deployment-6cffcddd56-bn5mr):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:28:21.039238563Z","logger":"queueproxy","caller":"sharedmain/main.go:260","message":"Starting queue-proxy","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:28:21.039592679Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server metrics:9090","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:28:21.045308951Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server main:8012","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:28:21.045406725Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server admin:8022","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:28:47.877513746Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0001c02d0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:29:17.877315697Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc00070bb80}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:29:47.877668044Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc00016f9a0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:30:17.878144498Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0007838b0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:30:47.886866857Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc00068f220}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [workload queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:32:07.877102821Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0006296d0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [workload queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:33:57.877568127Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0001c11d0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:37:07.880178817Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000697040}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:42:37.879623695Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0007e4a00}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:43:07.876007735Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc00067dae0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:48:37.87768055Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0004a56d0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:49:07.877006982Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc00062eaf0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 193 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:54:37.884669611Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000619db0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T17:55:07.877529176Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000661770}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:00:47.877598089Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0007c7d60}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:01:17.87962222Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc00086aaf0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 194 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:06:47.879862632Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000833900}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:07:17.963066004Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0007400f0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:12:47.886750258Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0008339f0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:13:17.876486599Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0006e9d10}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:18:47.87875095Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000419e00}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:19:17.876724017Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000653400}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:24:47.87770608Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000784f00}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:25:17.887258665Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000653ae0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:30:57.887342423Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000822a50}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:31:27.897174091Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0006d05f0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:37:07.879782311Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc00084c9b0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:37:37.877570606Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc00088e640}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:43:07.883362348Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0006fa730}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:43:37.876835731Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0006afa90}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:49:07.878065092Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0007386e0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:49:37.877739865Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0008235e0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:55:17.880009173Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000773900}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T18:55:47.877201911Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc00016e910}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:01:17.877639844Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc00075c960}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:01:47.876892346Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc00088f810}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:07:27.880158965Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0007faaf0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:07:57.879054563Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0004f63c0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [workload queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:13:27.879855975Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0008db3b0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:13:57.878997275Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0006aebe0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:19:27.879849016Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0000ae7d0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 194 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:19:57.879099629Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0007fad20}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:25:27.879167674Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0008be0f0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:25:57.877534508Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000653270}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:31:27.878378779Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000618640}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:31:57.878130176Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0008644b0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:37:37.892197353Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000846d20}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:38:07.877365886Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc000419360}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:43:37.878093487Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc00087ed20}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:44:07.878931113Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0000ae820}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 193 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 194 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:49:37.882510184Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc0006d6ff0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00001-deployment-6cffcddd56-bn5mr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T19:50:07.878392319Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.48:8022 map[] map[] <nil> map[] 10.200.0.1:37952 /wait-for-drain <nil> <nil> <nil> 0xc00084d7c0}","commit":"f60eb32","knative.dev/key":"workloads3/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-6cffcddd56-bn5mr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 194 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
Tilt started on http://localhost:53301/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner/where-for-dinner-api-gateway/Tiltfile
Successfully loaded Tiltfile (682.967Âµs)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 5.38s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 5.38s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-build-1-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32menv:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_OCI_SOURCE[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:4e5a703a9fe15f67baf2680ea5a2bca0ed73ef9c19467616eff5277d29d44069[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_LIVE_RELOAD_ENABLED[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "true"[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_JVM_VERSION[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "17"[0m
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31msource: {}[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32msource:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  blob:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/4e5a703a9fe15f67baf2680ea5a2bca0ed73ef9c19467616eff5277d29d44069.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/4e5a703a9fe15f67baf2680ea5a2bca0ed73ef9c19467616eff5277d29d44069.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/4e5a703a9fe15f67baf2680ea5a2bca0ed73ef9c19467616eff5277d29d44069.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [analyze detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer started at 2024-01-04T10:10:03Z
where-for-diâ€¦ â”‚ [analyze] Restoring data for SBOM from previous image
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer ran for 6.893197942s and ended at 2024-01-04T10:10:09Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/dotnet-core-lite@2.9.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/go-lite@2.2.3' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/nodejs-lite@2.4.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] Timer: Detector started at 2024-01-04T10:10:10Z
where-for-diâ€¦ â”‚ [detect] 11 of 26 buildpacks participating
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/ca-certificates   3.6.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/bellsoft-liberica 9.13.0
where-for-diâ€¦ â”‚ [detect] paketo-buildpacks/syft             1.10.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/maven             6.15.12
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/watchexec         2.8.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/executable-jar    6.8.3
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/apache-tomcat     7.14.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/liberty           3.8.11
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/dist-zip          5.6.8
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/spring-boot       5.27.6
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/image-labels      4.5.6
where-for-diâ€¦ â”‚ [detect] Timer: Detector ran for 1.183242528s and ended at 2024-01-04T10:10:11Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [restore] Timer: Restorer started at 2024-01-04T10:10:12Z
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/ca-certificates:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:jre" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:java-security-properties" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:web-application-type" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:spring-cloud-bindings" from app image
where-for-diâ€¦ â”‚ [restore] Timer: Restorer ran for 4.342874ms and ended at 2024-01-04T10:10:12Z
where-for-diâ€¦ â”‚ [build] Timer: Builder started at 2024-01-04T10:10:13Z
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for CA Certificates[0m[34m 3.6.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-ca-certificates[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for BellSoft Liberica[0m[34m 9.13.0[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-bellsoft-liberica[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ARGS           --no-man-pages --no-header-files --strip-debug --compress=1  configure custom link arguments (--output must be omitted)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ENABLED        false                                                        enables running jlink tool to generate custom JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_TYPE                 JRE                                                          the JVM type - JDK or JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_VERSION              17                                                           the Java version[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_ENABLED           false                                                        enables Java remote debugging support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_PORT              8000                                                         configure the remote debugging port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_SUSPEND           false                                                        configure whether to suspend execution until a debugger has attached[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_HEAP_DUMP_PATH                                                                       write heap dumps on error to this path[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_ENABLED        true                                                         enables Java Native Memory Tracking (NMT)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_LEVEL          summary                                                      configure level of NMT, summary or detail[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ARGS                                                                             configure custom Java Flight Recording (JFR) arguments[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ENABLED             false                                                        enables Java Flight Recording (JFR)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_ENABLED             false                                                        enables Java Management Extensions (JMX)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_PORT                5000                                                         configure the JMX port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_HEAD_ROOM           0                                                            the headroom in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_LOADED_CLASS_COUNT  35% of classes                                               the number of loaded classes in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_THREAD_COUNT        250                                                          the number of threads in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $JAVA_TOOL_OPTIONS                                                                        the JVM launch flags[0m
where-for-diâ€¦ â”‚ [build] [2m    [2mUsing Java version 17 from BP_JVM_VERSION[0m[2m[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JDK 17.0.9[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/bell-sw/Liberica/releases/download/17.0.9+11/bellsoft-jdk17.0.9+11-linux-amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Expanding to /layers/tanzu-buildpacks_bellsoft-liberica/jdk[0m
where-for-diâ€¦ â”‚ [build] [2m    Adding 137 container CA certificates to JVM truststore[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JAVA_HOME.override[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JDK_HOME.override[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JRE 17.0.9[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mJava Security Properties[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mPaketo Syft Buildpack[0m[34m 1.10.1[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/paketo-buildpacks/syft[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/anchore/syft/releases/download/v0.41.1/syft_0.41.1_linux_amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/SYFT_CHECK_FOR_APP_UPDATE.default[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Maven[0m[34m 6.15.12[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-maven[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_EXCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are removed[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_INCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are included[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JAVA_INSTALL_NODE                 false                                                  whether to install Yarn/Node binaries based on the presence of a package.json or yarn.lock file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ACTIVE_PROFILES                                                                    the active profiles (comma separated: such as: p1,!p2,?p3) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ADDITIONAL_BUILD_ARGUMENTS                                                         the additionnal arguments (appended to BP_MAVEN_BUILD_ARGUMENTS) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILD_ARGUMENTS             -Dmaven.test.skip=true --no-transfer-progress package  the arguments to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_ARTIFACT              target/*.[ejw]ar                                       the built application artifact explicitly.  Supersedes $BP_MAVEN_BUILT_MODULE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_MODULE                                                                       the module to find application artifact in[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_DAEMON_ENABLED              false                                                  use maven daemon[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_POM_FILE                    pom.xml                                                the location of the main pom.xml file, relative to the application root[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_SETTINGS_PATH                                                                      the path to a Maven settings file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_VERSION                     3                                                      the Maven version[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_NODE_PROJECT_PATH                                                                        configure a project subdirectory to look for `package.json` and `yarn.lock` files[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating cache directory /home/cnb/.m2[0m
where-for-diâ€¦ â”‚ [build]   [34mCompiled Application[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Executing mvnw --batch-mode -Dmaven.test.skip=true --no-transfer-progress package[0m
where-for-diâ€¦ â”‚ [build]         % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
where-for-diâ€¦ â”‚ [build]                                        Dload  Upload   Total   Spent    Left  Speed
where-for-diâ€¦ â”‚ [build]         0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 59925  100 59925    0     0   236k      0 --:--:-- --:--:-- --:--:--  237k
where-for-diâ€¦ â”‚ [build]       [0m[0m[INFO] Scanning for projects...
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --------------< com.example:where-for-dinner-api-gateway >--------------
where-for-diâ€¦ â”‚ [build]       [INFO] Building where-for-dinner-api-gateway 0.0.1-SNAPSHOT
where-for-diâ€¦ â”‚ [build]       [INFO] --------------------------------[ jar ]---------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 1 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 0 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Changes detected - recompiling the module! :dependency
where-for-diâ€¦ â”‚ [build]       [INFO] Compiling 1 source file with javac [debug release 17] to target/classes
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[40,17] httpBasic() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[41,13] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[42,13] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[60,12] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[63,12] and() in org.springframework.security.config.web.server.ServerHttpSecurity.AuthorizeExchangeSpec has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[65,8] logout() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[70,12] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:testResources (default-testResources) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not copying test resources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:testCompile (default-testCompile) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not compiling test sources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-surefire-plugin:3.0.0:test (default-test) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Tests are skipped.
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-jar-plugin:3.3.0:jar (default-jar) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Building jar: /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- spring-boot-maven-plugin:3.1.3:repackage (repackage) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Replacing main artifact /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar with repackaged archive, adding nested dependencies in BOOT-INF/.
where-for-diâ€¦ â”‚ [build]       [INFO] The original artifact has been renamed to /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar.original
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] BUILD SUCCESS
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] Total time:  49.599 s
where-for-diâ€¦ â”‚ [build]       [INFO] Finished at: 2024-01-04T10:11:20Z
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [0m[0m
where-for-diâ€¦ â”‚ [build]   Removing source code
where-for-diâ€¦ â”‚ [build]   Restoring application artifact
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Watchexec[0m[34m 2.8.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-watchexec[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/watchexec/watchexec/releases/download/cli-v1.20.5/watchexec-1.20.5-x86_64-unknown-linux-musl.tar.xz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Executable JAR[0m[34m 6.8.3[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-executable-jar[0m
where-for-diâ€¦ â”‚ [build]   [34mClass Path[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.delim[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.prepend[0m
where-for-diâ€¦ â”‚ [build]   Process types:
where-for-diâ€¦ â”‚ [build]     [36mexecutable-jar[0m: java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mreload[0m:         watchexec -r --shell=none -- java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mtask[0m:           java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mweb[0m:            java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build] SKIPPED: `Main-Class` found in `META-INF/MANIFEST.MF`, skipping build
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Spring Boot[0m[34m 5.27.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-spring-boot[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_DISABLED   false  whether to contribute Spring Boot cloud bindings support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_VERSION    1      default version of Spring Cloud Bindings library to contribute[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_DISABLED  false  whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_ENABLED   true   Deprecated - whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build]   Creating slices from layers index
where-for-diâ€¦ â”‚ [build] [2m    dependencies (74.3 MB)[0m
where-for-diâ€¦ â”‚ [build] [2m    spring-boot-loader (269.4 KB)[0m
where-for-diâ€¦ â”‚ [build] [2m    snapshot-dependencies (0.0 B)[0m
where-for-diâ€¦ â”‚ [build] [2m    application (59.1 KB)[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mSpring Cloud Bindings 2.0.2[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-bindings/2.0.2/spring-cloud-bindings-2.0.2.jar[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Copying to /layers/tanzu-buildpacks_spring-boot/spring-cloud-bindings[0m
where-for-diâ€¦ â”‚ [build]   [34mWeb Application Type[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Reactive web application detected[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/BPL_JVM_THREAD_COUNT.default[0m
where-for-diâ€¦ â”‚ [build]   4 application slices
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.title
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.version
where-for-diâ€¦ â”‚ [build]     org.springframework.boot.version
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Image Labels[0m[34m 4.5.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-image-labels[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_IMAGE_LABELS                                                                                                                                                    arbitrary image labels[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_AUTHORS                                                                                                                                                     the org.opencontainers.image.authors image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_CREATED                                                                                                                                                     the org.opencontainers.image.created image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DESCRIPTION                                                                                                                                                 the org.opencontainers.image.description image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DOCUMENTATION                                                                                                                                               the org.opencontainers.image.documentation image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_LICENSES                                                                                                                                                    the org.opencontainers.image.licenses image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REF_NAME                                                                                                                                                    the org.opencontainers.image.ref.name image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REVISION                                                                                                                                                    the org.opencontainers.image.revision image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_SOURCE         tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:4e5a703a9fe15f67baf2680ea5a2bca0ed73ef9c19467616eff5277d29d44069  the org.opencontainers.image.source image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_TITLE                                                                                                                                                       the org.opencontainers.image.title image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_URL                                                                                                                                                         the org.opencontainers.image.url image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VENDOR                                                                                                                                                      the org.opencontainers.image.vendor image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VERSION                                                                                                                                                     the org.opencontainers.image.version image label[0m
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.source
where-for-diâ€¦ â”‚ [build] Timer: Builder ran for 1m14.740028178s and ended at 2024-01-04T10:11:28Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [export] Reusing layers from image 'tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:56232a5383d5adc44f40b35e701a360fec73f9d5aa931452056b83477fa2c9ed'
where-for-diâ€¦ â”‚ [export] Timer: Exporter started at 2024-01-04T10:13:04Z
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/ca-certificates:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:java-security-properties'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:jre'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/watchexec:watchexec'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/executable-jar:classpath'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:spring-cloud-bindings'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:web-application-type'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:launch.sbom'
where-for-diâ€¦ â”‚ [export] Reusing 2/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Adding 3/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:launcher'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:config'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:process-types'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.lifecycle.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.build.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.project.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.title'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.springframework.boot.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.source'
where-for-diâ€¦ â”‚ [export] Setting default process type 'reload'
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads... started at 2024-01-04T10:13:06Z
where-for-diâ€¦ â”‚ [export] *** Images (sha256:2fb660aa09e421a4d082f23c9c6cfee17b35d591a0d5515059fe7b53b65d2327):
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads:b1.20240104.100839
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads... ran for 4.128668618s and ended at 2024-01-04T10:13:10Z
where-for-diâ€¦ â”‚ [export] Timer: Exporter ran for 6.065073283s and ended at 2024-01-04T10:13:10Z
where-for-diâ€¦ â”‚ [export] Timer: Cache started at 2024-01-04T10:13:10Z
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/bellsoft-liberica:jdk'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'paketo-buildpacks/syft:syft'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:application'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:cache'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'buildpacksio/lifecycle:cache.sbom'
where-for-diâ€¦ â”‚ [export] Timer: Cache ran for 9.447089189s and ended at 2024-01-04T10:13:20Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:0b5c1bfe87a700cf473725e7cd133f4c7da48ec85baccf2d0c5d6127beb675e7" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 4m41s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ Build successful
where-for-diâ€¦ â”‚ [event: taskrun workloads/where-for-dinner-config-writer-dtjpv] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-config-writer-dtjpv-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-dtjpv-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-dtjpv-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/04 10:13:49 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-dtjpv-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/04 10:13:49 Decoded script /tekton/scripts/script-0-pdvcr
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.NZZJIFWLYD
where-for-diâ€¦ â”‚ + base64 --decode
where-for-diâ€¦ â”‚ + echo -e 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
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:4e5a703a9fe15f67baf2680ea5a2bca0ed73ef9c19467616eff5277d29d44069
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:4e5a703a9fe15f67baf2680ea5a2bca0ed73ef9c19467616eff5277d29d44069
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:2fb660aa09e421a4d082f23c9c6cfee17b35d591a0d5515059fe7b53b65d2327
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:4e5a703a9fe15f67baf2680ea5a2bca0ed73ef9c19467616eff5277d29d44069
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:4e5a703a9fe15f67baf2680ea5a2bca0ed73ef9c19467616eff5277d29d44069
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:2fb660aa09e421a4d082f23c9c6cfee17b35d591a0d5515059fe7b53b65d2327
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads-bundle:1d2109d6-30c3-4b4e-83c9-9bf2b23d8bdc -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 2s
where-for-diâ€¦ â”‚      â”Š Completed       - 5s
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-00001-deployment-7bdbf5fd7-rqnhw):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T10:14:53.100898083Z","logger":"queueproxy","caller":"sharedmain/main.go:260","message":"Starting queue-proxy","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-7bdbf5fd7-rqnhw"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T10:14:53.10200161Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server metrics:9090","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-7bdbf5fd7-rqnhw"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T10:14:53.110528394Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server main:8012","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-7bdbf5fd7-rqnhw"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T10:14:53.110605422Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server admin:8022","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-7bdbf5fd7-rqnhw"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload]   .   ____          _            __ _ _
where-for-diâ€¦ â”‚ [workload]  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
where-for-diâ€¦ â”‚ [workload] ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
where-for-diâ€¦ â”‚ [workload]  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
where-for-diâ€¦ â”‚ [workload]   '  |____| .__|_| |_|_| |_\__, | / / / /
where-for-diâ€¦ â”‚ [workload]  =========|_|==============|___/=/_/_/_/
where-for-diâ€¦ â”‚ [workload]  :: Spring Boot ::                (v3.1.3)
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:14:56.494Z  INFO 116 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : Starting DinnerAPIGatewayApplication v0.0.1-SNAPSHOT using Java 17.0.9 with PID 116 (/workspace/BOOT-INF/classes started by cnb in /workspace)
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:14:56.501Z  INFO 116 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : The following 1 profile is active: "kubernetes"
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:01.222Z  INFO 116 --- [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=13afbb8a-a579-326a-b409-ed4b39bebfd3
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 189 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:04.121Z  INFO 116 --- [           main] ctiveUserDetailsServiceAutoConfiguration : 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] Using generated security password: b56cef22-7f7c-4b6a-99a4-7515345a3aeb
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.298Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [After]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.299Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Before]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.299Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Between]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.299Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Cookie]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.299Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Header]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.299Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Host]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.299Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Method]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.299Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Path]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.300Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Query]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.300Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [ReadBody]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.300Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [RemoteAddr]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.300Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [XForwardedRemoteAddr]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.300Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Weight]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:06.300Z  INFO 116 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [CloudFoundryRouteService]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:08.283Z  INFO 116 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8080
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:08.589Z  INFO 116 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 15 endpoint(s) beneath base path '/actuator'
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:08.909Z  INFO 116 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8081
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:09.316Z  INFO 116 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : Started DinnerAPIGatewayApplication in 14.897 seconds (process running for 16.016)
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”Š Ready           - 20s
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:15:16.516Z ERROR 116 --- [or-http-epoll-3] a.w.r.e.AbstractErrorWebExceptionHandler : [6cdacf20-12]  500 Server Error for HTTP GET "/api/search/search"
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] java.net.UnknownHostException: Failed to resolve 'where-for-dinner-search.workloads' [A(1)] after 2 queries 
where-for-diâ€¦ â”‚ [workload] 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
where-for-diâ€¦ â”‚ [workload] Error has been observed at the following site(s):
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.cloud.gateway.filter.WeightCalculatorWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? AuthorizationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ExceptionTranslationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? LogoutWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerRequestCacheWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? SecurityContextServerWebExchangeWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HttpHeaderWriterWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerWebExchangeReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.security.web.server.WebFilterChainProxy [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.web.filter.reactive.ServerHttpObservationFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HTTP GET "/api/search/search" [ExceptionHandlingWebHandler]
where-for-diâ€¦ â”‚ [workload] Original Stack Trace:
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1044) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:432) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:662) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.access$500(DnsResolveContext.java:66) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:489) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:317) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.finishSuccess(DnsQueryContext.java:309) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1392) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.processPacket(EpollDatagramChannel.java:662) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.recvmsg(EpollDatagramChannel.java:697) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.access$300(EpollDatagramChannel.java:56) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel$EpollDatagramChannelUnsafe.epollInReady(EpollDatagramChannel.java:536) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:509) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:407) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at java.base/java.lang.Thread.run(Unknown Source) ~[na:na]
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:16:16.090Z ERROR 116 --- [or-http-epoll-3] a.w.r.e.AbstractErrorWebExceptionHandler : [6cdacf20-35]  500 Server Error for HTTP GET "/api/search/search"
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] java.net.UnknownHostException: Failed to resolve 'where-for-dinner-search.workloads' [A(1)] after 2 queries 
where-for-diâ€¦ â”‚ [workload] 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
where-for-diâ€¦ â”‚ [workload] Error has been observed at the following site(s):
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.cloud.gateway.filter.WeightCalculatorWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? AuthorizationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ExceptionTranslationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? LogoutWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerRequestCacheWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? SecurityContextServerWebExchangeWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HttpHeaderWriterWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerWebExchangeReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.security.web.server.WebFilterChainProxy [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.web.filter.reactive.ServerHttpObservationFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HTTP GET "/api/search/search" [ExceptionHandlingWebHandler]
where-for-diâ€¦ â”‚ [workload] Original Stack Trace:
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1044) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:432) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:662) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.access$500(DnsResolveContext.java:66) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:489) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:317) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.finishSuccess(DnsQueryContext.java:309) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1392) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.processPacket(EpollDatagramChannel.java:662) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.recvmsg(EpollDatagramChannel.java:697) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.access$300(EpollDatagramChannel.java:56) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel$EpollDatagramChannelUnsafe.epollInReady(EpollDatagramChannel.java:536) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:509) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:407) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at java.base/java.lang.Thread.run(Unknown Source) ~[na:na]
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:17:16.033Z ERROR 116 --- [or-http-epoll-3] a.w.r.e.AbstractErrorWebExceptionHandler : [6cdacf20-58]  500 Server Error for HTTP GET "/api/search/search"
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] java.net.UnknownHostException: Failed to resolve 'where-for-dinner-search.workloads' [A(1)] after 2 queries 
where-for-diâ€¦ â”‚ [workload] 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
where-for-diâ€¦ â”‚ [workload] Error has been observed at the following site(s):
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.cloud.gateway.filter.WeightCalculatorWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? AuthorizationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ExceptionTranslationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? LogoutWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerRequestCacheWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? SecurityContextServerWebExchangeWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HttpHeaderWriterWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerWebExchangeReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.security.web.server.WebFilterChainProxy [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.web.filter.reactive.ServerHttpObservationFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HTTP GET "/api/search/search" [ExceptionHandlingWebHandler]
where-for-diâ€¦ â”‚ [workload] Original Stack Trace:
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1044) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:432) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:662) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.access$500(DnsResolveContext.java:66) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:489) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:317) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.finishSuccess(DnsQueryContext.java:309) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1392) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.processPacket(EpollDatagramChannel.java:662) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.recvmsg(EpollDatagramChannel.java:697) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.access$300(EpollDatagramChannel.java:56) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel$EpollDatagramChannelUnsafe.epollInReady(EpollDatagramChannel.java:536) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:509) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:407) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at java.base/java.lang.Thread.run(Unknown Source) ~[na:na]
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:18:16.148Z ERROR 116 --- [or-http-epoll-3] a.w.r.e.AbstractErrorWebExceptionHandler : [6cdacf20-81]  500 Server Error for HTTP GET "/api/search/search"
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] java.net.UnknownHostException: Failed to resolve 'where-for-dinner-search.workloads' [A(1)] after 2 queries 
where-for-diâ€¦ â”‚ [workload] 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
where-for-diâ€¦ â”‚ [workload] Error has been observed at the following site(s):
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.cloud.gateway.filter.WeightCalculatorWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? AuthorizationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ExceptionTranslationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? LogoutWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerRequestCacheWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? SecurityContextServerWebExchangeWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HttpHeaderWriterWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerWebExchangeReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.security.web.server.WebFilterChainProxy [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.web.filter.reactive.ServerHttpObservationFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HTTP GET "/api/search/search" [ExceptionHandlingWebHandler]
where-for-diâ€¦ â”‚ [workload] Original Stack Trace:
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1044) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:432) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:662) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.access$500(DnsResolveContext.java:66) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:489) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:317) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.finishSuccess(DnsQueryContext.java:309) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1392) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.processPacket(EpollDatagramChannel.java:662) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.recvmsg(EpollDatagramChannel.java:697) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.access$300(EpollDatagramChannel.java:56) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel$EpollDatagramChannelUnsafe.epollInReady(EpollDatagramChannel.java:536) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:509) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:407) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at java.base/java.lang.Thread.run(Unknown Source) ~[na:na]
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:19:16.061Z ERROR 116 --- [or-http-epoll-3] a.w.r.e.AbstractErrorWebExceptionHandler : [6cdacf20-104]  500 Server Error for HTTP GET "/api/search/search"
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] java.net.UnknownHostException: Failed to resolve 'where-for-dinner-search.workloads' [A(1)] after 2 queries 
where-for-diâ€¦ â”‚ [workload] 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
where-for-diâ€¦ â”‚ [workload] Error has been observed at the following site(s):
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.cloud.gateway.filter.WeightCalculatorWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? AuthorizationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ExceptionTranslationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? LogoutWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerRequestCacheWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? SecurityContextServerWebExchangeWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HttpHeaderWriterWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerWebExchangeReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.security.web.server.WebFilterChainProxy [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.web.filter.reactive.ServerHttpObservationFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HTTP GET "/api/search/search" [ExceptionHandlingWebHandler]
where-for-diâ€¦ â”‚ [workload] Original Stack Trace:
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1044) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:432) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:662) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.access$500(DnsResolveContext.java:66) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:489) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:317) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.finishSuccess(DnsQueryContext.java:309) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1392) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.processPacket(EpollDatagramChannel.java:662) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.recvmsg(EpollDatagramChannel.java:697) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.access$300(EpollDatagramChannel.java:56) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel$EpollDatagramChannelUnsafe.epollInReady(EpollDatagramChannel.java:536) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:509) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:407) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at java.base/java.lang.Thread.run(Unknown Source) ~[na:na]
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:20:16.035Z ERROR 116 --- [or-http-epoll-3] a.w.r.e.AbstractErrorWebExceptionHandler : [6cdacf20-127]  500 Server Error for HTTP GET "/api/search/search"
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] java.net.UnknownHostException: Failed to resolve 'where-for-dinner-search.workloads' [A(1)] after 2 queries 
where-for-diâ€¦ â”‚ [workload] 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
where-for-diâ€¦ â”‚ [workload] Error has been observed at the following site(s):
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.cloud.gateway.filter.WeightCalculatorWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? AuthorizationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ExceptionTranslationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? LogoutWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerRequestCacheWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? SecurityContextServerWebExchangeWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HttpHeaderWriterWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerWebExchangeReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.security.web.server.WebFilterChainProxy [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.web.filter.reactive.ServerHttpObservationFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HTTP GET "/api/search/search" [ExceptionHandlingWebHandler]
where-for-diâ€¦ â”‚ [workload] Original Stack Trace:
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1044) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:432) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:662) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.access$500(DnsResolveContext.java:66) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:489) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:317) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.finishSuccess(DnsQueryContext.java:309) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1392) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.processPacket(EpollDatagramChannel.java:662) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.recvmsg(EpollDatagramChannel.java:697) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.access$300(EpollDatagramChannel.java:56) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel$EpollDatagramChannelUnsafe.epollInReady(EpollDatagramChannel.java:536) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:509) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:407) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at java.base/java.lang.Thread.run(Unknown Source) ~[na:na]
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:21:16.098Z ERROR 116 --- [or-http-epoll-3] a.w.r.e.AbstractErrorWebExceptionHandler : [6cdacf20-151]  500 Server Error for HTTP GET "/api/search/search"
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] java.net.UnknownHostException: Failed to resolve 'where-for-dinner-search.workloads' [A(1)] after 2 queries 
where-for-diâ€¦ â”‚ [workload] 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
where-for-diâ€¦ â”‚ [workload] Error has been observed at the following site(s):
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.cloud.gateway.filter.WeightCalculatorWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? AuthorizationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ExceptionTranslationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? LogoutWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerRequestCacheWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? SecurityContextServerWebExchangeWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HttpHeaderWriterWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerWebExchangeReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.security.web.server.WebFilterChainProxy [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.web.filter.reactive.ServerHttpObservationFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HTTP GET "/api/search/search" [ExceptionHandlingWebHandler]
where-for-diâ€¦ â”‚ [workload] Original Stack Trace:
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1044) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:432) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:662) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.access$500(DnsResolveContext.java:66) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:489) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:317) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.finishSuccess(DnsQueryContext.java:309) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1392) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.processPacket(EpollDatagramChannel.java:662) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.recvmsg(EpollDatagramChannel.java:697) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.access$300(EpollDatagramChannel.java:56) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel$EpollDatagramChannelUnsafe.epollInReady(EpollDatagramChannel.java:536) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:509) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:407) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at java.base/java.lang.Thread.run(Unknown Source) ~[na:na]
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:16.044Z ERROR 116 --- [or-http-epoll-3] a.w.r.e.AbstractErrorWebExceptionHandler : [6cdacf20-174]  500 Server Error for HTTP GET "/api/search/search"
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] java.net.UnknownHostException: Failed to resolve 'where-for-dinner-search.workloads' [A(1)] after 2 queries 
where-for-diâ€¦ â”‚ [workload] 	at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
where-for-diâ€¦ â”‚ [workload] Error has been observed at the following site(s):
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.cloud.gateway.filter.WeightCalculatorWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? AuthorizationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ExceptionTranslationWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? LogoutWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerRequestCacheWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? SecurityContextServerWebExchangeWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HttpHeaderWriterWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? ServerWebExchangeReactorContextWebFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.security.web.server.WebFilterChainProxy [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? org.springframework.web.filter.reactive.ServerHttpObservationFilter [DefaultWebFilterChain]
where-for-diâ€¦ â”‚ [workload] 	*__checkpoint ? HTTP GET "/api/search/search" [ExceptionHandlingWebHandler]
where-for-diâ€¦ â”‚ [workload] Original Stack Trace:
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.finishResolve(DnsResolveContext.java:1097) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.tryToFinishResolve(DnsResolveContext.java:1044) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.query(DnsResolveContext.java:432) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.onResponse(DnsResolveContext.java:662) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext.access$500(DnsResolveContext.java:66) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsResolveContext$2.operationComplete(DnsResolveContext.java:489) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:583) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:559) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:625) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:105) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.trySuccess(DnsQueryContext.java:317) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsQueryContext.finishSuccess(DnsQueryContext.java:309) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.resolver.dns.DnsNameResolver$DnsResponseHandler.channelRead(DnsNameResolver.java:1392) ~[netty-resolver-dns-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) ~[netty-codec-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.processPacket(EpollDatagramChannel.java:662) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.recvmsg(EpollDatagramChannel.java:697) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel.access$300(EpollDatagramChannel.java:56) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollDatagramChannel$EpollDatagramChannelUnsafe.epollInReady(EpollDatagramChannel.java:536) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:509) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:407) ~[netty-transport-classes-epoll-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.97.Final.jar:4.1.97.Final]
where-for-diâ€¦ â”‚ [workload] 		at java.base/java.lang.Thread.run(Unknown Source) ~[na:na]
where-for-diâ€¦ â”‚ [workload] 
Tilt started on http://localhost:53301/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner/where-for-dinner-api-gateway/Tiltfile
Successfully loaded Tiltfile (728.6Âµs)
ERROR: [] "msg"="Reconciler error" "error"="Failed to update API server: create clusters/default: clusters.tilt.dev \"default\" already exists" "controller"="tiltfile" "controllerGroup"="tilt.dev" "controllerKind"="Tiltfile" "Tiltfile"={"name":"(Tiltfile)"} "namespace"="" "name"="(Tiltfile)" "reconcileID"="9883e890-30dd-42a4-aac3-76848194d50f"
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace default --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 5.30s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 5.30s 
where-for-diâ€¦ â”‚ 
Tilt started on http://localhost:53301/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner/where-for-dinner-api-gateway/Tiltfile
Successfully loaded Tiltfile (671.765Âµs)
ERROR: [] "msg"="Reconciler error" "error"="Failed to update API server: create configmaps/where-for-dinner-disable: configmaps.tilt.dev \"where-for-dinner-disable\" already exists" "controller"="tiltfile" "controllerGroup"="tilt.dev" "controllerKind"="Tiltfile" "Tiltfile"={"name":"(Tiltfile)"} "namespace"="" "name"="(Tiltfile)" "reconcileID"="6396c875-8b28-483a-82e6-7021cd51c238"
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 4.99s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 4.99s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Attaching to existing pod (where-for-dinner-00001-deployment-7bdbf5fd7-rqnhw). Only new logs will be streamed.
where-for-diâ€¦ â”‚ [prepare] 
where-for-diâ€¦ â”‚ [detect] 
where-for-diâ€¦ â”‚ [analyze] 
where-for-diâ€¦ â”‚ [restore] 
where-for-diâ€¦ â”‚ [prepare] 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ [place-scripts] 
where-for-diâ€¦ â”‚ [build] 
where-for-diâ€¦ â”‚ [export] 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-build-2-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-2-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	env:
where-for-diâ€¦ â”‚ [prepare] 	- name: BP_OCI_SOURCE
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:4e5a703a9fe15f67baf2680ea5a2bca0ed73ef9c19467616eff5277d29d44069[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:c4eec16b56d526082f27ef41c0d1ab227c7119e04745db471a1d52e50d90b83e[0m
where-for-diâ€¦ â”‚ [prepare] 	- name: BP_LIVE_RELOAD_ENABLED
where-for-diâ€¦ â”‚ [prepare] 	  value: "true"
where-for-diâ€¦ â”‚ [prepare] 	- name: BP_JVM_VERSION
where-for-diâ€¦ â”‚ [prepare] 	  value: "17"
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	source:
where-for-diâ€¦ â”‚ [prepare] 	  blob:
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/4e5a703a9fe15f67baf2680ea5a2bca0ed73ef9c19467616eff5277d29d44069.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/c4eec16b56d526082f27ef41c0d1ab227c7119e04745db471a1d52e50d90b83e.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/c4eec16b56d526082f27ef41c0d1ab227c7119e04745db471a1d52e50d90b83e.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner/c4eec16b56d526082f27ef41c0d1ab227c7119e04745db471a1d52e50d90b83e.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-2-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [analyze detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [event: taskrun workloads/where-for-dinner-config-writer-g727l] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-config-writer-g727l-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-g727l-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/04 14:25:17 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-g727l-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-g727l-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/04 14:25:18 Decoded script /tekton/scripts/script-0-vxksr
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.lqoPGlM6Pu
where-for-diâ€¦ â”‚ + echo -e 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
where-for-diâ€¦ â”‚ + base64 --decode
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:4e5a703a9fe15f67baf2680ea5a2bca0ed73ef9c19467616eff5277d29d44069
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:c4eec16b56d526082f27ef41c0d1ab227c7119e04745db471a1d52e50d90b83e
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:2fb660aa09e421a4d082f23c9c6cfee17b35d591a0d5515059fe7b53b65d2327
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:4e5a703a9fe15f67baf2680ea5a2bca0ed73ef9c19467616eff5277d29d44069
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:c4eec16b56d526082f27ef41c0d1ab227c7119e04745db471a1d52e50d90b83e
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:2fb660aa09e421a4d082f23c9c6cfee17b35d591a0d5515059fe7b53b65d2327
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads-bundle:1d2109d6-30c3-4b4e-83c9-9bf2b23d8bdc -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 4s
where-for-diâ€¦ â”‚      â”Š Completed       - 5s
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-00002-deployment-559b899fc5-7mzwg):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:25:44.97286113Z","logger":"queueproxy","caller":"sharedmain/main.go:260","message":"Starting queue-proxy","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00002","knative.dev/pod":"where-for-dinner-00002-deployment-559b899fc5-7mzwg"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:25:44.973878762Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server metrics:9090","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00002","knative.dev/pod":"where-for-dinner-00002-deployment-559b899fc5-7mzwg"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:25:44.987437838Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server main:8012","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00002","knative.dev/pod":"where-for-dinner-00002-deployment-559b899fc5-7mzwg"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:25:44.987496946Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server admin:8022","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00002","knative.dev/pod":"where-for-dinner-00002-deployment-559b899fc5-7mzwg"}
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload]   .   ____          _            __ _ _
where-for-diâ€¦ â”‚ [workload]  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
where-for-diâ€¦ â”‚ [workload] ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
where-for-diâ€¦ â”‚ [workload]  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
where-for-diâ€¦ â”‚ [workload]   '  |____| .__|_| |_|_| |_\__, | / / / /
where-for-diâ€¦ â”‚ [workload]  =========|_|==============|___/=/_/_/_/
where-for-diâ€¦ â”‚ [workload]  :: Spring Boot ::                (v3.1.3)
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:25:50.256Z  INFO 87 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : Starting DinnerAPIGatewayApplication v0.0.1-SNAPSHOT using Java 17.0.9 with PID 87 (/workspace/BOOT-INF/classes started by cnb in /workspace)
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:25:50.269Z  INFO 87 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : The following 1 profile is active: "kubernetes"
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:25:55.869Z  INFO 87 --- [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=13afbb8a-a579-326a-b409-ed4b39bebfd3
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:25:57.697Z  INFO 87 --- [           main] ctiveUserDetailsServiceAutoConfiguration : 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] Using generated security password: 56509e1d-3e18-400a-af6b-9e824d4daa85
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.411Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [After]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.412Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Before]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.413Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Between]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.413Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Cookie]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.413Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Header]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.413Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Host]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.413Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Method]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.413Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Path]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.413Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Query]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.413Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [ReadBody]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.414Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [RemoteAddr]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.414Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [XForwardedRemoteAddr]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.414Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Weight]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:00.414Z  INFO 87 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [CloudFoundryRouteService]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:03.409Z  INFO 87 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8080
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:04.012Z  INFO 87 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 15 endpoint(s) beneath base path '/actuator'
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:04.322Z  INFO 87 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8081
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:26:04.803Z  INFO 87 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : Started DinnerAPIGatewayApplication in 17.77 seconds (process running for 18.879)
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”Š Ready           - 31s
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer started at 2024-01-04T14:26:40Z
where-for-diâ€¦ â”‚ [analyze] Restoring data for SBOM from previous image
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer ran for 8.044224345s and ended at 2024-01-04T14:26:48Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-2-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/dotnet-core-lite@2.9.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/go-lite@2.2.3' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/nodejs-lite@2.4.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] Timer: Detector started at 2024-01-04T14:26:49Z
where-for-diâ€¦ â”‚ [detect] 11 of 26 buildpacks participating
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/ca-certificates   3.6.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/bellsoft-liberica 9.13.0
where-for-diâ€¦ â”‚ [detect] paketo-buildpacks/syft             1.10.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/maven             6.15.12
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/watchexec         2.8.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/executable-jar    6.8.3
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/apache-tomcat     7.14.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/liberty           3.8.11
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/dist-zip          5.6.8
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/spring-boot       5.27.6
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/image-labels      4.5.6
where-for-diâ€¦ â”‚ [detect] Timer: Detector ran for 2.265351101s and ended at 2024-01-04T14:26:51Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-2-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [restore] Timer: Restorer started at 2024-01-04T14:26:52Z
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/ca-certificates:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:java-security-properties" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:jre" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:jdk" from cache
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "paketo-buildpacks/syft:syft" from cache
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/maven:application" from cache
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/maven:cache" from cache
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/watchexec:watchexec" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:spring-cloud-bindings" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:web-application-type" from app image
where-for-diâ€¦ â”‚ [restore] Restoring data for "tanzu-buildpacks/bellsoft-liberica:jdk" from cache
where-for-diâ€¦ â”‚ [restore] Restoring data for "paketo-buildpacks/syft:syft" from cache
where-for-diâ€¦ â”‚ [restore] Restoring data for "tanzu-buildpacks/maven:application" from cache
where-for-diâ€¦ â”‚ [restore] Restoring data for "tanzu-buildpacks/maven:cache" from cache
where-for-diâ€¦ â”‚ [restore] Restoring data for SBOM from cache
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:26:58.356214013Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.105:8022 map[] map[] <nil> map[] 10.200.1.1:46854 /wait-for-drain <nil> <nil> <nil> 0xc0001f0f50}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-7bdbf5fd7-rqnhw"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:26:58.383451251Z","logger":"queueproxy","caller":"sharedmain/main.go:290","message":"Received TERM signal, attempting to gracefully shutdown servers.","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-7bdbf5fd7-rqnhw"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:26:58.38363584Z","logger":"queueproxy","caller":"sharedmain/main.go:291","message":"Sleeping 30s to allow K8s propagation of non-ready state","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-7bdbf5fd7-rqnhw"}
where-for-diâ€¦ â”‚ [restore] Timer: Restorer ran for 14.88752239s and ended at 2024-01-04T14:27:07Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-2-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [build] Timer: Builder started at 2024-01-04T14:27:08Z
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for CA Certificates[0m[34m 3.6.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-ca-certificates[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for BellSoft Liberica[0m[34m 9.13.0[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-bellsoft-liberica[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ARGS           --no-man-pages --no-header-files --strip-debug --compress=1  configure custom link arguments (--output must be omitted)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ENABLED        false                                                        enables running jlink tool to generate custom JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_TYPE                 JRE                                                          the JVM type - JDK or JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_VERSION              17                                                           the Java version[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_ENABLED           false                                                        enables Java remote debugging support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_PORT              8000                                                         configure the remote debugging port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_SUSPEND           false                                                        configure whether to suspend execution until a debugger has attached[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_HEAP_DUMP_PATH                                                                       write heap dumps on error to this path[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_ENABLED        true                                                         enables Java Native Memory Tracking (NMT)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_LEVEL          summary                                                      configure level of NMT, summary or detail[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ARGS                                                                             configure custom Java Flight Recording (JFR) arguments[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ENABLED             false                                                        enables Java Flight Recording (JFR)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_ENABLED             false                                                        enables Java Management Extensions (JMX)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_PORT                5000                                                         configure the JMX port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_HEAD_ROOM           0                                                            the headroom in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_LOADED_CLASS_COUNT  35% of classes                                               the number of loaded classes in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_THREAD_COUNT        250                                                          the number of threads in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $JAVA_TOOL_OPTIONS                                                                        the JVM launch flags[0m
where-for-diâ€¦ â”‚ [build] [2m    [2mUsing Java version 17 from BP_JVM_VERSION[0m[2m[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JDK 17.0.9[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JRE 17.0.9[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mJava Security Properties[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mPaketo Syft Buildpack[0m[34m 1.10.1[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/paketo-buildpacks/syft[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Maven[0m[34m 6.15.12[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-maven[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_EXCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are removed[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_INCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are included[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JAVA_INSTALL_NODE                 false                                                  whether to install Yarn/Node binaries based on the presence of a package.json or yarn.lock file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ACTIVE_PROFILES                                                                    the active profiles (comma separated: such as: p1,!p2,?p3) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ADDITIONAL_BUILD_ARGUMENTS                                                         the additionnal arguments (appended to BP_MAVEN_BUILD_ARGUMENTS) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILD_ARGUMENTS             -Dmaven.test.skip=true --no-transfer-progress package  the arguments to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_ARTIFACT              target/*.[ejw]ar                                       the built application artifact explicitly.  Supersedes $BP_MAVEN_BUILT_MODULE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_MODULE                                                                       the module to find application artifact in[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_DAEMON_ENABLED              false                                                  use maven daemon[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_POM_FILE                    pom.xml                                                the location of the main pom.xml file, relative to the application root[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_SETTINGS_PATH                                                                      the path to a Maven settings file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_VERSION                     3                                                      the Maven version[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_NODE_PROJECT_PATH                                                                        configure a project subdirectory to look for `package.json` and `yarn.lock` files[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating cache directory /home/cnb/.m2[0m
where-for-diâ€¦ â”‚ [build]   [34mCompiled Application[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Executing mvnw --batch-mode -Dmaven.test.skip=true --no-transfer-progress package[0m
where-for-diâ€¦ â”‚ [build]         % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
where-for-diâ€¦ â”‚ [build]                                        Dload  Upload   Total   Spent    Left  Speed
where-for-diâ€¦ â”‚ [build]         0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 59925  100 59925    0     0   173k      0 --:--:-- --:--:-- --:--:--  173k
where-for-diâ€¦ â”‚ [build]       [0m[0m[INFO] Scanning for projects...
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --------------< com.example:where-for-dinner-api-gateway >--------------
where-for-diâ€¦ â”‚ [build]       [INFO] Building where-for-dinner-api-gateway 0.0.1-SNAPSHOT
where-for-diâ€¦ â”‚ [build]       [INFO] --------------------------------[ jar ]---------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 1 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 0 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Changes detected - recompiling the module! :input tree
where-for-diâ€¦ â”‚ [build]       [INFO] Compiling 1 source file with javac [debug release 17] to target/classes
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[40,17] httpBasic() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[41,13] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[42,13] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[60,12] authorizeExchange() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[63,12] and() in org.springframework.security.config.web.server.ServerHttpSecurity.AuthorizeExchangeSpec has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[65,8] logout() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [WARNING] /workspace/src/main/java/com/java/example/tanzu/wherefordinner/DinnerAPIGatewayApplication.java:[70,12] csrf() in org.springframework.security.config.web.server.ServerHttpSecurity has been deprecated and marked for removal
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:testResources (default-testResources) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not copying test resources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:testCompile (default-testCompile) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not compiling test sources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-surefire-plugin:3.0.0:test (default-test) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Tests are skipped.
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-jar-plugin:3.3.0:jar (default-jar) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Building jar: /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- spring-boot-maven-plugin:3.1.3:repackage (repackage) @ where-for-dinner-api-gateway ---
where-for-diâ€¦ â”‚ [build]       [INFO] Replacing main artifact /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar with repackaged archive, adding nested dependencies in BOOT-INF/.
where-for-diâ€¦ â”‚ [build]       [INFO] The original artifact has been renamed to /workspace/target/where-for-dinner-api-gateway-0.0.1-SNAPSHOT.jar.original
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] BUILD SUCCESS
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] Total time:  8.567 s
where-for-diâ€¦ â”‚ [build]       [INFO] Finished at: 2024-01-04T14:27:19Z
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [0m[0m
where-for-diâ€¦ â”‚ [build]   Removing source code
where-for-diâ€¦ â”‚ [build]   Restoring application artifact
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Watchexec[0m[34m 2.8.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-watchexec[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/watchexec/watchexec/releases/download/cli-v1.20.5/watchexec-1.20.5-x86_64-unknown-linux-musl.tar.xz[0m
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:27:28.360593786Z","logger":"queueproxy","caller":"sharedmain/main.go:295","message":"Shutting down server: admin","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-7bdbf5fd7-rqnhw"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:27:28.363006792Z","logger":"queueproxy","caller":"sharedmain/main.go:295","message":"Shutting down server: metrics","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-7bdbf5fd7-rqnhw"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:27:28.363965668Z","logger":"queueproxy","caller":"sharedmain/main.go:295","message":"Shutting down server: main","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-7bdbf5fd7-rqnhw"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:27:28.364167516Z","logger":"queueproxy","caller":"sharedmain/main.go:306","message":"Shutdown complete, exiting...","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00001","knative.dev/pod":"where-for-dinner-00001-deployment-7bdbf5fd7-rqnhw"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Executable JAR[0m[34m 6.8.3[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-executable-jar[0m
where-for-diâ€¦ â”‚ [build]   [34mClass Path[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.delim[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.prepend[0m
where-for-diâ€¦ â”‚ [build]   Process types:
where-for-diâ€¦ â”‚ [build]     [36mexecutable-jar[0m: java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mreload[0m:         watchexec -r --shell=none -- java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mtask[0m:           java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mweb[0m:            java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build] SKIPPED: `Main-Class` found in `META-INF/MANIFEST.MF`, skipping build
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Spring Boot[0m[34m 5.27.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-spring-boot[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_DISABLED   false  whether to contribute Spring Boot cloud bindings support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_VERSION    1      default version of Spring Cloud Bindings library to contribute[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_DISABLED  false  whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_ENABLED   true   Deprecated - whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build]   Creating slices from layers index
where-for-diâ€¦ â”‚ [build] [2m    dependencies (74.3 MB)[0m
where-for-diâ€¦ â”‚ [build] [2m    spring-boot-loader (269.4 KB)[0m
where-for-diâ€¦ â”‚ [build] [2m    snapshot-dependencies (0.0 B)[0m
where-for-diâ€¦ â”‚ [build] [2m    application (59.1 KB)[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mSpring Cloud Bindings 2.0.2[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-bindings/2.0.2/spring-cloud-bindings-2.0.2.jar[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Copying to /layers/tanzu-buildpacks_spring-boot/spring-cloud-bindings[0m
where-for-diâ€¦ â”‚ [build]   [34mWeb Application Type[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   4 application slices
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.title
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.version
where-for-diâ€¦ â”‚ [build]     org.springframework.boot.version
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Image Labels[0m[34m 4.5.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-image-labels[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_IMAGE_LABELS                                                                                                                                                    arbitrary image labels[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_AUTHORS                                                                                                                                                     the org.opencontainers.image.authors image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_CREATED                                                                                                                                                     the org.opencontainers.image.created image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DESCRIPTION                                                                                                                                                 the org.opencontainers.image.description image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DOCUMENTATION                                                                                                                                               the org.opencontainers.image.documentation image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_LICENSES                                                                                                                                                    the org.opencontainers.image.licenses image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REF_NAME                                                                                                                                                    the org.opencontainers.image.ref.name image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REVISION                                                                                                                                                    the org.opencontainers.image.revision image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_SOURCE         tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:c4eec16b56d526082f27ef41c0d1ab227c7119e04745db471a1d52e50d90b83e  the org.opencontainers.image.source image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_TITLE                                                                                                                                                       the org.opencontainers.image.title image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_URL                                                                                                                                                         the org.opencontainers.image.url image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VENDOR                                                                                                                                                      the org.opencontainers.image.vendor image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VERSION                                                                                                                                                     the org.opencontainers.image.version image label[0m
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.source
where-for-diâ€¦ â”‚ [build] Timer: Builder ran for 30.046172876s and ended at 2024-01-04T14:27:38Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-2-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [export] Reusing layers from image 'tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:2fb660aa09e421a4d082f23c9c6cfee17b35d591a0d5515059fe7b53b65d2327'
where-for-diâ€¦ â”‚ [export] Timer: Exporter started at 2024-01-04T14:29:14Z
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/ca-certificates:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:java-security-properties'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:jre'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/watchexec:watchexec'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/executable-jar:classpath'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:spring-cloud-bindings'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:web-application-type'
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:launch.sbom'
where-for-diâ€¦ â”‚ [export] Reusing 5/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:launcher'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:config'
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:process-types'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.lifecycle.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.build.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.project.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.title'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.springframework.boot.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.source'
where-for-diâ€¦ â”‚ [export] Setting default process type 'reload'
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads... started at 2024-01-04T14:29:16Z
where-for-diâ€¦ â”‚ [export] *** Images (sha256:85fac912d10abc4ebad4576afd375973ac5c16efd40680a1ade7e67c16cc1872):
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads:b2.20240104.142453
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads... ran for 3.976349195s and ended at 2024-01-04T14:29:19Z
where-for-diâ€¦ â”‚ [export] Timer: Exporter ran for 5.276003409s and ended at 2024-01-04T14:29:19Z
where-for-diâ€¦ â”‚ [export] Timer: Cache started at 2024-01-04T14:29:20Z
where-for-diâ€¦ â”‚ [export] Reusing cache layer 'tanzu-buildpacks/bellsoft-liberica:jdk'
where-for-diâ€¦ â”‚ [export] Reusing cache layer 'paketo-buildpacks/syft:syft'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:application'
where-for-diâ€¦ â”‚ [export] Reusing cache layer 'tanzu-buildpacks/maven:cache'
where-for-diâ€¦ â”‚ [export] Reusing cache layer 'buildpacksio/lifecycle:cache.sbom'
where-for-diâ€¦ â”‚ [export] Timer: Cache ran for 15.839722385s and ended at 2024-01-04T14:29:35Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-build-2-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:0b5c1bfe87a700cf473725e7cd133f4c7da48ec85baccf2d0c5d6127beb675e7" already present on machine
where-for-diâ€¦ â”‚ Build successful
where-for-diâ€¦ â”‚ [event: taskrun workloads/where-for-dinner-config-writer-j6n8m] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-config-writer-j6n8m-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-j6n8m-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/04 14:30:07 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-j6n8m-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-config-writer-j6n8m-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/04 14:30:08 Decoded script /tekton/scripts/script-0-76hvx
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.aic5NT4dft
where-for-diâ€¦ â”‚ + base64 --decode
where-for-diâ€¦ â”‚ + echo -e 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
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:c4eec16b56d526082f27ef41c0d1ab227c7119e04745db471a1d52e50d90b83e
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:c4eec16b56d526082f27ef41c0d1ab227c7119e04745db471a1d52e50d90b83e
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:85fac912d10abc4ebad4576afd375973ac5c16efd40680a1ade7e67c16cc1872
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-scg
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:c4eec16b56d526082f27ef41c0d1ab227c7119e04745db471a1d52e50d90b83e
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner@sha256:c4eec16b56d526082f27ef41c0d1ab227c7119e04745db471a1d52e50d90b83e
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-api-gateway
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot_spring-cloud-gateway
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads@sha256:85fac912d10abc4ebad4576afd375973ac5c16efd40680a1ade7e67c16cc1872
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-workloads-bundle:1d2109d6-30c3-4b4e-83c9-9bf2b23d8bdc -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 4s
where-for-diâ€¦ â”‚      â”Š Completed       - 5s
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-00003-deployment-59779b4984-d56wh):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:30:50.734639375Z","logger":"queueproxy","caller":"sharedmain/main.go:260","message":"Starting queue-proxy","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00003","knative.dev/pod":"where-for-dinner-00003-deployment-59779b4984-d56wh"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:30:50.734981643Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server admin:8022","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00003","knative.dev/pod":"where-for-dinner-00003-deployment-59779b4984-d56wh"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:30:50.742129847Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server metrics:9090","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00003","knative.dev/pod":"where-for-dinner-00003-deployment-59779b4984-d56wh"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:30:50.74217953Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server main:8012","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00003","knative.dev/pod":"where-for-dinner-00003-deployment-59779b4984-d56wh"}
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload]   .   ____          _            __ _ _
where-for-diâ€¦ â”‚ [workload]  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
where-for-diâ€¦ â”‚ [workload] ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
where-for-diâ€¦ â”‚ [workload]  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
where-for-diâ€¦ â”‚ [workload]   '  |____| .__|_| |_|_| |_\__, | / / / /
where-for-diâ€¦ â”‚ [workload]  =========|_|==============|___/=/_/_/_/
where-for-diâ€¦ â”‚ [workload]  :: Spring Boot ::                (v3.1.3)
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:30:55.617Z  INFO 111 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : Starting DinnerAPIGatewayApplication v0.0.1-SNAPSHOT using Java 17.0.9 with PID 111 (/workspace/BOOT-INF/classes started by cnb in /workspace)
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:30:55.630Z  INFO 111 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : The following 1 profile is active: "kubernetes"
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:00.805Z  INFO 111 --- [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=13afbb8a-a579-326a-b409-ed4b39bebfd3
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:02.286Z  INFO 111 --- [           main] ctiveUserDetailsServiceAutoConfiguration : 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] Using generated security password: 22a3f7ef-7050-44e4-8625-91d8b1944140
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.420Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [After]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.420Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Before]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.420Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Between]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.420Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Cookie]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.420Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Header]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.421Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Host]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.421Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Method]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.421Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Path]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.421Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Query]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.421Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [ReadBody]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.421Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [RemoteAddr]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.421Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [XForwardedRemoteAddr]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.421Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Weight]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:04.421Z  INFO 111 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [CloudFoundryRouteService]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:07.099Z  INFO 111 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8080
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:07.392Z  INFO 111 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 15 endpoint(s) beneath base path '/actuator'
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:07.905Z  INFO 111 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8081
where-for-diâ€¦ â”‚ [workload] 2024-01-04T14:31:08.319Z  INFO 111 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : Started DinnerAPIGatewayApplication in 16.237 seconds (process running for 17.395)
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”Š Ready           - 31s
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:31:57.254971465Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.113:8022 map[] map[] <nil> map[] 10.200.1.1:37918 /wait-for-drain <nil> <nil> <nil> 0xc00007fae0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00002","knative.dev/pod":"where-for-dinner-00002-deployment-559b899fc5-7mzwg"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:31:57.270744853Z","logger":"queueproxy","caller":"sharedmain/main.go:290","message":"Received TERM signal, attempting to gracefully shutdown servers.","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00002","knative.dev/pod":"where-for-dinner-00002-deployment-559b899fc5-7mzwg"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:31:57.270838521Z","logger":"queueproxy","caller":"sharedmain/main.go:291","message":"Sleeping 30s to allow K8s propagation of non-ready state","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00002","knative.dev/pod":"where-for-dinner-00002-deployment-559b899fc5-7mzwg"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:32:27.258970368Z","logger":"queueproxy","caller":"sharedmain/main.go:295","message":"Shutting down server: main","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00002","knative.dev/pod":"where-for-dinner-00002-deployment-559b899fc5-7mzwg"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:32:27.264205463Z","logger":"queueproxy","caller":"sharedmain/main.go:295","message":"Shutting down server: admin","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00002","knative.dev/pod":"where-for-dinner-00002-deployment-559b899fc5-7mzwg"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:32:27.264615755Z","logger":"queueproxy","caller":"sharedmain/main.go:295","message":"Shutting down server: metrics","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00002","knative.dev/pod":"where-for-dinner-00002-deployment-559b899fc5-7mzwg"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T14:32:27.265443165Z","logger":"queueproxy","caller":"sharedmain/main.go:306","message":"Shutdown complete, exiting...","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00002","knative.dev/pod":"where-for-dinner-00002-deployment-559b899fc5-7mzwg"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] 
where-for-diâ€¦ â”‚ [queue-proxy] Error streaming where-for-dinner-00003-deployment-59779b4984-d56wh logs: Get "https://192.168.112.16:10250/containerLogs/workloads/where-for-dinner-00003-deployment-59779b4984-d56wh/queue-proxy?follow=true&sinceTime=2024-01-05T03%3A33%3A17Z": dial timeout, backstop
where-for-diâ€¦ â”‚ [workload] Error streaming where-for-dinner-00003-deployment-59779b4984-d56wh logs: Get "https://192.168.112.16:10250/containerLogs/workloads/where-for-dinner-00003-deployment-59779b4984-d56wh/workload?follow=true&sinceTime=2024-01-05T03%3A33%3A17Z": dial timeout, backstop
where-for-diâ€¦ â”‚ [queue-proxy] Error streaming where-for-dinner-00003-deployment-59779b4984-d56wh logs: Get "https://192.168.112.16:10250/containerLogs/workloads/where-for-dinner-00003-deployment-59779b4984-d56wh/queue-proxy?follow=true&sinceTime=2024-01-05T03%3A33%3A48Z": dial timeout, backstop
where-for-diâ€¦ â”‚ [workload] Error streaming where-for-dinner-00003-deployment-59779b4984-d56wh logs: Get "https://192.168.112.16:10250/containerLogs/workloads/where-for-dinner-00003-deployment-59779b4984-d56wh/workload?follow=true&sinceTime=2024-01-05T03%3A33%3A48Z": dial timeout, backstop
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 3 times): Get "http://127.0.0.1:8080/readyz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"ERROR","timestamp":"2024-01-05T03:34:24.169219805Z","logger":"queueproxy","caller":"network/error_handler.go:33","message":"error reverse proxying request; sockstat: sockets: used 24\nTCP: inuse 14 orphan 7 tw 1 alloc 189 mem 633\nUDP: inuse 4 mem 0\nUDPLITE: inuse 0\nRAW: inuse 0\nFRAG: inuse 0 memory 0\n","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00003","knative.dev/pod":"where-for-dinner-00003-deployment-59779b4984-d56wh","error":"context canceled","stacktrace":"knative.dev/pkg/network.ErrorHandler.func1\n\tknative.dev/pkg@v0.0.0-20230718152110-aef227e72ead/network/error_handler.go:33\nnet/http/httputil.(*ReverseProxy).ServeHTTP\n\tnet/http/httputil/reverseproxy.go:475\nknative.dev/serving/pkg/queue.(*appRequestMetricsHandler).ServeHTTP\n\tknative.dev/serving/pkg/queue/request_metric.go:199\nknative.dev/serving/pkg/queue/sharedmain.mainHandler.ProxyHandler.func3\n\tknative.dev/serving/pkg/queue/handler.go:76\nnet/http.HandlerFunc.ServeHTTP\n\tnet/http/server.go:2136\nknative.dev/serving/pkg/queue/sharedmain.mainHandler.ForwardedShimHandler.func4\n\tknative.dev/serving/pkg/queue/forwarded_shim.go:54\nnet/http.HandlerFunc.ServeHTTP\n\tnet/http/server.go:2136\nknative.dev/serving/pkg/http/handler.(*timeoutHandler).ServeHTTP.func4\n\tknative.dev/serving/pkg/http/handler/timeout.go:118"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”Š Ready           - 13h3m59s
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00003-deployment-59779b4984-d56wh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 169 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload]   .   ____          _            __ _ _
where-for-diâ€¦ â”‚ [workload]  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
where-for-diâ€¦ â”‚ [workload] ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
where-for-diâ€¦ â”‚ [workload]  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
where-for-diâ€¦ â”‚ [workload]   '  |____| .__|_| |_|_| |_\__, | / / / /
where-for-diâ€¦ â”‚ [workload]  =========|_|==============|___/=/_/_/_/
where-for-diâ€¦ â”‚ [workload]  :: Spring Boot ::                (v3.1.3)
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:35:14.481Z  INFO 195 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : Starting DinnerAPIGatewayApplication v0.0.1-SNAPSHOT using Java 17.0.9 with PID 195 (/workspace/BOOT-INF/classes started by cnb in /workspace)
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:35:14.489Z  INFO 195 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : The following 1 profile is active: "kubernetes"
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-05T03:35:17.314031173Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.116:8022 map[] map[] <nil> map[] 10.200.1.1:45880 /wait-for-drain <nil> <nil> <nil> 0xc0000e1f40}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00003","knative.dev/pod":"where-for-dinner-00003-deployment-59779b4984-d56wh"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 145 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00003-deployment-59779b4984-d56wh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 140 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 137 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload]   .   ____          _            __ _ _
where-for-diâ€¦ â”‚ [workload]  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
where-for-diâ€¦ â”‚ [workload] ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
where-for-diâ€¦ â”‚ [workload]  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
where-for-diâ€¦ â”‚ [workload]   '  |____| .__|_| |_|_| |_\__, | / / / /
where-for-diâ€¦ â”‚ [workload]  =========|_|==============|___/=/_/_/_/
where-for-diâ€¦ â”‚ [workload]  :: Spring Boot ::                (v3.1.3)
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:35:44.284Z  INFO 176 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : Starting DinnerAPIGatewayApplication v0.0.1-SNAPSHOT using Java 17.0.9 with PID 176 (/workspace/BOOT-INF/classes started by cnb in /workspace)
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:35:44.290Z  INFO 176 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : The following 1 profile is active: "kubernetes"
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-05T03:35:48.026460047Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.116:8022 map[] map[] <nil> map[] 10.200.1.1:45880 /wait-for-drain <nil> <nil> <nil> 0xc0005cd900}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00003","knative.dev/pod":"where-for-dinner-00003-deployment-59779b4984-d56wh"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 130 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00003-deployment-59779b4984-d56wh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 154 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 123 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-05T03:36:17.87731887Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.116:8022 map[] map[] <nil> map[] 10.200.1.1:45880 /wait-for-drain <nil> <nil> <nil> 0xc0002ab4f0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00003","knative.dev/pod":"where-for-dinner-00003-deployment-59779b4984-d56wh"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 151 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload]   .   ____          _            __ _ _
where-for-diâ€¦ â”‚ [workload]  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
where-for-diâ€¦ â”‚ [workload] ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
where-for-diâ€¦ â”‚ [workload]  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
where-for-diâ€¦ â”‚ [workload]   '  |____| .__|_| |_|_| |_\__, | / / / /
where-for-diâ€¦ â”‚ [workload]  =========|_|==============|___/=/_/_/_/
where-for-diâ€¦ â”‚ [workload]  :: Spring Boot ::                (v3.1.3)
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 97 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 138 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 163 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00003-deployment-59779b4984-d56wh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [queue-proxy] Error streaming where-for-dinner-00003-deployment-59779b4984-d56wh logs: Get "https://192.168.112.16:10250/containerLogs/workloads/where-for-dinner-00003-deployment-59779b4984-d56wh/queue-proxy?follow=true&sinceTime=2024-01-05T03%3A36%3A52Z": http2: client connection lost
where-for-diâ€¦ â”‚ [workload] Error streaming where-for-dinner-00003-deployment-59779b4984-d56wh logs: Get "https://192.168.112.16:10250/containerLogs/workloads/where-for-dinner-00003-deployment-59779b4984-d56wh/workload?follow=true&sinceTime=2024-01-05T03%3A36%3A55Z": http2: client connection lost
where-for-diâ€¦ â”‚ [workload] Error streaming where-for-dinner-00003-deployment-59779b4984-d56wh logs: Get "https://192.168.112.16:10250/containerLogs/workloads/where-for-dinner-00003-deployment-59779b4984-d56wh/workload?follow=true&sinceTime=2024-01-05T03%3A37%3A38Z": dial timeout, backstop
where-for-diâ€¦ â”‚ [queue-proxy] Error streaming where-for-dinner-00003-deployment-59779b4984-d56wh logs: Get "https://192.168.112.16:10250/containerLogs/workloads/where-for-dinner-00003-deployment-59779b4984-d56wh/queue-proxy?follow=true&sinceTime=2024-01-05T03%3A37%3A38Z": dial timeout, backstop
where-for-diâ€¦ â”‚ [workload] Error streaming where-for-dinner-00003-deployment-59779b4984-d56wh logs: Get "https://192.168.112.16:10250/containerLogs/workloads/where-for-dinner-00003-deployment-59779b4984-d56wh/workload?follow=true&sinceTime=2024-01-05T03%3A38%3A16Z": dial timeout, backstop
where-for-diâ€¦ â”‚ [queue-proxy] Error streaming where-for-dinner-00003-deployment-59779b4984-d56wh logs: Get "https://192.168.112.16:10250/containerLogs/workloads/where-for-dinner-00003-deployment-59779b4984-d56wh/queue-proxy?follow=true&sinceTime=2024-01-05T03%3A38%3A16Z": dial timeout, backstop
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner (53302 -> 8080): error upgrading connection: error dialing backend: dial timeout, backstop
where-for-diâ€¦ â”‚ [workload] Error streaming where-for-dinner-00003-deployment-59779b4984-d56wh logs: Get "https://192.168.112.16:10250/containerLogs/workloads/where-for-dinner-00003-deployment-59779b4984-d56wh/workload?follow=true&sinceTime=2024-01-05T03%3A38%3A50Z": dial timeout, backstop
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 17 times): Get "http://127.0.0.1:8080/readyz": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 16 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 80 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 117 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-05T03:39:35.497410246Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.116:8022 map[] map[] <nil> map[] 10.200.1.1:45880 /wait-for-drain <nil> <nil> <nil> 0xc000390fa0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00003","knative.dev/pod":"where-for-dinner-00003-deployment-59779b4984-d56wh"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 167 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 148 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 147 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 186 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 176 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 101 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 124 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 132 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 80 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00003-deployment-59779b4984-d56wh. Container: workload.
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 131 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 154 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 143 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-05T03:41:50.245018772Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.116:8022 map[] map[] <nil> map[] 10.200.1.1:45880 /wait-for-drain <nil> <nil> <nil> 0xc000594780}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00003","knative.dev/pod":"where-for-dinner-00003-deployment-59779b4984-d56wh"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 147 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 161 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 162 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 180 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 141 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 160 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 176 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 184 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 189 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 183 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 177 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 173 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 188 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 152 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 166 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 177 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 122 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 106 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00003-deployment-59779b4984-d56wh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 138 times): Get "http://127.0.0.1:8080/readyz": context deadline exceeded
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 69 times): Get "http://127.0.0.1:8080/readyz": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-05T03:46:02.617575167Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.116:8022 map[] map[] <nil> map[] 10.200.1.1:45880 /wait-for-drain <nil> <nil> <nil> 0xc000634280}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00003","knative.dev/pod":"where-for-dinner-00003-deployment-59779b4984-d56wh"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 47 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 98 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [workload queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 101 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 112 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 82 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 98 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 147 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 171 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 124 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 194 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 187 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 187 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 190 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 188 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 173 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00003-deployment-59779b4984-d56wh. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 131 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 156 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload]   .   ____          _            __ _ _
where-for-diâ€¦ â”‚ [workload]  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
where-for-diâ€¦ â”‚ [workload] ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
where-for-diâ€¦ â”‚ [workload]  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
where-for-diâ€¦ â”‚ [workload]   '  |____| .__|_| |_|_| |_\__, | / / / /
where-for-diâ€¦ â”‚ [workload]  =========|_|==============|___/=/_/_/_/
where-for-diâ€¦ â”‚ [workload]  :: Spring Boot ::                (v3.1.3)
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:49:32.458Z  INFO 100 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : Starting DinnerAPIGatewayApplication v0.0.1-SNAPSHOT using Java 17.0.9 with PID 100 (/workspace/BOOT-INF/classes started by cnb in /workspace)
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:49:32.600Z  INFO 100 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : The following 1 profile is active: "kubernetes"
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 125 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-05T03:49:31.114566678Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.116:8022 map[] map[] <nil> map[] 10.200.1.1:45880 /wait-for-drain <nil> <nil> <nil> 0xc0005844b0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00003","knative.dev/pod":"where-for-dinner-00003-deployment-59779b4984-d56wh"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 93 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 102 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 123 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 90 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 105 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 147 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 162 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 181 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 180 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 152 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 156 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 167 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 166 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 181 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 190 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 192 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 83 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 178 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 34 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 155 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 116 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 166 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 165 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 181 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 186 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00003-deployment-59779b4984-d56wh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload]   .   ____          _            __ _ _
where-for-diâ€¦ â”‚ [workload]  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
where-for-diâ€¦ â”‚ [workload] ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
where-for-diâ€¦ â”‚ [workload]  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
where-for-diâ€¦ â”‚ [workload]   '  |____| .__|_| |_|_| |_\__, | / / / /
where-for-diâ€¦ â”‚ [workload]  =========|_|==============|___/=/_/_/_/
where-for-diâ€¦ â”‚ [workload]  :: Spring Boot ::                (v3.1.3)
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:50.398Z  INFO 91 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : Starting DinnerAPIGatewayApplication v0.0.1-SNAPSHOT using Java 17.0.9 with PID 91 (/workspace/BOOT-INF/classes started by cnb in /workspace)
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:50.404Z  INFO 91 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : The following 1 profile is active: "kubernetes"
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:55.412Z  INFO 91 --- [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=13afbb8a-a579-326a-b409-ed4b39bebfd3
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:56.990Z  INFO 91 --- [           main] ctiveUserDetailsServiceAutoConfiguration : 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] Using generated security password: b2600f9a-53d0-4309-842d-2f49dc074566
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.592Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [After]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.592Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Before]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.592Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Between]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.592Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Cookie]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.592Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Header]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.592Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Host]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.592Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Method]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.593Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Path]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.593Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Query]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.593Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [ReadBody]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.593Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [RemoteAddr]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.593Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [XForwardedRemoteAddr]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.593Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Weight]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:54:59.593Z  INFO 91 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [CloudFoundryRouteService]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 184 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-05T03:55:07.352470193Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.1.116:8022 map[] map[] <nil> map[] 10.200.1.1:45880 /wait-for-drain <nil> <nil> <nil> 0xc0005553b0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-00003","knative.dev/pod":"where-for-dinner-00003-deployment-59779b4984-d56wh"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-00003-deployment-59779b4984-d56wh. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 22596, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx283562K -XX:MaxMetaspaceSize=141657K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 169 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload]   .   ____          _            __ _ _
where-for-diâ€¦ â”‚ [workload]  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
where-for-diâ€¦ â”‚ [workload] ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
where-for-diâ€¦ â”‚ [workload]  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
where-for-diâ€¦ â”‚ [workload]   '  |____| .__|_| |_|_| |_\__, | / / / /
where-for-diâ€¦ â”‚ [workload]  =========|_|==============|___/=/_/_/_/
where-for-diâ€¦ â”‚ [workload]  :: Spring Boot ::                (v3.1.3)
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:16.327Z  INFO 97 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : Starting DinnerAPIGatewayApplication v0.0.1-SNAPSHOT using Java 17.0.9 with PID 97 (/workspace/BOOT-INF/classes started by cnb in /workspace)
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:16.385Z  INFO 97 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : The following 1 profile is active: "kubernetes"
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:22.724Z  INFO 97 --- [           main] o.s.cloud.context.scope.GenericScope     : BeanFactory id=13afbb8a-a579-326a-b409-ed4b39bebfd3
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 183 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:24.895Z  INFO 97 --- [           main] ctiveUserDetailsServiceAutoConfiguration : 
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] Using generated security password: 9f4b5d83-0a30-4253-bb95-1d98ad39dfbc
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.524Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [After]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.524Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Before]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.524Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Between]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.524Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Cookie]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.525Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Header]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.525Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Host]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.525Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Method]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.525Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Path]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.525Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Query]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.525Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [ReadBody]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.525Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [RemoteAddr]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.525Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [XForwardedRemoteAddr]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.525Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [Weight]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:26.525Z  INFO 97 --- [           main] o.s.c.g.r.RouteDefinitionRouteLocator    : Loaded RoutePredicateFactory [CloudFoundryRouteService]
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:28.614Z  INFO 97 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8080
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:29.097Z  INFO 97 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 15 endpoint(s) beneath base path '/actuator'
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:29.605Z  INFO 97 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8081
where-for-diâ€¦ â”‚ [workload] 2024-01-05T03:55:30.137Z  INFO 97 --- [           main] c.j.e.t.w.DinnerAPIGatewayApplication    : Started DinnerAPIGatewayApplication in 17.507 seconds (process running for 20.593)
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 138 times): Get "http://127.0.0.1:8080/readyz": net/http: request canceled (Client.Timeout exceeded while awaiting headers)
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
