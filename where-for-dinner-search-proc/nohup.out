Tilt started on http://localhost:51313/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-search-proc/Tiltfile
Successfully loaded Tiltfile (704.289Âµs)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace default --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner-search-proc:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 4.77s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 4.77s 
where-for-diâ€¦ â”‚ 
Tilt started on http://localhost:51316/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-search-proc/Tiltfile
Successfully loaded Tiltfile (875.705Âµs)
ERROR: [] "msg"="Reconciler error" "error"="Failed to update API server: create uiresources/where-for-dinner-search-proc: uiresources.tilt.dev \"where-for-dinner-search-proc\" already exists" "controller"="tiltfile" "controllerGroup"="tilt.dev" "controllerKind"="Tiltfile" "Tiltfile"={"name":"(Tiltfile)"} "namespace"="" "name"="(Tiltfile)" "reconcileID"="48675fe4-8c98-458f-9f63-a295017cdde5"
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner-search-proc:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 5.76s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 5.76s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ [event: imagerepository workloads/where-for-dinner-search-proc] Failed to update status: Operation cannot be fulfilled on imagerepositories.source.apps.tanzu.vmware.com "where-for-dinner-search-proc": the object has been modified; please apply your changes to the latest version and try again
where-for-diâ€¦ â”‚ [event: imagerepository workloads/where-for-dinner-search-proc-delivery] Failed to update status: Operation cannot be fulfilled on imagerepositories.source.apps.tanzu.vmware.com "where-for-dinner-search-proc-delivery": the object has been modified; please apply your changes to the latest version and try again
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-proc-kpzpq-test-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-kpzpq-test-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-kpzpq-test-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/03 06:17:39 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-kpzpq-test-pod] Container image "projects.registry.vmware.com/tanzu_adv_eng/gradle@sha256:fdb628563dde15aebb58ea90d179247e8c24fd25df5f267db33a7250fbe9c8b8" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/03 06:17:40 Decoded script /tekton/scripts/script-0-m4p2h
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 3s
where-for-diâ€¦ â”‚      â”Š Completed       - 3s
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-proc-build-1-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32menv:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_OCI_SOURCE[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_LIVE_RELOAD_ENABLED[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "true"[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_JVM_VERSION[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "17"[0m
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31msource: {}[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32msource:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  blob:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search-proc/48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search-proc/48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search-proc/48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/ipillai/tkgi/buildservice@sha256:b69dec1f1d4f87a006d8861c931944b6c4adf1a3d887e3320c31fa9e55959dbc" already present on machine
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer started at 2024-01-03T06:19:15Z
where-for-diâ€¦ â”‚ [analyze] Restoring data for SBOM from previous image
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer ran for 6.493435371s and ended at 2024-01-03T06:19:21Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/ipillai/tkgi/buildservice@sha256:b69dec1f1d4f87a006d8861c931944b6c4adf1a3d887e3320c31fa9e55959dbc" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/dotnet-core-lite@2.9.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/go-lite@2.2.3' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/nodejs-lite@2.4.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] Timer: Detector started at 2024-01-03T06:19:24Z
where-for-diâ€¦ â”‚ [detect] 11 of 26 buildpacks participating
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/ca-certificates   3.6.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/bellsoft-liberica 9.13.0
where-for-diâ€¦ â”‚ [detect] paketo-buildpacks/syft             1.10.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/maven             6.15.12
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/watchexec         2.8.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/executable-jar    6.8.3
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/apache-tomcat     7.14.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/liberty           3.8.11
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/dist-zip          5.6.8
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/spring-boot       5.27.6
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/image-labels      4.5.6
where-for-diâ€¦ â”‚ [detect] Timer: Detector ran for 1.797681273s and ended at 2024-01-03T06:19:26Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/ipillai/tkgi/buildservice@sha256:b69dec1f1d4f87a006d8861c931944b6c4adf1a3d887e3320c31fa9e55959dbc" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/ipillai/tkgi/buildservice@sha256:b69dec1f1d4f87a006d8861c931944b6c4adf1a3d887e3320c31fa9e55959dbc" already present on machine
where-for-diâ€¦ â”‚ [restore] [33;1mWarning: [0mNo cached data will be used, no cache specified.
where-for-diâ€¦ â”‚ [restore] Timer: Restorer started at 2024-01-03T06:19:27Z
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/ca-certificates:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:jre" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:java-security-properties" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:spring-cloud-bindings" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:web-application-type" from app image
where-for-diâ€¦ â”‚ [restore] Timer: Restorer ran for 3.311723ms and ended at 2024-01-03T06:19:27Z
where-for-diâ€¦ â”‚ [build] Timer: Builder started at 2024-01-03T06:19:28Z
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for CA Certificates[0m[34m 3.6.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-ca-certificates[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for BellSoft Liberica[0m[34m 9.13.0[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-bellsoft-liberica[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ARGS           --no-man-pages --no-header-files --strip-debug --compress=1  configure custom link arguments (--output must be omitted)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ENABLED        false                                                        enables running jlink tool to generate custom JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_TYPE                 JRE                                                          the JVM type - JDK or JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_VERSION              17                                                           the Java version[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_ENABLED           false                                                        enables Java remote debugging support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_PORT              8000                                                         configure the remote debugging port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_SUSPEND           false                                                        configure whether to suspend execution until a debugger has attached[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_HEAP_DUMP_PATH                                                                       write heap dumps on error to this path[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_ENABLED        true                                                         enables Java Native Memory Tracking (NMT)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_LEVEL          summary                                                      configure level of NMT, summary or detail[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ARGS                                                                             configure custom Java Flight Recording (JFR) arguments[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ENABLED             false                                                        enables Java Flight Recording (JFR)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_ENABLED             false                                                        enables Java Management Extensions (JMX)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_PORT                5000                                                         configure the JMX port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_HEAD_ROOM           0                                                            the headroom in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_LOADED_CLASS_COUNT  35% of classes                                               the number of loaded classes in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_THREAD_COUNT        250                                                          the number of threads in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $JAVA_TOOL_OPTIONS                                                                        the JVM launch flags[0m
where-for-diâ€¦ â”‚ [build] [2m    [2mUsing Java version 17 from BP_JVM_VERSION[0m[2m[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JDK 17.0.9[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/bell-sw/Liberica/releases/download/17.0.9+11/bellsoft-jdk17.0.9+11-linux-amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Expanding to /layers/tanzu-buildpacks_bellsoft-liberica/jdk[0m
where-for-diâ€¦ â”‚ [build] [2m    Adding 137 container CA certificates to JVM truststore[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JAVA_HOME.override[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JDK_HOME.override[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JRE 17.0.9[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mJava Security Properties[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mPaketo Syft Buildpack[0m[34m 1.10.1[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/paketo-buildpacks/syft[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/anchore/syft/releases/download/v0.41.1/syft_0.41.1_linux_amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/SYFT_CHECK_FOR_APP_UPDATE.default[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Maven[0m[34m 6.15.12[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-maven[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_EXCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are removed[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_INCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are included[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JAVA_INSTALL_NODE                 false                                                  whether to install Yarn/Node binaries based on the presence of a package.json or yarn.lock file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ACTIVE_PROFILES                                                                    the active profiles (comma separated: such as: p1,!p2,?p3) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ADDITIONAL_BUILD_ARGUMENTS                                                         the additionnal arguments (appended to BP_MAVEN_BUILD_ARGUMENTS) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILD_ARGUMENTS             -Dmaven.test.skip=true --no-transfer-progress package  the arguments to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_ARTIFACT              target/*.[ejw]ar                                       the built application artifact explicitly.  Supersedes $BP_MAVEN_BUILT_MODULE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_MODULE                                                                       the module to find application artifact in[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_DAEMON_ENABLED              false                                                  use maven daemon[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_POM_FILE                    pom.xml                                                the location of the main pom.xml file, relative to the application root[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_SETTINGS_PATH                                                                      the path to a Maven settings file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_VERSION                     3                                                      the Maven version[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_NODE_PROJECT_PATH                                                                        configure a project subdirectory to look for `package.json` and `yarn.lock` files[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating cache directory /home/cnb/.m2[0m
where-for-diâ€¦ â”‚ [build]   [34mCompiled Application[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Executing mvnw --batch-mode -Dmaven.test.skip=true --no-transfer-progress package[0m
where-for-diâ€¦ â”‚ [build]         % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
where-for-diâ€¦ â”‚ [build]                                        Dload  Upload   Total   Spent    Left  Speed
where-for-diâ€¦ â”‚ [build]         0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 59925  100 59925    0     0   179k      0 --:--:-- --:--:-- --:--:--  179k
where-for-diâ€¦ â”‚ [build]       [0m[0m[INFO] Scanning for projects...
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --------------< com.example:where-for-dinner-search-proc >--------------
where-for-diâ€¦ â”‚ [build]       [INFO] Building where-for-dinner-search-proc 0.0.1-SNAPSHOT
where-for-diâ€¦ â”‚ [build]       [INFO] --------------------------------[ jar ]---------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 1 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 0 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Changes detected - recompiling the module! :dependency
where-for-diâ€¦ â”‚ [build]       [INFO] Compiling 15 source files with javac [debug release 17] to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:testResources (default-testResources) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not copying test resources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:testCompile (default-testCompile) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not compiling test sources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-surefire-plugin:3.0.0:test (default-test) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Tests are skipped.
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-jar-plugin:3.3.0:jar (default-jar) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Building jar: /workspace/target/where-for-dinner-search-proc-0.0.1-SNAPSHOT.jar
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- spring-boot-maven-plugin:3.1.3:repackage (repackage) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Replacing main artifact /workspace/target/where-for-dinner-search-proc-0.0.1-SNAPSHOT.jar with repackaged archive, adding nested dependencies in BOOT-INF/.
where-for-diâ€¦ â”‚ [build]       [INFO] The original artifact has been renamed to /workspace/target/where-for-dinner-search-proc-0.0.1-SNAPSHOT.jar.original
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] BUILD SUCCESS
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] Total time:  59.221 s
where-for-diâ€¦ â”‚ [build]       [INFO] Finished at: 2024-01-03T06:20:41Z
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [0m[0m
where-for-diâ€¦ â”‚ [build]   Removing source code
where-for-diâ€¦ â”‚ [build]   Restoring application artifact
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Watchexec[0m[34m 2.8.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-watchexec[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/watchexec/watchexec/releases/download/cli-v1.20.5/watchexec-1.20.5-x86_64-unknown-linux-musl.tar.xz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Executable JAR[0m[34m 6.8.3[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-executable-jar[0m
where-for-diâ€¦ â”‚ [build]   [34mClass Path[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.delim[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.prepend[0m
where-for-diâ€¦ â”‚ [build]   Process types:
where-for-diâ€¦ â”‚ [build]     [36mexecutable-jar[0m: java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mreload[0m:         watchexec -r --shell=none -- java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mtask[0m:           java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mweb[0m:            java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build] SKIPPED: `Main-Class` found in `META-INF/MANIFEST.MF`, skipping build
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Spring Boot[0m[34m 5.27.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-spring-boot[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_DISABLED   false  whether to contribute Spring Boot cloud bindings support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_VERSION    1      default version of Spring Cloud Bindings library to contribute[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_DISABLED  false  whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_ENABLED   true   Deprecated - whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build]   Creating slices from layers index
where-for-diâ€¦ â”‚ [build] [2m    dependencies (55.6 MB)[0m
where-for-diâ€¦ â”‚ [build] [2m    spring-boot-loader (269.4 KB)[0m
where-for-diâ€¦ â”‚ [build] [2m    snapshot-dependencies (0.0 B)[0m
where-for-diâ€¦ â”‚ [build] [2m    application (132.7 KB)[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mSpring Cloud Bindings 2.0.2[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-bindings/2.0.2/spring-cloud-bindings-2.0.2.jar[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Copying to /layers/tanzu-buildpacks_spring-boot/spring-cloud-bindings[0m
where-for-diâ€¦ â”‚ [build]   [34mWeb Application Type[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Reactive web application detected[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/BPL_JVM_THREAD_COUNT.default[0m
where-for-diâ€¦ â”‚ [build]   4 application slices
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.title
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.version
where-for-diâ€¦ â”‚ [build]     org.springframework.boot.version
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Image Labels[0m[34m 4.5.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-image-labels[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_IMAGE_LABELS                                                                                                                                                                arbitrary image labels[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_AUTHORS                                                                                                                                                                 the org.opencontainers.image.authors image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_CREATED                                                                                                                                                                 the org.opencontainers.image.created image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DESCRIPTION                                                                                                                                                             the org.opencontainers.image.description image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DOCUMENTATION                                                                                                                                                           the org.opencontainers.image.documentation image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_LICENSES                                                                                                                                                                the org.opencontainers.image.licenses image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REF_NAME                                                                                                                                                                the org.opencontainers.image.ref.name image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REVISION                                                                                                                                                                the org.opencontainers.image.revision image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_SOURCE         tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6  the org.opencontainers.image.source image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_TITLE                                                                                                                                                                   the org.opencontainers.image.title image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_URL                                                                                                                                                                     the org.opencontainers.image.url image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VENDOR                                                                                                                                                                  the org.opencontainers.image.vendor image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VERSION                                                                                                                                                                 the org.opencontainers.image.version image label[0m
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.source
where-for-diâ€¦ â”‚ [build] Timer: Builder ran for 1m20.025079666s and ended at 2024-01-03T06:20:48Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/ipillai/tkgi/buildservice@sha256:b69dec1f1d4f87a006d8861c931944b6c4adf1a3d887e3320c31fa9e55959dbc" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [export] [33;1mWarning: [0mNo cached data will be used, no cache specified.
where-for-diâ€¦ â”‚ [export] Reusing layers from image 'tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-proc-workloads@sha256:a4cd65562d82656bdb1211372ae2331ab1ff675f62ff3b8de86aaa0754152b96'
where-for-diâ€¦ â”‚ [export] Timer: Exporter started at 2024-01-03T06:22:27Z
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/ca-certificates:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:java-security-properties'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:jre'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/watchexec:watchexec'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/executable-jar:classpath'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:spring-cloud-bindings'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:web-application-type'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:launch.sbom'
where-for-diâ€¦ â”‚ [export] Reusing 2/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Adding 3/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:launcher'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:config'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:process-types'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.lifecycle.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.build.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.project.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.title'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.springframework.boot.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.source'
where-for-diâ€¦ â”‚ [export] Setting default process type 'reload'
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-proc-workloads... started at 2024-01-03T06:22:29Z
where-for-diâ€¦ â”‚ [export] *** Images (sha256:64f9363ae0b4307b2a9de8fb5546f1be7d4c4fa825688ba984dc8e80d2547108):
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-proc-workloads
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-proc-workloads:b1.20240103.061801
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-proc-workloads... ran for 4.349924141s and ended at 2024-01-03T06:22:33Z
where-for-diâ€¦ â”‚ [export] Timer: Exporter ran for 6.630672576s and ended at 2024-01-03T06:22:33Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:0b5c1bfe87a700cf473725e7cd133f4c7da48ec85baccf2d0c5d6127beb675e7" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 4m33s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Build successful
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): error upgrading connection: unable to upgrade connection: pod not found ("where-for-dinner-search-proc-build-1-build-pod_workloads")
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): error upgrading connection: unable to upgrade connection: pod not found ("where-for-dinner-search-proc-build-1-build-pod_workloads")
where-for-diâ€¦ â”‚ [event: taskrun workloads/where-for-dinner-search-proc-config-writer-h8w4p] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-proc-config-writer-h8w4p-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-config-writer-h8w4p-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-config-writer-h8w4p-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/03 06:22:57 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-config-writer-h8w4p-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/03 06:22:57 Decoded script /tekton/scripts/script-0-4v725
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.7TmnMCoe7M
where-for-diâ€¦ â”‚ + base64 --decode
where-for-diâ€¦ â”‚ + echo -e {"delivery.yml":"apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: where-for-dinner-search-proc\n  annotations:\n    ootb.apps.tanzu.vmware.com/servicebinding-workload: \"true\"\n    ootb.apps.tanzu.vmware.com/apidescriptor-ref: \"true\"\n    kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings\n  labels:\n    app.kubernetes.io/part-of: where-for-dinner-search-proc\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search-proc\nspec:\n  template:\n    metadata:\n      annotations:\n        apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc?sub_path=/\n        apps.tanzu.vmware.com/debug: \"true\"\n        apps.tanzu.vmware.com/live-update: \"true\"\n        autoscaling.knative.dev/maxScale: \"1\"\n        autoscaling.knative.dev/minScale: \"1\"\n        boot.spring.io/actuator: http://:8081/actuator\n        boot.spring.io/version: 3.1.3\n        conventions.carto.run/applied-conventions: |-\n          appliveview-sample/app-live-view-appflavour-check\n          developer-conventions/debug-convention\n          developer-conventions/live-update-convention\n          developer-conventions/add-source-image-label\n          spring-boot-convention/auto-configure-actuators-check\n          spring-boot-convention/is-native-app-check\n          spring-boot-convention/spring-boot\n          spring-boot-convention/spring-boot-web\n          spring-boot-convention/spring-boot-actuator\n          spring-boot-convention/spring-boot-actuator-probes\n          spring-boot-convention/app-live-view-appflavour-check\n          spring-boot-convention/app-live-view-connector-boot\n          spring-boot-convention/app-live-view-appflavours-boot\n          spring-boot-convention/service-intent-rabbitmq\n          spring-boot-convention/service-intent-kafka\n        developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6\n        developer.conventions/target-containers: workload\n        local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6\n        services.conventions.carto.run/kafka: kafka-clients/3.4.1\n        services.conventions.carto.run/rabbitmq: amqp-client/5.17.1\n      labels:\n        app.kubernetes.io/component: run\n        app.kubernetes.io/part-of: where-for-dinner-search-proc\n        apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n        apps.tanzu.vmware.com/has-tests: \"true\"\n        apps.tanzu.vmware.com/workload-type: web\n        carto.run/workload-name: where-for-dinner-search-proc\n        conventions.carto.run/framework: spring-boot\n        networking.knative.dev/visibility: cluster-local\n        services.conventions.carto.run/kafka: workload\n        services.conventions.carto.run/rabbitmq: workload\n        tanzu.app.live.view: \"true\"\n        tanzu.app.live.view.application.actuator.path: actuator\n        tanzu.app.live.view.application.actuator.port: \"8081\"\n        tanzu.app.live.view.application.flavours: spring-boot\n        tanzu.app.live.view.application.name: where-for-dinner-search-proc\n    spec:\n      containers:\n      - env:\n        - name: BPL_DEBUG_ENABLED\n          value: \"true\"\n        - name: BPL_DEBUG_PORT\n          value: \"9005\"\n        - name: JAVA_TOOL_OPTIONS\n          value: -Dmanagement.endpoint.health.probes.add-additional-paths=\"true\" -Dmanagement.endpoint.health.show-details=\"always\" -Dmanagement.endpoints.web.base-path=\"/actuator\" -Dmanagement.endpoints.web.exposure.include=\"*\" -Dmanagement.health.probes.enabled=\"true\" -Dmanagement.server.port=\"8081\" -Dserver.port=\"8080\"\n        image: tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-proc-workloads@sha256:64f9363ae0b4307b2a9de8fb5546f1be7d4c4fa825688ba984dc8e80d2547108\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: 8080\n            scheme: HTTP\n        name: workload\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 1500m\n            memory: 750M\n          requests:\n            cpu: 100m\n            memory: 500M\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        startupProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n            scheme: HTTP\n      serviceAccountName: default\n","kapp-config.yml":"apiVersion: kapp.k14s.io/v1alpha1\nkind: Config\nrebaseRules:\n- path:\n  - metadata\n  - annotations\n  - serving.knative.dev/creator\n  type: copy\n  sources:\n  - new\n  - existing\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n- path:\n  - metadata\n  - annotations\n  - serving.knative.dev/lastModifier\n  type: copy\n  sources:\n  - new\n  - existing\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\nwaitRules:\n- resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n  conditionMatchers:\n  - type: Ready\n    status: \"True\"\n    success: true\n  - type: Ready\n    status: \"False\"\n    failure: true\nownershipLabelRules:\n- path:\n  - spec\n  - template\n  - metadata\n  - labels\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n","serviceclaims.yml":"apiVersion: kapp.k14s.io/v1alpha1\nkind: Config\nwaitRules:\n- conditionMatchers:\n  - type: ServiceAvailable\n    status: \"False\"\n    failure: true\n  - type: ServiceAvailable\n    status: \"True\"\n    success: true\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: servicebinding.io/v1alpha3\n      kind: ServiceBinding\n---\napiVersion: servicebinding.io/v1alpha3\nkind: ServiceBinding\nmetadata:\n  name: where-for-dinner-search-proc-rmq\n  annotations:\n    local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6\n    autoscaling.knative.dev/minScale: \"1\"\n    kapp.k14s.io/change-group: servicebinding.io/ServiceBindings\n  labels:\n    app.kubernetes.io/part-of: where-for-dinner-search-proc\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search-proc\nspec:\n  name: rmq\n  service:\n    apiVersion: services.apps.tanzu.vmware.com/v1alpha1\n    kind: ClassClaim\n    name: msgbroker-where-for-dinner\n  workload:\n    apiVersion: serving.knative.dev/v1\n    kind: Service\n    name: where-for-dinner-search-proc\n"}
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-rabbitmq
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-kafka
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: kafka-clients/3.4.1
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: amqp-client/5.17.1
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: workload
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-proc-workloads@sha256:64f9363ae0b4307b2a9de8fb5546f1be7d4c4fa825688ba984dc8e80d2547108
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''serviceclaims.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - conditionMatchers:
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚       kind: ServiceBinding
where-for-diâ€¦ â”‚ ---
where-for-diâ€¦ â”‚ apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚ kind: ServiceBinding
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-proc-rmq
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-group: servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   name: rmq
where-for-diâ€¦ â”‚   service:
where-for-diâ€¦ â”‚     apiVersion: services.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚     kind: ClassClaim
where-for-diâ€¦ â”‚     name: msgbroker-where-for-dinner
where-for-diâ€¦ â”‚   workload:
where-for-diâ€¦ â”‚     apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚     kind: Service
where-for-diâ€¦ â”‚     name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ '\'' > '\''serviceclaims.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-rabbitmq
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-kafka
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: kafka-clients/3.4.1
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: amqp-client/5.17.1
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: workload
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-proc-workloads@sha256:64f9363ae0b4307b2a9de8fb5546f1be7d4c4fa825688ba984dc8e80d2547108
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname serviceclaims.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - conditionMatchers:
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚       kind: ServiceBinding
where-for-diâ€¦ â”‚ ---
where-for-diâ€¦ â”‚ apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚ kind: ServiceBinding
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-proc-rmq
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:48099f60362fd9bdea079b270354c9a64317f97a7c8d9333079727e0b88128d6
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-group: servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   name: rmq
where-for-diâ€¦ â”‚   service:
where-for-diâ€¦ â”‚     apiVersion: services.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚     kind: ClassClaim
where-for-diâ€¦ â”‚     name: msgbroker-where-for-dinner
where-for-diâ€¦ â”‚   workload:
where-for-diâ€¦ â”‚     apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚     kind: Service
where-for-diâ€¦ â”‚     name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/ipillai/tkgifull/workload/where-for-dinner-search-proc-workloads-bundle:5343916b-98b1-4bfc-9e59-468554fcc9b9 -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 3s
where-for-diâ€¦ â”‚      â”Š Completed       - 4s
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): error upgrading connection: unable to upgrade connection: pod not found ("where-for-dinner-search-proc-config-writer-h8w4p-pod_workloads")
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): error upgrading connection: unable to upgrade connection: pod not found ("where-for-dinner-search-proc-config-writer-h8w4p-pod_workloads")
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:38:55.402208001Z","logger":"queueproxy","caller":"sharedmain/main.go:260","message":"Starting queue-proxy","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:38:55.402656965Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server metrics:9090","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:38:55.415963077Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server main:8012","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:38:55.416033666Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server admin:8022","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:39:25.294968074Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.3.79:8022 map[] map[] <nil> map[] 10.200.3.1:43134 /wait-for-drain <nil> <nil> <nil> 0xc000025720}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 182 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:39:55.289420944Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.3.79:8022 map[] map[] <nil> map[] 10.200.3.1:43134 /wait-for-drain <nil> <nil> <nil> 0xc0007130e0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:40:25.293950581Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.3.79:8022 map[] map[] <nil> map[] 10.200.3.1:43134 /wait-for-drain <nil> <nil> <nil> 0xc00024ad20}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:40:48.419599927Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.3.79:8022 map[] map[] <nil> map[] 10.200.3.1:43134 /wait-for-drain <nil> <nil> <nil> 0xc00060c5f0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:40:48.453806825Z","logger":"queueproxy","caller":"sharedmain/main.go:290","message":"Received TERM signal, attempting to gracefully shutdown servers.","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:40:48.453921921Z","logger":"queueproxy","caller":"sharedmain/main.go:291","message":"Sleeping 30s to allow K8s propagation of non-ready state","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:41:18.434171122Z","logger":"queueproxy","caller":"sharedmain/main.go:295","message":"Shutting down server: main","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:41:18.439484386Z","logger":"queueproxy","caller":"sharedmain/main.go:295","message":"Shutting down server: admin","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:41:18.440092559Z","logger":"queueproxy","caller":"sharedmain/main.go:295","message":"Shutting down server: metrics","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T06:41:18.440358832Z","logger":"queueproxy","caller":"sharedmain/main.go:306","message":"Shutdown complete, exiting...","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-7f4f64c49f-wvqkp"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [workload queue-proxy]
Tilt started on http://localhost:51315/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner 3/where-for-dinner-search-proc/Tiltfile
Successfully loaded Tiltfile (1.61799ms)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner-search-proc:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 4.82s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 4.82s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-proc-build-1-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32menv:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_OCI_SOURCE[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_LIVE_RELOAD_ENABLED[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "true"[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_JVM_VERSION[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "17"[0m
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31msource: {}[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32msource:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  blob:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search-proc/e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search-proc/e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search-proc/e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Pulling image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Successfully pulled image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" in 10.720808577s (10.720885748s including waiting)
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer started at 2024-01-03T09:34:09Z
where-for-diâ€¦ â”‚ [analyze] Restoring data for SBOM from previous image
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer ran for 6.839876856s and ended at 2024-01-03T09:34:16Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/dotnet-core-lite@2.9.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/go-lite@2.2.3' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/nodejs-lite@2.4.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] Timer: Detector started at 2024-01-03T09:34:17Z
where-for-diâ€¦ â”‚ [detect] 11 of 26 buildpacks participating
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/ca-certificates   3.6.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/bellsoft-liberica 9.13.0
where-for-diâ€¦ â”‚ [detect] paketo-buildpacks/syft             1.10.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/maven             6.15.12
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/watchexec         2.8.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/executable-jar    6.8.3
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/apache-tomcat     7.14.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/liberty           3.8.11
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/dist-zip          5.6.8
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/spring-boot       5.27.6
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/image-labels      4.5.6
where-for-diâ€¦ â”‚ [detect] Timer: Detector ran for 1.756195418s and ended at 2024-01-03T09:34:19Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [restore] [33;1mWarning: [0mNo cached data will be used, no cache specified.
where-for-diâ€¦ â”‚ [restore] Timer: Restorer started at 2024-01-03T09:34:20Z
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/ca-certificates:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:java-security-properties" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:jre" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:spring-cloud-bindings" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:web-application-type" from app image
where-for-diâ€¦ â”‚ [restore] Timer: Restorer ran for 5.028677ms and ended at 2024-01-03T09:34:20Z
where-for-diâ€¦ â”‚ [build] Timer: Builder started at 2024-01-03T09:34:21Z
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for CA Certificates[0m[34m 3.6.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-ca-certificates[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for BellSoft Liberica[0m[34m 9.13.0[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-bellsoft-liberica[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ARGS           --no-man-pages --no-header-files --strip-debug --compress=1  configure custom link arguments (--output must be omitted)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ENABLED        false                                                        enables running jlink tool to generate custom JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_TYPE                 JRE                                                          the JVM type - JDK or JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_VERSION              17                                                           the Java version[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_ENABLED           false                                                        enables Java remote debugging support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_PORT              8000                                                         configure the remote debugging port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_SUSPEND           false                                                        configure whether to suspend execution until a debugger has attached[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_HEAP_DUMP_PATH                                                                       write heap dumps on error to this path[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_ENABLED        true                                                         enables Java Native Memory Tracking (NMT)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_LEVEL          summary                                                      configure level of NMT, summary or detail[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ARGS                                                                             configure custom Java Flight Recording (JFR) arguments[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ENABLED             false                                                        enables Java Flight Recording (JFR)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_ENABLED             false                                                        enables Java Management Extensions (JMX)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_PORT                5000                                                         configure the JMX port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_HEAD_ROOM           0                                                            the headroom in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_LOADED_CLASS_COUNT  35% of classes                                               the number of loaded classes in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_THREAD_COUNT        250                                                          the number of threads in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $JAVA_TOOL_OPTIONS                                                                        the JVM launch flags[0m
where-for-diâ€¦ â”‚ [build] [2m    [2mUsing Java version 17 from BP_JVM_VERSION[0m[2m[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JDK 17.0.9[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/bell-sw/Liberica/releases/download/17.0.9+11/bellsoft-jdk17.0.9+11-linux-amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Expanding to /layers/tanzu-buildpacks_bellsoft-liberica/jdk[0m
where-for-diâ€¦ â”‚ [build] [2m    Adding 137 container CA certificates to JVM truststore[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JAVA_HOME.override[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JDK_HOME.override[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JRE 17.0.9[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mJava Security Properties[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mPaketo Syft Buildpack[0m[34m 1.10.1[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/paketo-buildpacks/syft[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/anchore/syft/releases/download/v0.41.1/syft_0.41.1_linux_amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/SYFT_CHECK_FOR_APP_UPDATE.default[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Maven[0m[34m 6.15.12[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-maven[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_EXCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are removed[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_INCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are included[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JAVA_INSTALL_NODE                 false                                                  whether to install Yarn/Node binaries based on the presence of a package.json or yarn.lock file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ACTIVE_PROFILES                                                                    the active profiles (comma separated: such as: p1,!p2,?p3) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ADDITIONAL_BUILD_ARGUMENTS                                                         the additionnal arguments (appended to BP_MAVEN_BUILD_ARGUMENTS) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILD_ARGUMENTS             -Dmaven.test.skip=true --no-transfer-progress package  the arguments to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_ARTIFACT              target/*.[ejw]ar                                       the built application artifact explicitly.  Supersedes $BP_MAVEN_BUILT_MODULE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_MODULE                                                                       the module to find application artifact in[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_DAEMON_ENABLED              false                                                  use maven daemon[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_POM_FILE                    pom.xml                                                the location of the main pom.xml file, relative to the application root[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_SETTINGS_PATH                                                                      the path to a Maven settings file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_VERSION                     3                                                      the Maven version[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_NODE_PROJECT_PATH                                                                        configure a project subdirectory to look for `package.json` and `yarn.lock` files[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating cache directory /home/cnb/.m2[0m
where-for-diâ€¦ â”‚ [build]   [34mCompiled Application[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Executing mvnw --batch-mode -Dmaven.test.skip=true --no-transfer-progress package[0m
where-for-diâ€¦ â”‚ [build]         % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
where-for-diâ€¦ â”‚ [build]                                        Dload  Upload   Total   Spent    Left  Speed
where-for-diâ€¦ â”‚ [build]         0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 59925  100 59925    0     0   220k      0 --:--:-- --:--:-- --:--:--  221k
where-for-diâ€¦ â”‚ [build]       [0m[0m[INFO] Scanning for projects...
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --------------< com.example:where-for-dinner-search-proc >--------------
where-for-diâ€¦ â”‚ [build]       [INFO] Building where-for-dinner-search-proc 0.0.1-SNAPSHOT
where-for-diâ€¦ â”‚ [build]       [INFO] --------------------------------[ jar ]---------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 1 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 0 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Changes detected - recompiling the module! :dependency
where-for-diâ€¦ â”‚ [build]       [INFO] Compiling 15 source files with javac [debug release 17] to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:testResources (default-testResources) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not copying test resources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:testCompile (default-testCompile) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not compiling test sources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-surefire-plugin:3.0.0:test (default-test) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Tests are skipped.
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-jar-plugin:3.3.0:jar (default-jar) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Building jar: /workspace/target/where-for-dinner-search-proc-0.0.1-SNAPSHOT.jar
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- spring-boot-maven-plugin:3.1.3:repackage (repackage) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Replacing main artifact /workspace/target/where-for-dinner-search-proc-0.0.1-SNAPSHOT.jar with repackaged archive, adding nested dependencies in BOOT-INF/.
where-for-diâ€¦ â”‚ [build]       [INFO] The original artifact has been renamed to /workspace/target/where-for-dinner-search-proc-0.0.1-SNAPSHOT.jar.original
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] BUILD SUCCESS
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] Total time:  01:03 min
where-for-diâ€¦ â”‚ [build]       [INFO] Finished at: 2024-01-03T09:35:48Z
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [0m[0m
where-for-diâ€¦ â”‚ [build]   Removing source code
where-for-diâ€¦ â”‚ [build]   Restoring application artifact
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Watchexec[0m[34m 2.8.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-watchexec[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/watchexec/watchexec/releases/download/cli-v1.20.5/watchexec-1.20.5-x86_64-unknown-linux-musl.tar.xz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Executable JAR[0m[34m 6.8.3[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-executable-jar[0m
where-for-diâ€¦ â”‚ [build]   [34mClass Path[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.delim[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.prepend[0m
where-for-diâ€¦ â”‚ [build]   Process types:
where-for-diâ€¦ â”‚ [build]     [36mexecutable-jar[0m: java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mreload[0m:         watchexec -r --shell=none -- java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mtask[0m:           java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mweb[0m:            java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build] SKIPPED: `Main-Class` found in `META-INF/MANIFEST.MF`, skipping build
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Spring Boot[0m[34m 5.27.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-spring-boot[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_DISABLED   false  whether to contribute Spring Boot cloud bindings support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_VERSION    1      default version of Spring Cloud Bindings library to contribute[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_DISABLED  false  whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_ENABLED   true   Deprecated - whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build]   Creating slices from layers index
where-for-diâ€¦ â”‚ [build] [2m    dependencies (55.6 MB)[0m
where-for-diâ€¦ â”‚ [build] [2m    spring-boot-loader (269.4 KB)[0m
where-for-diâ€¦ â”‚ [build] [2m    snapshot-dependencies (0.0 B)[0m
where-for-diâ€¦ â”‚ [build] [2m    application (132.7 KB)[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mSpring Cloud Bindings 2.0.2[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-bindings/2.0.2/spring-cloud-bindings-2.0.2.jar[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Copying to /layers/tanzu-buildpacks_spring-boot/spring-cloud-bindings[0m
where-for-diâ€¦ â”‚ [build]   [34mWeb Application Type[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Reactive web application detected[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/BPL_JVM_THREAD_COUNT.default[0m
where-for-diâ€¦ â”‚ [build]   4 application slices
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.title
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.version
where-for-diâ€¦ â”‚ [build]     org.springframework.boot.version
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Image Labels[0m[34m 4.5.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-image-labels[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_IMAGE_LABELS                                                                                                                                                                arbitrary image labels[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_AUTHORS                                                                                                                                                                 the org.opencontainers.image.authors image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_CREATED                                                                                                                                                                 the org.opencontainers.image.created image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DESCRIPTION                                                                                                                                                             the org.opencontainers.image.description image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DOCUMENTATION                                                                                                                                                           the org.opencontainers.image.documentation image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_LICENSES                                                                                                                                                                the org.opencontainers.image.licenses image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REF_NAME                                                                                                                                                                the org.opencontainers.image.ref.name image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REVISION                                                                                                                                                                the org.opencontainers.image.revision image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_SOURCE         tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803  the org.opencontainers.image.source image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_TITLE                                                                                                                                                                   the org.opencontainers.image.title image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_URL                                                                                                                                                                     the org.opencontainers.image.url image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VENDOR                                                                                                                                                                  the org.opencontainers.image.vendor image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VERSION                                                                                                                                                                 the org.opencontainers.image.version image label[0m
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.source
where-for-diâ€¦ â”‚ [build] Timer: Builder ran for 1m30.783156842s and ended at 2024-01-03T09:35:52Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [export] [33;1mWarning: [0mNo cached data will be used, no cache specified.
where-for-diâ€¦ â”‚ [export] Reusing layers from image 'tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads@sha256:d4fe7638642eb1e14c9c9a7969d44cdcea231d7e7452b21af9f8d1a8d74844f4'
where-for-diâ€¦ â”‚ [export] Timer: Exporter started at 2024-01-03T09:37:28Z
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/ca-certificates:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:java-security-properties'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:jre'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/watchexec:watchexec'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/executable-jar:classpath'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:spring-cloud-bindings'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:web-application-type'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:launch.sbom'
where-for-diâ€¦ â”‚ [export] Reusing 2/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Adding 3/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:launcher'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:config'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:process-types'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.lifecycle.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.build.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.project.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.title'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.springframework.boot.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.source'
where-for-diâ€¦ â”‚ [export] Setting default process type 'reload'
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads... started at 2024-01-03T09:37:30Z
where-for-diâ€¦ â”‚ [export] *** Images (sha256:8c40428a1a08ca70bec1098b10a3498a7deae89f0038150bea85a481399eef93):
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads:b1.20240103.093246
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads... ran for 6.321146439s and ended at 2024-01-03T09:37:37Z
where-for-diâ€¦ â”‚ [export] Timer: Exporter ran for 8.256134583s and ended at 2024-01-03T09:37:37Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:0b5c1bfe87a700cf473725e7cd133f4c7da48ec85baccf2d0c5d6127beb675e7" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 4m51s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Build successful
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): error upgrading connection: unable to upgrade connection: pod not found ("where-for-dinner-search-proc-build-1-build-pod_workloads")
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): error upgrading connection: unable to upgrade connection: pod not found ("where-for-dinner-search-proc-build-1-build-pod_workloads")
where-for-diâ€¦ â”‚ [event: taskrun workloads/where-for-dinner-search-proc-config-writer-s4px5] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-proc-config-writer-s4px5-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-config-writer-s4px5-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/03 09:38:05 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-config-writer-s4px5-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-config-writer-s4px5-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/03 09:38:08 Decoded script /tekton/scripts/script-0-44r2s
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 4s
where-for-diâ€¦ â”‚      â”Š Ready           - 2s
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.1exyeTDewg
where-for-diâ€¦ â”‚ + base64 --decode
where-for-diâ€¦ â”‚ + echo -e {"delivery.yml":"apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: where-for-dinner-search-proc\n  annotations:\n    ootb.apps.tanzu.vmware.com/servicebinding-workload: \"true\"\n    ootb.apps.tanzu.vmware.com/apidescriptor-ref: \"true\"\n    kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings\n  labels:\n    app.kubernetes.io/part-of: where-for-dinner-search-proc\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search-proc\nspec:\n  template:\n    metadata:\n      annotations:\n        apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc?sub_path=/\n        apps.tanzu.vmware.com/debug: \"true\"\n        apps.tanzu.vmware.com/live-update: \"true\"\n        autoscaling.knative.dev/maxScale: \"1\"\n        autoscaling.knative.dev/minScale: \"1\"\n        boot.spring.io/actuator: http://:8081/actuator\n        boot.spring.io/version: 3.1.3\n        conventions.carto.run/applied-conventions: |-\n          appliveview-sample/app-live-view-appflavour-check\n          developer-conventions/debug-convention\n          developer-conventions/live-update-convention\n          developer-conventions/add-source-image-label\n          spring-boot-convention/auto-configure-actuators-check\n          spring-boot-convention/is-native-app-check\n          spring-boot-convention/spring-boot\n          spring-boot-convention/spring-boot-web\n          spring-boot-convention/spring-boot-actuator\n          spring-boot-convention/spring-boot-actuator-probes\n          spring-boot-convention/app-live-view-appflavour-check\n          spring-boot-convention/app-live-view-connector-boot\n          spring-boot-convention/app-live-view-appflavours-boot\n          spring-boot-convention/service-intent-rabbitmq\n          spring-boot-convention/service-intent-kafka\n        developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803\n        developer.conventions/target-containers: workload\n        local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803\n        services.conventions.carto.run/kafka: kafka-clients/3.4.1\n        services.conventions.carto.run/rabbitmq: amqp-client/5.17.1\n      labels:\n        app.kubernetes.io/component: run\n        app.kubernetes.io/part-of: where-for-dinner-search-proc\n        apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n        apps.tanzu.vmware.com/has-tests: \"true\"\n        apps.tanzu.vmware.com/workload-type: web\n        carto.run/workload-name: where-for-dinner-search-proc\n        conventions.carto.run/framework: spring-boot\n        networking.knative.dev/visibility: cluster-local\n        services.conventions.carto.run/kafka: workload\n        services.conventions.carto.run/rabbitmq: workload\n        tanzu.app.live.view: \"true\"\n        tanzu.app.live.view.application.actuator.path: actuator\n        tanzu.app.live.view.application.actuator.port: \"8081\"\n        tanzu.app.live.view.application.flavours: spring-boot\n        tanzu.app.live.view.application.name: where-for-dinner-search-proc\n    spec:\n      containers:\n      - env:\n        - name: BPL_DEBUG_ENABLED\n          value: \"true\"\n        - name: BPL_DEBUG_PORT\n          value: \"9005\"\n        - name: JAVA_TOOL_OPTIONS\n          value: -Dmanagement.endpoint.health.probes.add-additional-paths=\"true\" -Dmanagement.endpoint.health.show-details=\"always\" -Dmanagement.endpoints.web.base-path=\"/actuator\" -Dmanagement.endpoints.web.exposure.include=\"*\" -Dmanagement.health.probes.enabled=\"true\" -Dmanagement.server.port=\"8081\" -Dserver.port=\"8080\"\n        image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads@sha256:8c40428a1a08ca70bec1098b10a3498a7deae89f0038150bea85a481399eef93\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: 8080\n            scheme: HTTP\n        name: workload\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 1500m\n            memory: 750M\n          requests:\n            cpu: 100m\n            memory: 500M\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        startupProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n            scheme: HTTP\n      serviceAccountName: default\n","kapp-config.yml":"apiVersion: kapp.k14s.io/v1alpha1\nkind: Config\nrebaseRules:\n- path:\n  - metadata\n  - annotations\n  - serving.knative.dev/creator\n  type: copy\n  sources:\n  - new\n  - existing\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n- path:\n  - metadata\n  - annotations\n  - serving.knative.dev/lastModifier\n  type: copy\n  sources:\n  - new\n  - existing\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\nwaitRules:\n- resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n  conditionMatchers:\n  - type: Ready\n    status: \"True\"\n    success: true\n  - type: Ready\n    status: \"False\"\n    failure: true\nownershipLabelRules:\n- path:\n  - spec\n  - template\n  - metadata\n  - labels\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n","serviceclaims.yml":"apiVersion: kapp.k14s.io/v1alpha1\nkind: Config\nwaitRules:\n- conditionMatchers:\n  - type: ServiceAvailable\n    status: \"False\"\n    failure: true\n  - type: ServiceAvailable\n    status: \"True\"\n    success: true\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: servicebinding.io/v1alpha3\n      kind: ServiceBinding\n---\napiVersion: servicebinding.io/v1alpha3\nkind: ServiceBinding\nmetadata:\n  name: where-for-dinner-search-proc-rmq\n  annotations:\n    local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803\n    autoscaling.knative.dev/minScale: \"1\"\n    kapp.k14s.io/change-group: servicebinding.io/ServiceBindings\n  labels:\n    app.kubernetes.io/part-of: where-for-dinner-search-proc\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search-proc\nspec:\n  name: rmq\n  service:\n    apiVersion: services.apps.tanzu.vmware.com/v1alpha1\n    kind: ClassClaim\n    name: msgbroker-where-for-dinner\n  workload:\n    apiVersion: serving.knative.dev/v1\n    kind: Service\n    name: where-for-dinner-search-proc\n"}
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-rabbitmq
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-kafka
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: kafka-clients/3.4.1
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: amqp-client/5.17.1
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: workload
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads@sha256:8c40428a1a08ca70bec1098b10a3498a7deae89f0038150bea85a481399eef93
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''serviceclaims.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - conditionMatchers:
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚       kind: ServiceBinding
where-for-diâ€¦ â”‚ ---
where-for-diâ€¦ â”‚ apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚ kind: ServiceBinding
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-proc-rmq
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-group: servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   name: rmq
where-for-diâ€¦ â”‚   service:
where-for-diâ€¦ â”‚     apiVersion: services.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚     kind: ClassClaim
where-for-diâ€¦ â”‚     name: msgbroker-where-for-dinner
where-for-diâ€¦ â”‚   workload:
where-for-diâ€¦ â”‚     apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚     kind: Service
where-for-diâ€¦ â”‚     name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ '\'' > '\''serviceclaims.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-rabbitmq
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-kafka
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: kafka-clients/3.4.1
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: amqp-client/5.17.1
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: workload
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads@sha256:8c40428a1a08ca70bec1098b10a3498a7deae89f0038150bea85a481399eef93
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname serviceclaims.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - conditionMatchers:
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚       kind: ServiceBinding
where-for-diâ€¦ â”‚ ---
where-for-diâ€¦ â”‚ apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚ kind: ServiceBinding
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-proc-rmq
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:e4b36b0fe94b573d37f181dfacecbd737cc00adccb5f832a99a00900905cb803
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-group: servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   name: rmq
where-for-diâ€¦ â”‚   service:
where-for-diâ€¦ â”‚     apiVersion: services.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚     kind: ClassClaim
where-for-diâ€¦ â”‚     name: msgbroker-where-for-dinner
where-for-diâ€¦ â”‚   workload:
where-for-diâ€¦ â”‚     apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚     kind: Service
where-for-diâ€¦ â”‚     name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads-bundle:6c24e738-32ac-4753-bc6d-838e29a4eed3 -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): error upgrading connection: unable to upgrade connection: pod not found ("where-for-dinner-search-proc-config-writer-s4px5-pod_workloads")
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:39:09.11750716Z","logger":"queueproxy","caller":"sharedmain/main.go:260","message":"Starting queue-proxy","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:39:09.118896903Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server main:8012","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:39:09.126455326Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server admin:8022","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:39:09.126531799Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server metrics:9090","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:39:37.979780885Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0002687d0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:40:07.99507908Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0006ff770}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:40:37.986369235Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc000268f00}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:41:07.982979972Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0008485a0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:41:37.980161709Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0006da230}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [workload queue-proxy]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:42:47.983115736Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc00075b860}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:44:47.980513113Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0005a1e50}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:47:57.980256172Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0005cb9f0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:53:27.979955067Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0005a1e00}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:53:57.978948207Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc00070aa50}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 187 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 134 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:59:27.980148066Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc000842320}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T09:59:57.979179693Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0005d41e0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:05:37.982767463Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc000603c70}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:06:07.982605842Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc00070c2d0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:11:47.984363583Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0007daa50}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:12:17.982452018Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0006f60f0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 173 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 187 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:17:47.980327597Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc00075c870}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:18:17.985455243Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0007721e0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:23:57.980994218Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc000789220}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:24:28.014822594Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc000721f40}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:29:57.982515215Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0006dc730}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:30:27.979682231Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0006b38b0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 188 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 196 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 173 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 160 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 170 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 181 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 181 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 179 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 177 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 198 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 166 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 165 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 195 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:35:57.981360884Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc000664e60}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 187 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 197 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:36:27.988850095Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc00077fe00}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 182 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:42:07.979716571Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc0007c4eb0}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:42:37.98781695Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc00080fb30}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:48:17.997558179Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc00070d630}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ WARNING: Detected container restart. Pod: where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr. Container: workload.
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] ERROR: transport error 202: socket creation failed: Address family not supported by protocol
where-for-diâ€¦ â”‚ [workload] ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)
where-for-diâ€¦ â”‚ [workload] JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [src/jdk.jdwp.agent/share/native/libjdwp/debugInit.c:734]
where-for-diâ€¦ â”‚ [workload] [[Command exited with 2]]
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-03T10:48:47.983541056Z","logger":"queueproxy","caller":"sharedmain/handlers.go:107","message":"Attached drain handler from user-container&{GET /wait-for-drain HTTP/1.1 1 1 map[Accept:[*/*] Accept-Encoding:[gzip] User-Agent:[kube-lifecycle/1.26]] {} <nil> 0 [] false 10.200.0.32:8022 map[] map[] <nil> map[] 10.200.0.1:48836 /wait-for-drain <nil> <nil> <nil> 0xc00067ff90}","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-584c76d9cf-nckqr"}
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 200 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 201 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ Reconnecting... Error port-forwarding where-for-dinner-search-proc (8080 -> 8080): Unable to listen on port 8080: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:8080: bind: address already in use unable to create listener: Error listen tcp6 [::1]:8080: bind: address already in use]
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 202 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
Tilt started on http://localhost:53307/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner/where-for-dinner-search-proc/Tiltfile
Successfully loaded Tiltfile (811.407Âµs)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner-search-proc:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 5.29s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 5.29s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ [event: imagerepository workloads/where-for-dinner-search-proc] Failed to update status: Operation cannot be fulfilled on imagerepositories.source.apps.tanzu.vmware.com "where-for-dinner-search-proc": the object has been modified; please apply your changes to the latest version and try again
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-proc-build-1-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: imagerepository workloads/where-for-dinner-search-proc-delivery] Failed to update status: Operation cannot be fulfilled on imagerepositories.source.apps.tanzu.vmware.com "where-for-dinner-search-proc-delivery": the object has been modified; please apply your changes to the latest version and try again
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32menv:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_OCI_SOURCE[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:6515f448fb5ae20d674a59050fcff30ae7b62055780d0a883d67830df9178dfd[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_LIVE_RELOAD_ENABLED[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "true"[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_JVM_VERSION[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "17"[0m
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31msource: {}[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32msource:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  blob:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search-proc/6515f448fb5ae20d674a59050fcff30ae7b62055780d0a883d67830df9178dfd.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search-proc/6515f448fb5ae20d674a59050fcff30ae7b62055780d0a883d67830df9178dfd.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search-proc/6515f448fb5ae20d674a59050fcff30ae7b62055780d0a883d67830df9178dfd.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [analyze detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer started at 2024-01-04T10:11:28Z
where-for-diâ€¦ â”‚ [analyze] Restoring data for SBOM from previous image
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer ran for 6.64882351s and ended at 2024-01-04T10:11:34Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/dotnet-core-lite@2.9.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/go-lite@2.2.3' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/nodejs-lite@2.4.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] Timer: Detector started at 2024-01-04T10:11:36Z
where-for-diâ€¦ â”‚ [detect] 11 of 26 buildpacks participating
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/ca-certificates   3.6.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/bellsoft-liberica 9.13.0
where-for-diâ€¦ â”‚ [detect] paketo-buildpacks/syft             1.10.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/maven             6.15.12
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/watchexec         2.8.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/executable-jar    6.8.3
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/apache-tomcat     7.14.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/liberty           3.8.11
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/dist-zip          5.6.8
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/spring-boot       5.27.6
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/image-labels      4.5.6
where-for-diâ€¦ â”‚ [detect] Timer: Detector ran for 1.308286919s and ended at 2024-01-04T10:11:37Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [restore] Timer: Restorer started at 2024-01-04T10:11:38Z
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/ca-certificates:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:java-security-properties" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:jre" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:spring-cloud-bindings" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:web-application-type" from app image
where-for-diâ€¦ â”‚ [restore] Timer: Restorer ran for 4.446661ms and ended at 2024-01-04T10:11:38Z
where-for-diâ€¦ â”‚ [build] Timer: Builder started at 2024-01-04T10:11:39Z
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for CA Certificates[0m[34m 3.6.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-ca-certificates[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for BellSoft Liberica[0m[34m 9.13.0[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-bellsoft-liberica[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ARGS           --no-man-pages --no-header-files --strip-debug --compress=1  configure custom link arguments (--output must be omitted)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ENABLED        false                                                        enables running jlink tool to generate custom JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_TYPE                 JRE                                                          the JVM type - JDK or JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_VERSION              17                                                           the Java version[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_ENABLED           false                                                        enables Java remote debugging support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_PORT              8000                                                         configure the remote debugging port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_SUSPEND           false                                                        configure whether to suspend execution until a debugger has attached[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_HEAP_DUMP_PATH                                                                       write heap dumps on error to this path[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_ENABLED        true                                                         enables Java Native Memory Tracking (NMT)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_LEVEL          summary                                                      configure level of NMT, summary or detail[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ARGS                                                                             configure custom Java Flight Recording (JFR) arguments[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ENABLED             false                                                        enables Java Flight Recording (JFR)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_ENABLED             false                                                        enables Java Management Extensions (JMX)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_PORT                5000                                                         configure the JMX port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_HEAD_ROOM           0                                                            the headroom in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_LOADED_CLASS_COUNT  35% of classes                                               the number of loaded classes in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_THREAD_COUNT        250                                                          the number of threads in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $JAVA_TOOL_OPTIONS                                                                        the JVM launch flags[0m
where-for-diâ€¦ â”‚ [build] [2m    [2mUsing Java version 17 from BP_JVM_VERSION[0m[2m[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JDK 17.0.9[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/bell-sw/Liberica/releases/download/17.0.9+11/bellsoft-jdk17.0.9+11-linux-amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Expanding to /layers/tanzu-buildpacks_bellsoft-liberica/jdk[0m
Tilt started on http://localhost:53307/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner/where-for-dinner-search-proc/Tiltfile
Successfully loaded Tiltfile (632.817Âµs)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Error: Writing 'local-source-proxy.tap-local-source-system.svc.cluster.local/source:workloads-where-for-dinner-search-proc': Writing image: POST https://tap-full.eng.vmware.com:8443/api/v1/namespaces/tap-local-source-system/services/http:local-source-proxy:5001/proxy/v2/source/blobs/uploads/: UNAUTHORIZED: authentication required, visit https://aka.ms/acr/authorization for more information.; [map[Action:pull Name:ipillai/tkgifull/lsp Type:repository] map[Action:push Name:ipillai/tkgifull/lsp Type:repository]]
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ ERROR: Build Failed: apply command exited with status 1Tilt started on http://localhost:53307/
v0.33.10, built 2023-12-15

Initial Build
Loading Tiltfile at: /home/kubo/tkgi/where-for-dinner/where-for-dinner-search-proc/Tiltfile
Successfully loaded Tiltfile (1.280122ms)
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Initial Build
where-for-diâ€¦ â”‚ STEP 1/1 â€” Deploying
where-for-diâ€¦ â”‚      Running cmd: tanzu apps workload apply -f config/workload.yaml --update-strategy replace --debug --live-update --local-path . --namespace workloads --wait-timeout 10m00s --type web --yes --output yaml
where-for-diâ€¦ â”‚      Objects applied to cluster:
where-for-diâ€¦ â”‚        â†’ where-for-dinner-search-proc:workload
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚      Step 1 - 5.02s (Deploying)
where-for-diâ€¦ â”‚      DONE IN: 5.02s 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-proc-build-1-build-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:3f50a42730f27fd7798f7b7d0d1a17134419410ba1a2c57842e9b52ba73f6754" already present on machine
where-for-diâ€¦ â”‚ [prepare] Build reason(s): CONFIG
where-for-diâ€¦ â”‚ [prepare] CONFIG:
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32menv:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_OCI_SOURCE[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_LIVE_RELOAD_ENABLED[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "true"[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m- name: BP_JVM_VERSION[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  value: "17"[0m
where-for-diâ€¦ â”‚ [prepare] 	resources: {}
where-for-diâ€¦ â”‚ [prepare] 	[31m-[0m [31msource: {}[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32msource:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m  blob:[0m
where-for-diâ€¦ â”‚ [prepare] 	[32m+[0m [32m    url: http://source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search-proc/aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e.tar.gz[0m
where-for-diâ€¦ â”‚ [prepare] Loading registry credentials from service account secrets
where-for-diâ€¦ â”‚ [prepare] Loading secret for "registry.tanzu.vmware.com" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading secret for "tapscale.azurecr.io" from secret "registries-credentials" at location "/var/build-secrets/registries-credentials"
where-for-diâ€¦ â”‚ [prepare] Loading cluster credential helpers
where-for-diâ€¦ â”‚ [prepare] Downloading source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search-proc/aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e.tar.gz...
where-for-diâ€¦ â”‚ [prepare] Successfully downloaded source-controller-manager-artifact-service.source-system.svc.cluster.local./imagerepository/workloads/where-for-dinner-search-proc/aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e.tar.gz in path "/workspace"
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [analyze detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer started at 2024-01-04T10:17:13Z
where-for-diâ€¦ â”‚ [analyze] Restoring data for SBOM from previous image
where-for-diâ€¦ â”‚ [analyze] Timer: Analyzer ran for 6.53529979s and ended at 2024-01-04T10:17:20Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [detect restore build export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/dotnet-core-lite@2.9.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/go-lite@2.2.3' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] [33;1mWarning: [0mBuildpack 'tanzu-buildpacks/nodejs-lite@2.4.0' requests deprecated API '0.6'
where-for-diâ€¦ â”‚ [detect] Timer: Detector started at 2024-01-04T10:17:20Z
where-for-diâ€¦ â”‚ [detect] 11 of 26 buildpacks participating
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/ca-certificates   3.6.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/bellsoft-liberica 9.13.0
where-for-diâ€¦ â”‚ [detect] paketo-buildpacks/syft             1.10.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/maven             6.15.12
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/watchexec         2.8.7
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/executable-jar    6.8.3
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/apache-tomcat     7.14.1
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/liberty           3.8.11
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/dist-zip          5.6.8
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/spring-boot       5.27.6
where-for-diâ€¦ â”‚ [detect] tanzu-buildpacks/image-labels      4.5.6
where-for-diâ€¦ â”‚ [detect] Timer: Detector ran for 1.102804532s and ended at 2024-01-04T10:17:22Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚ [restore] Timer: Restorer started at 2024-01-04T10:17:22Z
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/ca-certificates:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:helper" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:java-security-properties" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/bellsoft-liberica:jre" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:spring-cloud-bindings" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:web-application-type" from app image
where-for-diâ€¦ â”‚ [restore] Restoring metadata for "tanzu-buildpacks/spring-boot:helper" from app image
where-for-diâ€¦ â”‚ [restore] Timer: Restorer ran for 3.048133ms and ended at 2024-01-04T10:17:22Z
where-for-diâ€¦ â”‚ [build] Timer: Builder started at 2024-01-04T10:17:23Z
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for CA Certificates[0m[34m 3.6.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-ca-certificates[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for BellSoft Liberica[0m[34m 9.13.0[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-bellsoft-liberica[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ARGS           --no-man-pages --no-header-files --strip-debug --compress=1  configure custom link arguments (--output must be omitted)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_JLINK_ENABLED        false                                                        enables running jlink tool to generate custom JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_TYPE                 JRE                                                          the JVM type - JDK or JRE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JVM_VERSION              17                                                           the Java version[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_ENABLED           false                                                        enables Java remote debugging support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_PORT              8000                                                         configure the remote debugging port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_DEBUG_SUSPEND           false                                                        configure whether to suspend execution until a debugger has attached[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_HEAP_DUMP_PATH                                                                       write heap dumps on error to this path[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_ENABLED        true                                                         enables Java Native Memory Tracking (NMT)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JAVA_NMT_LEVEL          summary                                                      configure level of NMT, summary or detail[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ARGS                                                                             configure custom Java Flight Recording (JFR) arguments[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JFR_ENABLED             false                                                        enables Java Flight Recording (JFR)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_ENABLED             false                                                        enables Java Management Extensions (JMX)[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JMX_PORT                5000                                                         configure the JMX port[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_HEAD_ROOM           0                                                            the headroom in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_LOADED_CLASS_COUNT  35% of classes                                               the number of loaded classes in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_JVM_THREAD_COUNT        250                                                          the number of threads in memory calculation[0m
where-for-diâ€¦ â”‚ [build] [2m    $JAVA_TOOL_OPTIONS                                                                        the JVM launch flags[0m
where-for-diâ€¦ â”‚ [build] [2m    [2mUsing Java version 17 from BP_JVM_VERSION[0m[2m[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JDK 17.0.9[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/bell-sw/Liberica/releases/download/17.0.9+11/bellsoft-jdk17.0.9+11-linux-amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Expanding to /layers/tanzu-buildpacks_bellsoft-liberica/jdk[0m
where-for-diâ€¦ â”‚ [build] [2m    Adding 137 container CA certificates to JVM truststore[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JAVA_HOME.override[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/JDK_HOME.override[0m
where-for-diâ€¦ â”‚ [build]   [34mBellSoft Liberica JRE 17.0.9[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mJava Security Properties[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mPaketo Syft Buildpack[0m[34m 1.10.1[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/paketo-buildpacks/syft[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/anchore/syft/releases/download/v0.41.1/syft_0.41.1_linux_amd64.tar.gz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.build/SYFT_CHECK_FOR_APP_UPDATE.default[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Maven[0m[34m 6.15.12[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-maven[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_EXCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are removed[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_INCLUDE_FILES                                                                            colon separated list of glob patterns, matched source files are included[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_JAVA_INSTALL_NODE                 false                                                  whether to install Yarn/Node binaries based on the presence of a package.json or yarn.lock file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ACTIVE_PROFILES                                                                    the active profiles (comma separated: such as: p1,!p2,?p3) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_ADDITIONAL_BUILD_ARGUMENTS                                                         the additionnal arguments (appended to BP_MAVEN_BUILD_ARGUMENTS) to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILD_ARGUMENTS             -Dmaven.test.skip=true --no-transfer-progress package  the arguments to pass to Maven[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_ARTIFACT              target/*.[ejw]ar                                       the built application artifact explicitly.  Supersedes $BP_MAVEN_BUILT_MODULE[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_BUILT_MODULE                                                                       the module to find application artifact in[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_DAEMON_ENABLED              false                                                  use maven daemon[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_POM_FILE                    pom.xml                                                the location of the main pom.xml file, relative to the application root[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_SETTINGS_PATH                                                                      the path to a Maven settings file[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_MAVEN_VERSION                     3                                                      the Maven version[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_NODE_PROJECT_PATH                                                                        configure a project subdirectory to look for `package.json` and `yarn.lock` files[0m
where-for-diâ€¦ â”‚ [build] [2m    Creating cache directory /home/cnb/.m2[0m
where-for-diâ€¦ â”‚ [build]   [34mCompiled Application[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Executing mvnw --batch-mode -Dmaven.test.skip=true --no-transfer-progress package[0m
where-for-diâ€¦ â”‚ [build]         % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
where-for-diâ€¦ â”‚ [build]                                        Dload  Upload   Total   Spent    Left  Speed
where-for-diâ€¦ â”‚ [build]         0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 59925  100 59925    0     0   207k      0 --:--:-- --:--:-- --:--:--  208k
where-for-diâ€¦ â”‚ [build]       [0m[0m[INFO] Scanning for projects...
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --------------< com.example:where-for-dinner-search-proc >--------------
where-for-diâ€¦ â”‚ [build]       [INFO] Building where-for-dinner-search-proc 0.0.1-SNAPSHOT
where-for-diâ€¦ â”‚ [build]       [INFO] --------------------------------[ jar ]---------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:resources (default-resources) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 1 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] Copying 0 resource from src/main/resources to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Changes detected - recompiling the module! :dependency
where-for-diâ€¦ â”‚ [build]       [INFO] Compiling 15 source files with javac [debug release 17] to target/classes
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-resources-plugin:3.3.1:testResources (default-testResources) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not copying test resources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-compiler-plugin:3.11.0:testCompile (default-testCompile) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Not compiling test sources
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-surefire-plugin:3.0.0:test (default-test) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Tests are skipped.
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- maven-jar-plugin:3.3.0:jar (default-jar) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Building jar: /workspace/target/where-for-dinner-search-proc-0.0.1-SNAPSHOT.jar
where-for-diâ€¦ â”‚ [build]       [INFO] 
where-for-diâ€¦ â”‚ [build]       [INFO] --- spring-boot-maven-plugin:3.1.3:repackage (repackage) @ where-for-dinner-search-proc ---
where-for-diâ€¦ â”‚ [build]       [INFO] Replacing main artifact /workspace/target/where-for-dinner-search-proc-0.0.1-SNAPSHOT.jar with repackaged archive, adding nested dependencies in BOOT-INF/.
where-for-diâ€¦ â”‚ [build]       [INFO] The original artifact has been renamed to /workspace/target/where-for-dinner-search-proc-0.0.1-SNAPSHOT.jar.original
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] BUILD SUCCESS
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [INFO] Total time:  55.829 s
where-for-diâ€¦ â”‚ [build]       [INFO] Finished at: 2024-01-04T10:18:35Z
where-for-diâ€¦ â”‚ [build]       [INFO] ------------------------------------------------------------------------
where-for-diâ€¦ â”‚ [build]       [0m[0m
where-for-diâ€¦ â”‚ [build]   Removing source code
where-for-diâ€¦ â”‚ [build]   Restoring application artifact
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Watchexec[0m[34m 2.8.7[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-watchexec[0m
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://github.com/watchexec/watchexec/releases/download/cli-v1.20.5/watchexec-1.20.5-x86_64-unknown-linux-musl.tar.xz[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Executable JAR[0m[34m 6.8.3[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-executable-jar[0m
where-for-diâ€¦ â”‚ [build]   [34mClass Path[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.delim[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env/CLASSPATH.prepend[0m
where-for-diâ€¦ â”‚ [build]   Process types:
where-for-diâ€¦ â”‚ [build]     [36mexecutable-jar[0m: java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mreload[0m:         watchexec -r --shell=none -- java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mtask[0m:           java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build]     [36mweb[0m:            java org.springframework.boot.loader.JarLauncher (direct)
where-for-diâ€¦ â”‚ [build] SKIPPED: `Main-Class` found in `META-INF/MANIFEST.MF`, skipping build
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Spring Boot[0m[34m 5.27.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-spring-boot[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_DISABLED   false  whether to contribute Spring Boot cloud bindings support[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_SPRING_CLOUD_BINDINGS_VERSION    1      default version of Spring Cloud Bindings library to contribute[0m
where-for-diâ€¦ â”‚ [build]   [2mLaunch Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_DISABLED  false  whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build] [2m    $BPL_SPRING_CLOUD_BINDINGS_ENABLED   true   Deprecated - whether to auto-configure Spring Boot environment properties from bindings[0m
where-for-diâ€¦ â”‚ [build]   Creating slices from layers index
where-for-diâ€¦ â”‚ [build] [2m    dependencies (55.6 MB)[0m
where-for-diâ€¦ â”‚ [build] [2m    spring-boot-loader (269.4 KB)[0m
where-for-diâ€¦ â”‚ [build] [2m    snapshot-dependencies (0.0 B)[0m
where-for-diâ€¦ â”‚ [build] [2m    application (132.7 KB)[0m
where-for-diâ€¦ â”‚ [build]   [34mLaunch Helper[0m: [32mReusing[0m cached layer
where-for-diâ€¦ â”‚ [build]   [34mSpring Cloud Bindings 2.0.2[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    [33mDownloading[0m[2m from https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-bindings/2.0.2/spring-cloud-bindings-2.0.2.jar[0m
where-for-diâ€¦ â”‚ [build] [2m    Verifying checksum[0m
where-for-diâ€¦ â”‚ [build] [2m    Copying to /layers/tanzu-buildpacks_spring-boot/spring-cloud-bindings[0m
where-for-diâ€¦ â”‚ [build]   [34mWeb Application Type[0m: [33mContributing[0m to layer
where-for-diâ€¦ â”‚ [build] [2m    Reactive web application detected[0m
where-for-diâ€¦ â”‚ [build] [2m    Writing env.launch/BPL_JVM_THREAD_COUNT.default[0m
where-for-diâ€¦ â”‚ [build]   4 application slices
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.title
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.version
where-for-diâ€¦ â”‚ [build]     org.springframework.boot.version
where-for-diâ€¦ â”‚ [build] [34m[0m
where-for-diâ€¦ â”‚ [build] [34m[1mTanzu Buildpack for Image Labels[0m[34m 4.5.6[0m
where-for-diâ€¦ â”‚ [build]   [34;2;3mhttps://github.com/pivotal-cf/tanzu-image-labels[0m
where-for-diâ€¦ â”‚ [build]   [2mBuild Configuration:[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_IMAGE_LABELS                                                                                                                                                                arbitrary image labels[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_AUTHORS                                                                                                                                                                 the org.opencontainers.image.authors image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_CREATED                                                                                                                                                                 the org.opencontainers.image.created image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DESCRIPTION                                                                                                                                                             the org.opencontainers.image.description image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_DOCUMENTATION                                                                                                                                                           the org.opencontainers.image.documentation image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_LICENSES                                                                                                                                                                the org.opencontainers.image.licenses image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REF_NAME                                                                                                                                                                the org.opencontainers.image.ref.name image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_REVISION                                                                                                                                                                the org.opencontainers.image.revision image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_SOURCE         tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e  the org.opencontainers.image.source image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_TITLE                                                                                                                                                                   the org.opencontainers.image.title image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_URL                                                                                                                                                                     the org.opencontainers.image.url image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VENDOR                                                                                                                                                                  the org.opencontainers.image.vendor image label[0m
where-for-diâ€¦ â”‚ [build] [2m    $BP_OCI_VERSION                                                                                                                                                                 the org.opencontainers.image.version image label[0m
where-for-diâ€¦ â”‚ [build]   Image labels:
where-for-diâ€¦ â”‚ [build]     org.opencontainers.image.source
where-for-diâ€¦ â”‚ [build] Timer: Builder ran for 1m16.005744599s and ended at 2024-01-04T10:18:39Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "tapscale.azurecr.io/buildservice/tkgi2/buildservice@sha256:c50eb31a7cacf39f6837fd0ffe79ef86e463c49250d98bee0585037f2ac9265e" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Initialized - (ContainersNotInitialized): containers with incomplete status: [export]
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ [export] Reusing layers from image 'tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads@sha256:a4cd65562d82656bdb1211372ae2331ab1ff675f62ff3b8de86aaa0754152b96'
where-for-diâ€¦ â”‚ [export] Timer: Exporter started at 2024-01-04T10:20:16Z
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/ca-certificates:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:java-security-properties'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/bellsoft-liberica:jre'
where-for-diâ€¦ â”‚ [export] Adding layer 'tanzu-buildpacks/watchexec:watchexec'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/executable-jar:classpath'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:helper'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:spring-cloud-bindings'
where-for-diâ€¦ â”‚ [export] Reusing layer 'tanzu-buildpacks/spring-boot:web-application-type'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:launch.sbom'
where-for-diâ€¦ â”‚ [export] Reusing 2/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Adding 3/5 app layer(s)
where-for-diâ€¦ â”‚ [export] Reusing layer 'buildpacksio/lifecycle:launcher'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:config'
where-for-diâ€¦ â”‚ [export] Adding layer 'buildpacksio/lifecycle:process-types'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.lifecycle.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.build.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'io.buildpacks.project.metadata'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.title'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.springframework.boot.version'
where-for-diâ€¦ â”‚ [export] Adding label 'org.opencontainers.image.source'
where-for-diâ€¦ â”‚ [export] Setting default process type 'reload'
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads... started at 2024-01-04T10:20:17Z
where-for-diâ€¦ â”‚ [export] *** Images (sha256:fc06c35673d7480dd479d71337269db01c37d1df3db5fa5d01ac927f61d08b97):
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads
where-for-diâ€¦ â”‚ [export]       tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads:b1.20240104.101544
where-for-diâ€¦ â”‚ [export] Timer: Saving tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads... ran for 4.789447091s and ended at 2024-01-04T10:20:22Z
where-for-diâ€¦ â”‚ [export] Timer: Exporter ran for 6.055478434s and ended at 2024-01-04T10:20:22Z
where-for-diâ€¦ â”‚ [export] Timer: Cache started at 2024-01-04T10:20:22Z
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/bellsoft-liberica:jdk'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'paketo-buildpacks/syft:syft'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:application'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'tanzu-buildpacks/maven:cache'
where-for-diâ€¦ â”‚ [export] Adding cache layer 'buildpacksio/lifecycle:cache.sbom'
where-for-diâ€¦ â”‚ [export] Timer: Cache ran for 12.695359104s and ended at 2024-01-04T10:20:35Z
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-build-1-build-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:0b5c1bfe87a700cf473725e7cd133f4c7da48ec85baccf2d0c5d6127beb675e7" already present on machine
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 4m51s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [completion]
where-for-diâ€¦ â”‚ Build successful
where-for-diâ€¦ â”‚ [event: taskrun workloads/where-for-dinner-search-proc-config-writer-s9pkz] 1 error occurred:
where-for-diâ€¦ â”‚ 	* resource request in progress
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-proc-config-writer-s9pkz-pod):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-config-writer-s9pkz-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:5f9057f548e6a2453448bbf53b8a1daa465dfa8c838169cf03a1e65300318b2e" already present on machine
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-config-writer-s9pkz-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:35af308924a4607a969455aa8301d347012976a4ddf706783419ffec87c8e2bc" already present on machine
where-for-diâ€¦ â”‚ [prepare] 2024/01/04 10:20:53 Entrypoint initialization
where-for-diâ€¦ â”‚ [event: pod workloads/where-for-dinner-search-proc-config-writer-s9pkz-pod] Container image "registry.tanzu.vmware.com/tanzu-application-platform/tap-packages@sha256:1d063b6be582302e083fecace0e105ef3e65c337a1f2c00e07175db88d36b2ad" already present on machine
where-for-diâ€¦ â”‚ [place-scripts] 2024/01/04 10:20:54 Decoded script /tekton/scripts/script-0-vsnkh
where-for-diâ€¦ â”‚ ++ mktemp -d
where-for-diâ€¦ â”‚ + cd /tmp/tmp.IeJaj4rZOC
where-for-diâ€¦ â”‚ + base64 --decode
where-for-diâ€¦ â”‚ + echo -e {"delivery.yml":"apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: where-for-dinner-search-proc\n  annotations:\n    ootb.apps.tanzu.vmware.com/servicebinding-workload: \"true\"\n    ootb.apps.tanzu.vmware.com/apidescriptor-ref: \"true\"\n    kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings\n  labels:\n    app.kubernetes.io/part-of: where-for-dinner-search-proc\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search-proc\nspec:\n  template:\n    metadata:\n      annotations:\n        apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc?sub_path=/\n        apps.tanzu.vmware.com/debug: \"true\"\n        apps.tanzu.vmware.com/live-update: \"true\"\n        autoscaling.knative.dev/maxScale: \"1\"\n        autoscaling.knative.dev/minScale: \"1\"\n        boot.spring.io/actuator: http://:8081/actuator\n        boot.spring.io/version: 3.1.3\n        conventions.carto.run/applied-conventions: |-\n          appliveview-sample/app-live-view-appflavour-check\n          developer-conventions/debug-convention\n          developer-conventions/live-update-convention\n          developer-conventions/add-source-image-label\n          spring-boot-convention/auto-configure-actuators-check\n          spring-boot-convention/is-native-app-check\n          spring-boot-convention/spring-boot\n          spring-boot-convention/spring-boot-web\n          spring-boot-convention/spring-boot-actuator\n          spring-boot-convention/spring-boot-actuator-probes\n          spring-boot-convention/app-live-view-appflavour-check\n          spring-boot-convention/app-live-view-connector-boot\n          spring-boot-convention/app-live-view-appflavours-boot\n          spring-boot-convention/service-intent-rabbitmq\n          spring-boot-convention/service-intent-kafka\n        developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e\n        developer.conventions/target-containers: workload\n        local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e\n        services.conventions.carto.run/kafka: kafka-clients/3.4.1\n        services.conventions.carto.run/rabbitmq: amqp-client/5.17.1\n      labels:\n        app.kubernetes.io/component: run\n        app.kubernetes.io/part-of: where-for-dinner-search-proc\n        apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n        apps.tanzu.vmware.com/has-tests: \"true\"\n        apps.tanzu.vmware.com/workload-type: web\n        carto.run/workload-name: where-for-dinner-search-proc\n        conventions.carto.run/framework: spring-boot\n        networking.knative.dev/visibility: cluster-local\n        services.conventions.carto.run/kafka: workload\n        services.conventions.carto.run/rabbitmq: workload\n        tanzu.app.live.view: \"true\"\n        tanzu.app.live.view.application.actuator.path: actuator\n        tanzu.app.live.view.application.actuator.port: \"8081\"\n        tanzu.app.live.view.application.flavours: spring-boot\n        tanzu.app.live.view.application.name: where-for-dinner-search-proc\n    spec:\n      containers:\n      - env:\n        - name: JAVA_TOOL_OPTIONS\n          value: -Djava.net.preferIPv4Stack=\"true\" -Dmanagement.endpoint.health.probes.add-additional-paths=\"true\" -Dmanagement.endpoint.health.show-details=\"always\" -Dmanagement.endpoints.web.base-path=\"/actuator\" -Dmanagement.endpoints.web.exposure.include=\"*\" -Dmanagement.health.probes.enabled=\"true\" -Dmanagement.server.port=\"8081\" -Dserver.port=\"8080\"\n        - name: BPL_DEBUG_ENABLED\n          value: \"true\"\n        - name: BPL_DEBUG_PORT\n          value: \"9005\"\n        image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads@sha256:fc06c35673d7480dd479d71337269db01c37d1df3db5fa5d01ac927f61d08b97\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: 8080\n            scheme: HTTP\n        name: workload\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 1500m\n            memory: 750M\n          requests:\n            cpu: 100m\n            memory: 500M\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        startupProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n            scheme: HTTP\n      serviceAccountName: default\n","kapp-config.yml":"apiVersion: kapp.k14s.io/v1alpha1\nkind: Config\nrebaseRules:\n- path:\n  - metadata\n  - annotations\n  - serving.knative.dev/creator\n  type: copy\n  sources:\n  - new\n  - existing\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n- path:\n  - metadata\n  - annotations\n  - serving.knative.dev/lastModifier\n  type: copy\n  sources:\n  - new\n  - existing\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\nwaitRules:\n- resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n  conditionMatchers:\n  - type: Ready\n    status: \"True\"\n    success: true\n  - type: Ready\n    status: \"False\"\n    failure: true\nownershipLabelRules:\n- path:\n  - spec\n  - template\n  - metadata\n  - labels\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: serving.knative.dev/v1\n      kind: Service\n","serviceclaims.yml":"apiVersion: kapp.k14s.io/v1alpha1\nkind: Config\nwaitRules:\n- conditionMatchers:\n  - type: ServiceAvailable\n    status: \"False\"\n    failure: true\n  - type: ServiceAvailable\n    status: \"True\"\n    success: true\n  resourceMatchers:\n  - apiVersionKindMatcher:\n      apiVersion: servicebinding.io/v1alpha3\n      kind: ServiceBinding\n---\napiVersion: servicebinding.io/v1alpha3\nkind: ServiceBinding\nmetadata:\n  name: where-for-dinner-search-proc-rmq\n  annotations:\n    local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e\n    autoscaling.knative.dev/minScale: \"1\"\n    kapp.k14s.io/change-group: servicebinding.io/ServiceBindings\n  labels:\n    app.kubernetes.io/part-of: where-for-dinner-search-proc\n    apps.tanzu.vmware.com/auto-configure-actuators: \"true\"\n    apps.tanzu.vmware.com/has-tests: \"true\"\n    apps.tanzu.vmware.com/workload-type: web\n    networking.knative.dev/visibility: cluster-local\n    app.kubernetes.io/component: run\n    carto.run/workload-name: where-for-dinner-search-proc\nspec:\n  name: rmq\n  service:\n    apiVersion: services.apps.tanzu.vmware.com/v1alpha1\n    kind: ClassClaim\n    name: msgbroker-where-for-dinner\n  workload:\n    apiVersion: serving.knative.dev/v1\n    kind: Service\n    name: where-for-dinner-search-proc\n"}
where-for-diâ€¦ â”‚ ++ jq -r 'to_entries | .[] | @sh "mkdir -p $(dirname \(.key)) && echo \(.value) > \(.key)"'
where-for-diâ€¦ â”‚ ++ cat files.json
where-for-diâ€¦ â”‚ + eval 'mkdir -p $(dirname '\''delivery.yml'\'') && echo '\''apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-rabbitmq
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-kafka
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: kafka-clients/3.4.1
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: amqp-client/5.17.1
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: workload
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads@sha256:fc06c35673d7480dd479d71337269db01c37d1df3db5fa5d01ac927f61d08b97
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '\'' > '\''delivery.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''kapp-config.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '\'' > '\''kapp-config.yml'\''
where-for-diâ€¦ â”‚ mkdir -p $(dirname '\''serviceclaims.yml'\'') && echo '\''apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - conditionMatchers:
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚       kind: ServiceBinding
where-for-diâ€¦ â”‚ ---
where-for-diâ€¦ â”‚ apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚ kind: ServiceBinding
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-proc-rmq
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-group: servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   name: rmq
where-for-diâ€¦ â”‚   service:
where-for-diâ€¦ â”‚     apiVersion: services.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚     kind: ClassClaim
where-for-diâ€¦ â”‚     name: msgbroker-where-for-dinner
where-for-diâ€¦ â”‚   workload:
where-for-diâ€¦ â”‚     apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚     kind: Service
where-for-diâ€¦ â”‚     name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ '\'' > '\''serviceclaims.yml'\'''
where-for-diâ€¦ â”‚ +++ dirname delivery.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚ kind: Service
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/servicebinding-workload: "true"
where-for-diâ€¦ â”‚     ootb.apps.tanzu.vmware.com/apidescriptor-ref: "true"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-rule: upsert after upserting servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   template:
where-for-diâ€¦ â”‚     metadata:
where-for-diâ€¦ â”‚       annotations:
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/correlationid: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc?sub_path=/
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/debug: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/live-update: "true"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/maxScale: "1"
where-for-diâ€¦ â”‚         autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚         boot.spring.io/actuator: http://:8081/actuator
where-for-diâ€¦ â”‚         boot.spring.io/version: 3.1.3
where-for-diâ€¦ â”‚         conventions.carto.run/applied-conventions: |-
where-for-diâ€¦ â”‚           appliveview-sample/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           developer-conventions/debug-convention
where-for-diâ€¦ â”‚           developer-conventions/live-update-convention
where-for-diâ€¦ â”‚           developer-conventions/add-source-image-label
where-for-diâ€¦ â”‚           spring-boot-convention/auto-configure-actuators-check
where-for-diâ€¦ â”‚           spring-boot-convention/is-native-app-check
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-web
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator
where-for-diâ€¦ â”‚           spring-boot-convention/spring-boot-actuator-probes
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavour-check
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-connector-boot
where-for-diâ€¦ â”‚           spring-boot-convention/app-live-view-appflavours-boot
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-rabbitmq
where-for-diâ€¦ â”‚           spring-boot-convention/service-intent-kafka
where-for-diâ€¦ â”‚         developer.apps.tanzu.vmware.com/image-source-digest: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e
where-for-diâ€¦ â”‚         developer.conventions/target-containers: workload
where-for-diâ€¦ â”‚         local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: kafka-clients/3.4.1
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: amqp-client/5.17.1
where-for-diâ€¦ â”‚       labels:
where-for-diâ€¦ â”‚         app.kubernetes.io/component: run
where-for-diâ€¦ â”‚         app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚         apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚         carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚         conventions.carto.run/framework: spring-boot
where-for-diâ€¦ â”‚         networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚         services.conventions.carto.run/kafka: workload
where-for-diâ€¦ â”‚         services.conventions.carto.run/rabbitmq: workload
where-for-diâ€¦ â”‚         tanzu.app.live.view: "true"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.path: actuator
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.actuator.port: "8081"
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.flavours: spring-boot
where-for-diâ€¦ â”‚         tanzu.app.live.view.application.name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     spec:
where-for-diâ€¦ â”‚       containers:
where-for-diâ€¦ â”‚       - env:
where-for-diâ€¦ â”‚         - name: JAVA_TOOL_OPTIONS
where-for-diâ€¦ â”‚           value: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_ENABLED
where-for-diâ€¦ â”‚           value: "true"
where-for-diâ€¦ â”‚         - name: BPL_DEBUG_PORT
where-for-diâ€¦ â”‚           value: "9005"
where-for-diâ€¦ â”‚         image: tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads@sha256:fc06c35673d7480dd479d71337269db01c37d1df3db5fa5d01ac927f61d08b97
where-for-diâ€¦ â”‚         livenessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /livez
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         name: workload
where-for-diâ€¦ â”‚         ports:
where-for-diâ€¦ â”‚         - containerPort: 8080
where-for-diâ€¦ â”‚           protocol: TCP
where-for-diâ€¦ â”‚         readinessProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚         resources:
where-for-diâ€¦ â”‚           limits:
where-for-diâ€¦ â”‚             cpu: 1500m
where-for-diâ€¦ â”‚             memory: 750M
where-for-diâ€¦ â”‚           requests:
where-for-diâ€¦ â”‚             cpu: 100m
where-for-diâ€¦ â”‚             memory: 500M
where-for-diâ€¦ â”‚         securityContext:
where-for-diâ€¦ â”‚           allowPrivilegeEscalation: false
where-for-diâ€¦ â”‚           capabilities:
where-for-diâ€¦ â”‚             drop:
where-for-diâ€¦ â”‚             - ALL
where-for-diâ€¦ â”‚           runAsNonRoot: true
where-for-diâ€¦ â”‚           runAsUser: 1000
where-for-diâ€¦ â”‚           seccompProfile:
where-for-diâ€¦ â”‚             type: RuntimeDefault
where-for-diâ€¦ â”‚         startupProbe:
where-for-diâ€¦ â”‚           httpGet:
where-for-diâ€¦ â”‚             path: /readyz
where-for-diâ€¦ â”‚             port: 8080
where-for-diâ€¦ â”‚             scheme: HTTP
where-for-diâ€¦ â”‚       serviceAccountName: default
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname kapp-config.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ rebaseRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/creator
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - annotations
where-for-diâ€¦ â”‚   - serving.knative.dev/lastModifier
where-for-diâ€¦ â”‚   type: copy
where-for-diâ€¦ â”‚   sources:
where-for-diâ€¦ â”‚   - new
where-for-diâ€¦ â”‚   - existing
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚   conditionMatchers:
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   - type: Ready
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚ ownershipLabelRules:
where-for-diâ€¦ â”‚ - path:
where-for-diâ€¦ â”‚   - spec
where-for-diâ€¦ â”‚   - template
where-for-diâ€¦ â”‚   - metadata
where-for-diâ€¦ â”‚   - labels
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚       kind: Service
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ +++ dirname serviceclaims.yml
where-for-diâ€¦ â”‚ ++ mkdir -p .
where-for-diâ€¦ â”‚ ++ echo 'apiVersion: kapp.k14s.io/v1alpha1
where-for-diâ€¦ â”‚ kind: Config
where-for-diâ€¦ â”‚ waitRules:
where-for-diâ€¦ â”‚ - conditionMatchers:
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "False"
where-for-diâ€¦ â”‚     failure: true
where-for-diâ€¦ â”‚   - type: ServiceAvailable
where-for-diâ€¦ â”‚     status: "True"
where-for-diâ€¦ â”‚     success: true
where-for-diâ€¦ â”‚   resourceMatchers:
where-for-diâ€¦ â”‚   - apiVersionKindMatcher:
where-for-diâ€¦ â”‚       apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚       kind: ServiceBinding
where-for-diâ€¦ â”‚ ---
where-for-diâ€¦ â”‚ apiVersion: servicebinding.io/v1alpha3
where-for-diâ€¦ â”‚ kind: ServiceBinding
where-for-diâ€¦ â”‚ metadata:
where-for-diâ€¦ â”‚   name: where-for-dinner-search-proc-rmq
where-for-diâ€¦ â”‚   annotations:
where-for-diâ€¦ â”‚     local-source-proxy.apps.tanzu.vmware.com: tapscale.azurecr.io/ipillai/tkgifull/lsp:workloads-where-for-dinner-search-proc@sha256:aeb3c9c4f29a8e9c300879ed315c4106fb637281b503ba5a63983f3c36acd50e
where-for-diâ€¦ â”‚     autoscaling.knative.dev/minScale: "1"
where-for-diâ€¦ â”‚     kapp.k14s.io/change-group: servicebinding.io/ServiceBindings
where-for-diâ€¦ â”‚   labels:
where-for-diâ€¦ â”‚     app.kubernetes.io/part-of: where-for-dinner-search-proc
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/auto-configure-actuators: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/has-tests: "true"
where-for-diâ€¦ â”‚     apps.tanzu.vmware.com/workload-type: web
where-for-diâ€¦ â”‚     networking.knative.dev/visibility: cluster-local
where-for-diâ€¦ â”‚     app.kubernetes.io/component: run
where-for-diâ€¦ â”‚     carto.run/workload-name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ spec:
where-for-diâ€¦ â”‚   name: rmq
where-for-diâ€¦ â”‚   service:
where-for-diâ€¦ â”‚     apiVersion: services.apps.tanzu.vmware.com/v1alpha1
where-for-diâ€¦ â”‚     kind: ClassClaim
where-for-diâ€¦ â”‚     name: msgbroker-where-for-dinner
where-for-diâ€¦ â”‚   workload:
where-for-diâ€¦ â”‚     apiVersion: serving.knative.dev/v1
where-for-diâ€¦ â”‚     kind: Service
where-for-diâ€¦ â”‚     name: where-for-dinner-search-proc
where-for-diâ€¦ â”‚ '
where-for-diâ€¦ â”‚ + mkdir -p .imgpkg
where-for-diâ€¦ â”‚ + echo '---
where-for-diâ€¦ â”‚ apiVersion: imgpkg.carvel.dev/v1alpha1
where-for-diâ€¦ â”‚ kind: ImagesLock'
where-for-diâ€¦ â”‚ + imgpkg_params=
where-for-diâ€¦ â”‚ + [[ ! -z '' ]]
where-for-diâ€¦ â”‚ + export IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + IMGPKG_ENABLE_IAAS_AUTH=false
where-for-diâ€¦ â”‚ + imgpkg push -b tapscale.azurecr.io/buildservice/tkgi2/workloads/where-for-dinner-search-proc-workloads-bundle:d99c892a-efe3-4f38-acd2-8dc0b61f1d5e -f .
where-for-diâ€¦ â”‚ IMGPKG_ENABLE_IAAS_AUTH environment variable will be deprecated, please use the flag --activate-keychain to activate the needed keychains
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - 3s
where-for-diâ€¦ â”‚      â”Š Completed       - 5s
where-for-diâ€¦ â”‚ 
where-for-diâ€¦ â”‚ Tracking new pod rollout (where-for-dinner-search-proc-00001-deployment-789d445f97-47xc8):
where-for-diâ€¦ â”‚      â”Š Scheduled       - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Initialized     - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Ready           - (â€¦) Pending
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”ƒ Not Ready       - (ContainersNotReady): containers with unready status: [queue-proxy]
where-for-diâ€¦ â”‚ [workload] Setting Active Processor Count to 4
where-for-diâ€¦ â”‚ [workload] Debugging enabled on port *:9005
where-for-diâ€¦ â”‚ [workload] Calculated JVM Memory Configuration: -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M (Total Memory: 732420K, Thread Count: 50, Loaded Class Count: 16833, Headroom: 0%)
where-for-diâ€¦ â”‚ [workload] Enabling Java Native Memory Tracking
where-for-diâ€¦ â”‚ [workload] Adding 137 container CA certificates to JVM truststore
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T10:22:04.70524621Z","logger":"queueproxy","caller":"sharedmain/main.go:260","message":"Starting queue-proxy","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-789d445f97-47xc8"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T10:22:04.705547281Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server metrics:9090","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-789d445f97-47xc8"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T10:22:04.711640789Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server main:8012","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-789d445f97-47xc8"}
where-for-diâ€¦ â”‚ [queue-proxy] {"severity":"INFO","timestamp":"2024-01-04T10:22:04.711706814Z","logger":"queueproxy","caller":"sharedmain/main.go:266","message":"Starting http server admin:8022","commit":"f60eb32","knative.dev/key":"workloads/where-for-dinner-search-proc-00001","knative.dev/pod":"where-for-dinner-search-proc-00001-deployment-789d445f97-47xc8"}
where-for-diâ€¦ â”‚ [workload] Spring Cloud Bindings Enabled
where-for-diâ€¦ â”‚ [workload] Picked up JAVA_TOOL_OPTIONS: -Djava.net.preferIPv4Stack="true" -Dmanagement.endpoint.health.probes.add-additional-paths="true" -Dmanagement.endpoint.health.show-details="always" -Dmanagement.endpoints.web.base-path="/actuator" -Dmanagement.endpoints.web.exposure.include="*" -Dmanagement.health.probes.enabled="true" -Dmanagement.server.port="8081" -Dserver.port="8080" -Djava.security.properties=/layers/tanzu-buildpacks_bellsoft-liberica/java-security-properties/java-security.properties -XX:+ExitOnOutOfMemoryError -XX:ActiveProcessorCount=4 -agentlib:jdwp=transport=dt_socket,server=y,address=*:9005,suspend=n -XX:MaxDirectMemorySize=10M -Xmx316204K -XX:MaxMetaspaceSize=109015K -XX:ReservedCodeCacheSize=240M -Xss1M -XX:+UnlockDiagnosticVMOptions -XX:NativeMemoryTracking=summary -XX:+PrintNMTStatistics -Dorg.springframework.cloud.bindings.boot.enable=true
where-for-diâ€¦ â”‚ [workload] Listening for transport dt_socket at address: 9005
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload]   .   ____          _            __ _ _
where-for-diâ€¦ â”‚ [workload]  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
where-for-diâ€¦ â”‚ [workload] ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
where-for-diâ€¦ â”‚ [workload]  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
where-for-diâ€¦ â”‚ [workload]   '  |____| .__|_| |_|_| |_\__, | / / / /
where-for-diâ€¦ â”‚ [workload]  =========|_|==============|___/=/_/_/_/
where-for-diâ€¦ â”‚ [workload]  :: Spring Boot ::                (v3.1.3)
where-for-diâ€¦ â”‚ [workload] 
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:07.006Z  INFO 102 --- [           main] .t.w.WhereForDinnerSearchProcApplication : Starting WhereForDinnerSearchProcApplication v0.0.1-SNAPSHOT using Java 17.0.9 with PID 102 (/workspace/BOOT-INF/classes started by cnb in /workspace)
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:07.026Z  INFO 102 --- [           main] .t.w.WhereForDinnerSearchProcApplication : No active profile set, falling back to 1 default profile: "default"
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:07.291Z  INFO 102 --- [           main] .BindingSpecificEnvironmentPostProcessor : Creating binding-specific PropertySource from Kubernetes Service Bindings
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:11.092Z  INFO 102 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Multiple Spring Data modules found, entering strict repository configuration mode
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:11.095Z  INFO 102 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data Redis repositories in DEFAULT mode.
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:11.133Z  INFO 102 --- [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 12 ms. Found 0 Redis repository interfaces.
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:11.426Z  INFO 102 --- [           main] faultConfiguringBeanFactoryPostProcessor : No bean named 'errorChannel' has been explicitly defined. Therefore, a default PublishSubscribeChannel will be created.
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:11.487Z  INFO 102 --- [           main] faultConfiguringBeanFactoryPostProcessor : No bean named 'integrationHeaderChannelRegistry' has been explicitly defined. Therefore, a default DefaultHeaderChannelRegistry will be created.
where-for-diâ€¦ â”‚ [queue-proxy] aggressive probe error (failed 199 times): Get "http://127.0.0.1:8080/readyz": dial tcp 127.0.0.1:8080: connect: connection refused
where-for-diâ€¦ â”‚ [queue-proxy] timed out waiting for the condition
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:17.206Z  INFO 102 --- [           main] o.s.c.f.web.flux.FunctionHandlerMapping  : FunctionCatalog: org.springframework.cloud.function.context.catalog.BeanFactoryAwareFunctionRegistry@5ed25612
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:19.446Z  INFO 102 --- [           main] o.s.c.s.binder.DefaultBinderFactory      : Creating binder: rabbit
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:19.446Z  INFO 102 --- [           main] o.s.c.s.binder.DefaultBinderFactory      : Constructing binder child context for rabbit
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:19.783Z  INFO 102 --- [           main] o.s.c.s.binder.DefaultBinderFactory      : Caching the binder: rabbit
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:21.091Z  INFO 102 --- [           main] o.s.i.endpoint.EventDrivenConsumer       : Adding {logging-channel-adapter:_org.springframework.integration.errorLogger} as a subscriber to the 'errorChannel' channel
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:21.092Z  INFO 102 --- [           main] o.s.i.channel.PublishSubscribeChannel    : Channel 'where-for-dinner-search-proc.errorChannel' has 1 subscriber(s).
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:21.092Z  INFO 102 --- [           main] o.s.i.endpoint.EventDrivenConsumer       : started bean '_org.springframework.integration.errorLogger'
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:21.107Z  INFO 102 --- [           main] o.s.a.r.c.CachingConnectionFactory       : Attempting to connect to: [msgbroker-where-for-dinner.service-instances.svc:5672]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:21.492Z  INFO 102 --- [           main] o.s.a.r.c.CachingConnectionFactory       : Created new connection: rabbitConnectionFactory#681de87f:0/SimpleConnection@4d2bc56a [delegate=amqp://default_user_zB7KCw1r2aJ8u7icyMW@10.100.200.203:5672/, localPort=53236]
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:21.690Z  INFO 102 --- [           main] o.s.i.endpoint.ReactiveStreamsConsumer   : started org.springframework.integration.endpoint.ReactiveStreamsConsumer@8167f57
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:21.850Z  INFO 102 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8080
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:22.288Z  INFO 102 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 17 endpoint(s) beneath base path '/actuator'
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:22.612Z  INFO 102 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8081
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:22.692Z  INFO 102 --- [           main] c.s.b.r.p.RabbitExchangeQueueProvisioner : declaring queue for inbound: where-for-dinner-search-criteria.where-for-dinner-search-criteria-group, bound to: where-for-dinner-search-criteria
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:22.810Z  INFO 102 --- [           main] o.s.c.stream.binder.BinderErrorChannel   : Channel 'rabbit-2000689527.processSearch-in-0.errors' has 1 subscriber(s).
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:22.812Z  INFO 102 --- [           main] o.s.c.stream.binder.BinderErrorChannel   : Channel 'rabbit-2000689527.processSearch-in-0.errors' has 2 subscriber(s).
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:22.984Z  INFO 102 --- [           main] o.s.i.a.i.AmqpInboundChannelAdapter      : started bean 'inbound.where-for-dinner-search-criteria.where-for-dinner-search-criteria-group'
where-for-diâ€¦ â”‚ [workload] 2024-01-04T10:22:23.102Z  INFO 102 --- [           main] .t.w.WhereForDinnerSearchProcApplication : Started WhereForDinnerSearchProcApplication in 17.375 seconds (process running for 18.336)
where-for-diâ€¦ â”‚      â”Š Scheduled       - <1s
where-for-diâ€¦ â”‚      â”Š Initialized     - <1s
where-for-diâ€¦ â”‚      â”Š Ready           - 30s
